{"pages":[{"title":"分类","text":"","link":"/categories/index.html"},{"title":"关于我","text":"出生于Linux OS发布的前一天，喜欢 声乐 唱歌，目前希望养一只猫。本站有两个部分构成：主站技术（原理、原则、模式）、读书与思考随笔WikiNotes类库、框架 API 实战 做一个快乐的调包侠","link":"/about/index.html"},{"title":"标签","text":"","link":"/tags/index.html"},{"title":"系列","text":"Java 集合内部原理ArrayList 内部原理PriorityQueue 内部原理DelayQueue 内部原理HashMap 内部原理TreeMap 内部原理一二三四LinkedHashMap 内部原理WeakHashMap 内部原理HashSet 内部原理TreeSet 内部原理","link":"/series/index.html"},{"title":"札记","text":"2019年06月22日今天看书[2]的第三章，一时兴起搜索了下关键字（Linux namespace），然后发现了书作者在 infoQ 的文章，接着看到了与该文章及其类似的某耗子的博客（而且并没有注明参考了前者），感叹人类果然是复读机。印证了先生说的「遮蔽效应」2019年06月21日好的技术书，除去代码部分，文字部分应该起到连接现实商业与 IT 技术、澄清与旧技术勾连关系的作用，如果还能说清技术史，那就更好不过了。[2]2019年06月20日你要理解和把握一门学问，你必须站在比他更高的高点上俯视它，你才能看清这个事物的全貌。[1]Ref1.东岳学习坊 第 25 课，广义哲学与狭义哲学的天壤之别 ↩2.读《Docker 容器与容器云》第二版 有感 ↩","link":"/jour/index.html"}],"posts":[{"title":"TreeMap内部原理(四)","text":"二叉树的遍历我们知道二叉查找树的遍历有前序遍历、中序遍历、后序遍历。前序遍历，先遍历我，再遍历我的左子节点，最后遍历我的右子节点；中序遍历，先遍历我的左子节点，再遍历我，最后遍历我的右子节点；后序遍历，先遍历我的左子节点，再遍历我的右子节点，最后遍历我；这里的前中后都是以“我”的顺序为准的，我在前就是前序遍历，我在中就是中序遍历，我在后就是后序遍历。下面让我们看看经典的中序遍历是怎么实现的：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class TreeMapTest { public static void main(String[] args) { // 构建一颗10个元素的树 TreeNode&lt;Integer&gt; node = new TreeNode&lt;&gt;(1, null).insert(2) .insert(6).insert(3).insert(5).insert(9) .insert(7).insert(8).insert(4).insert(10); // 中序遍历，打印结果为1到10的顺序 node.root().inOrderTraverse(); }}/** * 树节点，假设不存在重复元素 * @param &lt;T&gt; */class TreeNode&lt;T extends Comparable&lt;T&gt;&gt; { T value; TreeNode&lt;T&gt; parent; TreeNode&lt;T&gt; left, right; public TreeNode(T value, TreeNode&lt;T&gt; parent) { this.value = value; this.parent = parent; } /** * 获取根节点 */ TreeNode&lt;T&gt; root() { TreeNode&lt;T&gt; cur = this; while (cur.parent != null) { cur = cur.parent; } return cur; } /** * 中序遍历 */ void inOrderTraverse() { if(this.left != null) this.left.inOrderTraverse(); System.out.println(this.value); if(this.right != null) this.right.inOrderTraverse(); } /** * 经典的二叉树插入元素的方法 */ TreeNode&lt;T&gt; insert(T value) { // 先找根元素 TreeNode&lt;T&gt; cur = root(); TreeNode&lt;T&gt; p; int dir; // 寻找元素应该插入的位置 do { p = cur; if ((dir=value.compareTo(p.value)) &lt; 0) { cur = cur.left; } else { cur = cur.right; } } while (cur != null); // 把元素放到找到的位置 if (dir &lt; 0) { p.left = new TreeNode&lt;&gt;(value, p); return p.left; } else { p.right = new TreeNode&lt;&gt;(value, p); return p.right; } }}TreeMap的遍历从上面二叉树的遍历我们很明显地看到，它是通过递归的方式实现的，但是递归会占用额外的空间，直接到线程栈整个释放掉才会把方法中申请的变量销毁掉，所以当元素特别多的时候是一件很危险的事。（上面的例子中，没有申请额外的空间，如果有声明变量，则可以理解为直到方法完成才会销毁变量）那么，有没有什么方法不用递归呢？让我们来看看java中的实现：12345678910111213141516@Overridepublic void forEach(BiConsumer&lt;? super K, ? super V&gt; action) { Objects.requireNonNull(action); // 遍历前的修改次数 int expectedModCount = modCount; // 执行遍历，先获取第一个元素的位置，再循环遍历后继节点 for (Entry&lt;K, V&gt; e = getFirstEntry(); e != null; e = successor(e)) { // 执行动作 action.accept(e.key, e.value); // 如果发现修改次数变了，则抛出异常 if (expectedModCount != modCount) { throw new ConcurrentModificationException(); } }}是不是很简单？！寻找第一个节点；从根节点开始找最左边的节点，即最小的元素。12345678final Entry&lt;K,V&gt; getFirstEntry() { Entry&lt;K,V&gt; p = root; // 从根节点开始找最左边的节点，即最小的元素 if (p != null) while (p.left != null) p = p.left; return p;}循环遍历后继节点；寻找后继节点这个方法我们在删除元素的时候也用到过，当时的场景是有右子树，则从其右子树中寻找最小的节点。1234567891011121314151617181920212223static &lt;K,V&gt; TreeMap.Entry&lt;K,V&gt; successor(Entry&lt;K,V&gt; t) { if (t == null) // 如果当前节点为空，返回空 return null; else if (t.right != null) { // 如果当前节点有右子树，取右子树中最小的节点 Entry&lt;K,V&gt; p = t.right; while (p.left != null) p = p.left; return p; } else { // 如果当前节点没有右子树 // 如果当前节点是父节点的左子节点，直接返回父节点 // 如果当前节点是父节点的右子节点，一直往上找，直到找到一个祖先节点是其父节点的左子节点为止，返回这个祖先节点的父节点 Entry&lt;K,V&gt; p = t.parent; Entry&lt;K,V&gt; ch = t; while (p != null &amp;&amp; ch == p.right) { ch = p; p = p.parent; } return p; }}让我们一起来分析下这种方式的时间复杂度吧。首先，寻找第一个元素，因为红黑树是接近平衡的二叉树，所以找最小的节点，相当于是从顶到底了，时间复杂度为O(log n)；其次，寻找后继节点，因为红黑树插入元素的时候会自动平衡，最坏的情况就是寻找右子树中最小的节点，时间复杂度为O(log k)，k为右子树元素个数；最后，需要遍历所有元素，时间复杂度为O(n)；所以，总的时间复杂度为 O(log n) + O(n * log k) ≈ O(n)。虽然遍历红黑树的时间复杂度是O(n)，但是它实际是要比跳表要慢一点的，啥？跳表是啥？安心，后面会讲到跳表的。总结到这里红黑树就整个讲完了，让我们再回顾下红黑树的特性：每个节点或者是黑色，或者是红色。根节点是黑色。每个叶子节点（NIL）是黑色。（注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！）如果一个节点是红色的，则它的子节点必须是黑色的。从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。除了上述这些标准的红黑树的特性，你还能讲出来哪些TreeMap的特性呢？TreeMap的存储结构只有一颗红黑树；TreeMap中的元素是有序的，按key的顺序排列；TreeMap比HashMap要慢一些，因为HashMap前面还做了一层桶，寻找元素要快很多；TreeMap没有扩容的概念；TreeMap的遍历不是采用传统的递归式遍历；TreeMap可以按范围查找元素，查找最近的元素；彩蛋上面我们说到的删除元素的时候，如果当前节点有右子树，则从右子树中寻找最小元素所在的位置，把这个位置的元素放到当前位置，再把删除的位置移到那个位置，再看有没有替代元素，balabala。那么，除了这种方式，还有没有其它方式呢？答案当然是肯定的。上面我们说的红黑树的插入元素、删除元素的过程都是标准的红黑树是那么干的，其实也不一定要完全那么做。比如说，删除元素，如果当前节点有左子树，那么，我们可以找左子树中最大元素的位置，然后把这个位置的元素放到当前节点，再把删除的位置移到那个位置，再看有没有替代元素，balabala。举例说明，比如下面这颗红黑树：我们删除10这个元素，从左子树中找最大的，找到了9这个元素，那么把9放到10的位置，然后把删除的位置移到原来9的位置，发现不需要作平衡（红+黑节点），直接把这个位置删除就可以了。同样是满足红黑树的特性的。","link":"/2016/07/08/TreeMap内部原理(四)/"},{"title":"scikit-learn 导出 JPMML","text":"提示: 本文是模型部署方案的一部分依赖Python 2.7, 3.4 or newer.scikit-learn 0.16.0 or newer.sklearn-pandas 0.0.10 or newer.sklearn2pmml 0.14.0 or newer.步骤使用sklearn训练一个模型使用sklearn原生API将模型到处为 pickle 格式使用 JPMML-SkLearn命令将原始pickle 格式文件转换成JPMML文件我们开始吧训练模型 &amp; 导出pickle1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import pandas# 0. 使用pandas加载iris数据集df = pandas.read_csv(\"file:./csv/Iris.csv\")iris_X = df[df.columns.difference([\"Species\"])]iris_y = df[\"Species\"]# 1. 创建一个sklearn_pandas.DataFrameMapper对象，它主要提供面向列的特征工程、标准化功能from sklearn_pandas import DataFrameMapperfrom sklearn.preprocessing import StandardScalerfrom sklearn2pmml.decoration import ContinuousDomaincolumn_preprocessor = DataFrameMapper([ ([\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"], [ContinuousDomain(), StandardScaler()])])# 2. 创建一个Transformer和Selector对象，它主要提供面向表的特征工程、标准化功能from sklearn.decomposition import PCAfrom sklearn.feature_selection import SelectKBestfrom sklearn.pipeline import Pipelinefrom sklearn2pmml import SelectorProxytable_preprocessor = Pipeline([ (\"pca\", PCA(n_components = 3)), (\"selector\", SelectorProxy(SelectKBest(k = 2)))])# 3. 创建一个Estimator对象from sklearn.tree import DecisionTreeClassifierclassifier = DecisionTreeClassifier(min_samples_leaf = 5)# 将上面的对象作为参数与sklearn2pmml.pipeline.PMMLPipeline对象组合起来，此时可以运行一下，看看能不能跑通from sklearn2pmml.pipeline import PMMLPipelinepipeline = PMMLPipeline([ (\"columns\", column_preprocessor), (\"table\", table_preprocessor), (\"classifier\", classifier)])pipeline.fit(iris_X, iris_y)# 内置的模型验证功能pipeline.verify(iris_X.sample(n = 15))# 将拟合好的PMMLPipeline对象，用joblib导出为pickle格式的文件from sklearn.externals import joblibjoblib.dump(pipeline, \"pipeline.pkl.z\", compress = 9)此时，我们得到了原始模型文件转换JPMML我们需要jpmml-sklearn的帮助，来转换原始模型文件。目前该子项目的最新版本为1.5.4，我们可以下载编译好的jar包，或者自己编译。这里笔者选择自行编译：12git clone https://github.com/jpmml/jpmml-sklearn.gitcd jpmml-sklearn &amp;&amp; mvn clean install -DskipTests讲我们上面得到的原始文件拷贝到jpmml-sklearn目录下, 执行:1java -jar target/jpmml-sklearn-executable-1.5-SNAPSHOT.jar --pkl-input pipeline.pkl.z --pmml-output pipeline.pmml我们成功的得到了pmml模型文件","link":"/2018/08/30/sklearn导出jpmml/"},{"title":"DelayQueue内部原理","text":"问题DelayQueue是阻塞队列吗？DelayQueue的实现方式？DelayQueue主要用于什么场景？简介DelayQueue是java并发包下的延时阻塞队列，常用于实现定时任务。实战另见 DelayQueue继承体系从继承体系可以看到，DelayQueue实现了BlockingQueue，所以它是一个阻塞队列。另外，DelayQueue还组合了一个叫做Delayed的接口，DelayQueue中存储的所有元素必须实现Delayed接口。那么，Delayed是什么呢？123public interface Delayed extends Comparable&lt;Delayed&gt; { long getDelay(TimeUnit unit);}Delayed是一个继承自Comparable的接口，并且定义了一个getDelay()方法，用于表示还有多少时间到期，到期了应返回小于等于0的数值。源码分析主要属性12345678// 用于控制并发的锁private final transient ReentrantLock lock = new ReentrantLock();// 优先级队列private final PriorityQueue&lt;E&gt; q = new PriorityQueue&lt;E&gt;();// 用于标记当前是否有线程在排队（仅用于取元素时）private Thread leader = null;// 条件，用于表示现在是否有可取的元素private final Condition available = lock.newCondition();从属性我们可以知道，延时队列主要使用优先级队列来实现，并辅以重入锁和条件来控制并发安全。因为优先级队列是无界的，所以这里只需要一个条件就可以了。还记得优先级队列吗？点击链接直达 PriorityQueue内部原理主要构造方法12345public DelayQueue() {}public DelayQueue(Collection&lt;? extends E&gt; c) { this.addAll(c);}构造方法比较简单，一个默认构造方法，一个初始化添加集合c中所有元素的构造方法。入队因为DelayQueue是阻塞队列，且优先级队列是无界的，所以入队不会阻塞不会超时，因此它的四个入队方法是一样的。1234567891011121314151617181920212223242526public boolean add(E e) { return offer(e);}public void put(E e) { offer(e);}public boolean offer(E e, long timeout, TimeUnit unit) { return offer(e);}public boolean offer(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { q.offer(e); if (q.peek() == e) { leader = null; available.signal(); } return true; } finally { lock.unlock(); }}入队方法比较简单：（1）加锁；（2）添加元素到优先级队列中；（3）如果添加的元素是堆顶元素，就把leader置为空，并唤醒等待在条件available上的线程；（4）解锁；出队因为DelayQueue是阻塞队列，所以它的出队有四个不同的方法，有抛出异常的，有阻塞的，有不阻塞的，有超时的。我们这里主要分析两个，poll()和take()方法。12345678910111213public E poll() { final ReentrantLock lock = this.lock; lock.lock(); try { E first = q.peek(); if (first == null || first.getDelay(NANOSECONDS) &gt; 0) return null; else return q.poll(); } finally { lock.unlock(); }}poll()方法比较简单：加锁；检查第一个元素，如果为空或者还没到期，就返回null；如果第一个元素到期了就调用poll()弹出第一个元素；解锁。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try { for (;;) { // 堆顶元素 E first = q.peek(); // 如果堆顶元素为空，说明队列中还没有元素，直接阻塞等待 if (first == null) available.await(); else { // 堆顶元素的到期时间 long delay = first.getDelay(NANOSECONDS); // 如果小于0说明已到期，直接调用poll()方法弹出堆顶元素 if (delay &lt;= 0) return q.poll(); // 如果delay大于0 ，则下面要阻塞了 // 将first置为空方便gc，因为有可能其它元素弹出了这个元素 // 这里还持有着引用不会被清理 first = null; // don't retain ref while waiting // 如果前面有其它线程在等待，直接进入等待 if (leader != null) available.await(); else { // 如果leader为null，把当前线程赋值给它 Thread thisThread = Thread.currentThread(); leader = thisThread; try { // 等待delay时间后自动醒过来 // 醒过来后把leader置空并重新进入循环判断堆顶元素是否到期 // 这里即使醒过来后也不一定能获取到元素 // 因为有可能其它线程先一步获取了锁并弹出了堆顶元素 // 条件锁的唤醒分成两步，先从Condition的队列里出队 // 再入队到AQS的队列中，当其它线程调用LockSupport.unpark(t)的时候才会真正唤醒 // 关于AQS我们后面会讲的^^ available.awaitNanos(delay); } finally { // 如果leader还是当前线程就把它置为空，让其它线程有机会获取元素 if (leader == thisThread) leader = null; } } } } } finally { // 成功出队后，如果leader为空且堆顶还有元素，就唤醒下一个等待的线程 if (leader == null &amp;&amp; q.peek() != null) // signal()只是把等待的线程放到AQS的队列里面，并不是真正的唤醒 available.signal(); // 解锁，这才是真正的唤醒 lock.unlock(); }}take()方法稍微要复杂一些：加锁；判断堆顶元素是否为空，为空的话直接阻塞等待；判断堆顶元素是否到期，到期了直接poll()出元素；没到期，再判断前面是否有其它线程在等待，有则直接等待；前面没有其它线程在等待，则把自己当作第一个线程等待delay时间后唤醒，再尝试获取元素；获取到元素之后再唤醒下一个等待的线程；解锁；使用方法说了那么多，是不是还是不知道怎么用呢？那怎么能行，请看下面的案例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class DelayQueueTest { public static void main(String[] args) { DelayQueue&lt;Message&gt; queue = new DelayQueue&lt;&gt;(); long now = System.currentTimeMillis(); // 启动一个线程从队列中取元素 new Thread(()-&gt;{ while (true) { try { // 将依次打印1000，2000，5000，7000，8000 System.out.println(queue.take().deadline - now); } catch (InterruptedException e) { e.printStackTrace(); } } }).start(); // 添加5个元素到队列中 queue.add(new Message(now + 5000)); queue.add(new Message(now + 8000)); queue.add(new Message(now + 2000)); queue.add(new Message(now + 1000)); queue.add(new Message(now + 7000)); }}class Message implements Delayed { long deadline; public Message(long deadline) { this.deadline = deadline; } @Override public long getDelay(TimeUnit unit) { return deadline - System.currentTimeMillis(); } @Override public int compareTo(Delayed o) { return (int) (getDelay(TimeUnit.MILLISECONDS) - o.getDelay(TimeUnit.MILLISECONDS)); } @Override public String toString() { return String.valueOf(deadline); }}是不是很简单，越早到期的元素越先出队。总结DelayQueue是阻塞队列；DelayQueue内部存储结构使用优先级队列；DelayQueue使用重入锁和条件来控制并发安全；DelayQueue常用于定时任务；彩蛋java中的线程池实现定时任务是直接用的DelayQueue吗？当然不是，ScheduledThreadPoolExecutor中使用的是它自己定义的内部类DelayedWorkQueue，其实里面的实现逻辑基本都是一样的，只不过DelayedWorkQueue里面没有使用现在的PriorityQueue，而是使用数组又实现了一遍优先级队列，本质上没有什么区别。","link":"/2016/07/19/DelayQueue内部原理/"},{"title":"PriorityQueue内部原理","text":"问题什么是优先级队列？怎么实现一个优先级队列？PriorityQueue是线程安全的吗？PriorityQueue就有序的吗？简介优先级队列，是0个或多个元素的集合，集合中的每个元素都有一个权重值，每次出队都弹出优先级最大或最小的元素。一般来说，优先级队列使用堆来实现。那么Java里面是如何通过“堆”这个数据结构来实现优先级队列的呢？让我们一起来学习吧。源码分析主要属性12345678910// 默认容量private static final int DEFAULT_INITIAL_CAPACITY = 11;// 存储元素的地方transient Object[] queue; // non-private to simplify nested class access// 元素个数private int size = 0;// 比较器private final Comparator&lt;? super E&gt; comparator;// 修改次数transient int modCount = 0; // non-private to simplify nested class access默认容量是11；queue，元素存储在数组中，这跟我们之前说的堆一般使用数组来存储是一致的；comparator，比较器，在优先级队列中，也有两种方式比较元素，一种是元素的自然顺序，一种是通过比较器来比较；modCount，修改次数，有这个属性表示PriorityQueue也是fast-fail的；入队入队有两个方法，add(E e)和offer(E e)，两者是一致的，add(E e)也是调用的offer(E e)。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public boolean add(E e) { return offer(e);}public boolean offer(E e) { // 不支持null元素 if (e == null) throw new NullPointerException(); modCount++; // 取size int i = size; // 元素个数达到最大容量了，扩容 if (i &gt;= queue.length) grow(i + 1); // 元素个数加1 size = i + 1; // 如果还没有元素 // 直接插入到数组第一个位置 // 这里跟我们之前讲堆不一样了 // java里面是从0开始的 // 我们说的堆是从1开始的 if (i == 0) queue[0] = e; else // 否则，插入元素到数组size的位置，也就是最后一个元素的下一位 // 注意这里的size不是数组大小，而是元素个数 // 然后，再做自下而上的堆化 siftUp(i, e); return true;}private void siftUp(int k, E x) { // 根据是否有比较器，使用不同的方法 if (comparator != null) siftUpUsingComparator(k, x); else siftUpComparable(k, x);}@SuppressWarnings(\"unchecked\")private void siftUpComparable(int k, E x) { Comparable&lt;? super E&gt; key = (Comparable&lt;? super E&gt;) x; while (k &gt; 0) { // 找到父节点的位置 // 因为元素是从0开始的，所以减1之后再除以2 int parent = (k - 1) &gt;&gt;&gt; 1; // 父节点的值 Object e = queue[parent]; // 比较插入的元素与父节点的值 // 如果比父节点大，则跳出循环 // 否则交换位置 if (key.compareTo((E) e) &gt;= 0) break; // 与父节点交换位置 queue[k] = e; // 现在插入的元素位置移到了父节点的位置 // 继续与父节点再比较 k = parent; } // 最后找到应该插入的位置，放入元素 queue[k] = key;}入队不允许null元素；如果数组不够用了，先扩容；如果还没有元素，就插入下标0的位置；如果有元素了，就插入到最后一个元素往后的一个位置（实际并没有插入哈）；自下而上堆化，一直往上跟父节点比较；如果比父节点小，就与父节点交换位置，直到出现比父节点大为止；由此可见，PriorityQueue是一个小顶堆。扩容12345678910111213141516171819202122232425private void grow(int minCapacity) { // 旧容量 int oldCapacity = queue.length; // Double size if small; else grow by 50% // 旧容量小于64时，容量翻倍 // 旧容量大于等于64，容量只增加旧容量的一半 int newCapacity = oldCapacity + ((oldCapacity &lt; 64) ? (oldCapacity + 2) : (oldCapacity &gt;&gt; 1)); // overflow-conscious code // 检查是否溢出 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 创建出一个新容量大小的新数组并把旧数组元素拷贝过去 queue = Arrays.copyOf(queue, newCapacity);}private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;}当数组比较小（小于64）的时候每次扩容容量翻倍；当数组比较大的时候每次扩容只增加一半的容量；出队出队有两个方法，remove()和poll()，remove()也是调用的poll()，只是没有元素的时候抛出异常。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public E remove() { // 调用poll弹出队首元素 E x = poll(); if (x != null) // 有元素就返回弹出的元素 return x; else // 没有元素就抛出异常 throw new NoSuchElementException();}@SuppressWarnings(\"unchecked\")public E poll() { // 如果size为0，说明没有元素 if (size == 0) return null; // 弹出元素，元素个数减1 int s = --size; modCount++; // 队列首元素 E result = (E) queue[0]; // 队列末元素 E x = (E) queue[s]; // 将队列末元素删除 queue[s] = null; // 如果弹出元素后还有元素 if (s != 0) // 将队列末元素移到队列首 // 再做自上而下的堆化 siftDown(0, x); // 返回弹出的元素 return result;}private void siftDown(int k, E x) { // 根据是否有比较器，选择不同的方法 if (comparator != null) siftDownUsingComparator(k, x); else siftDownComparable(k, x);}@SuppressWarnings(\"unchecked\")private void siftDownComparable(int k, E x) { Comparable&lt;? super E&gt; key = (Comparable&lt;? super E&gt;)x; // 只需要比较一半就行了，因为叶子节点占了一半的元素 int half = size &gt;&gt;&gt; 1; // loop while a non-leaf while (k &lt; half) { // 寻找子节点的位置，这里加1是因为元素从0号位置开始 int child = (k &lt;&lt; 1) + 1; // assume left child is least // 左子节点的值 Object c = queue[child]; // 右子节点的位置 int right = child + 1; if (right &lt; size &amp;&amp; ((Comparable&lt;? super E&gt;) c).compareTo((E) queue[right]) &gt; 0) // 左右节点取其小者 c = queue[child = right]; // 如果比子节点都小，则结束 if (key.compareTo((E) c) &lt;= 0) break; // 如果比最小的子节点大，则交换位置 queue[k] = c; // 指针移到最小子节点的位置继续往下比较 k = child; } // 找到正确的位置，放入元素 queue[k] = key;}将队列首元素弹出；将队列末元素移到队列首；自上而下堆化，一直往下与最小的子节点比较；如果比最小的子节点大，就交换位置，再继续与最小的子节点比较；如果比最小的子节点小，就不用交换位置了，堆化结束；这就是堆中的删除堆顶元素；取队首元素取队首元素有两个方法，element()和peek()，element()也是调用的peek()，只是没取到元素时抛出异常。12345678910public E element() { E x = peek(); if (x != null) return x; else throw new NoSuchElementException();}public E peek() { return (size == 0) ? null : (E) queue[0];}如果有元素就取下标0的元素；如果没有元素就返回null，element()抛出异常；总结PriorityQueue是一个小顶堆；PriorityQueue是非线程安全的；PriorityQueue不是有序的，只有堆顶存储着最小的元素；入队就是堆的插入元素的实现；出队就是堆的删除元素的实现；彩蛋论Queue中的那些方法？Queue是所有队列的顶级接口，它里面定义了一批方法，它们有什么区别呢？操作抛出异常返回特定值入队add(e)offer(e)——false出队remove()poll()——null检查element()peek()——null为什么PriorityQueue中的add(e)方法没有做异常检查呢？因为PriorityQueue是无限增长的队列，元素不够用了会扩容，所以添加元素不会失败。","link":"/2016/07/15/PriorityQueue内部原理/"},{"title":"LinkedHashMap内部原理","text":"简介LinkedHashMap内部维护了一个双向链表，能保证元素按插入的顺序访问，也能以访问顺序访问，可以用来实现LRU缓存策略。LinkedHashMap可以看成是 LinkedList + HashMap。实战另见 LinkedHashMap继承体系LinkedHashMap继承HashMap，拥有HashMap的所有特性，并且额外增加的按一定顺序访问的特性。存储结构我们知道HashMap使用（数组 + 单链表 + 红黑树）的存储结构，那LinkedHashMap是怎么存储的呢？通过上面的继承体系，我们知道它继承了Map，所以它的内部也有这三种结构，但是它还额外添加了一种“双向链表”的结构存储所有元素的顺序。添加删除元素的时候需要同时维护在HashMap中的存储，也要维护在LinkedList中的存储，所以性能上来说会比HashMap稍慢。源码解析属性1234567891011121314/*** 双向链表头节点 */transient LinkedHashMap.Entry&lt;K,V&gt; head;/*** 双向链表尾节点 */transient LinkedHashMap.Entry&lt;K,V&gt; tail;/*** 是否按访问顺序排序 */final boolean accessOrder;head双向链表的头节点，旧数据存在头节点。tail双向链表的尾节点，新数据存在尾节点。accessOrder是否需要按访问顺序排序，如果为false则按插入顺序存储元素，如果是true则按访问顺序存储元素。内部类123456789101112131415// 位于LinkedHashMap中static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); }}// 位于HashMap中static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; { final int hash; final K key; V value; Node&lt;K, V&gt; next;}存储节点，继承自HashMap的Node类，next用于单链表存储于桶中，before和after用于双向链表存储所有元素。构造方法123456789101112131415161718192021222324252627public LinkedHashMap(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor); accessOrder = false;}public LinkedHashMap(int initialCapacity) { super(initialCapacity); accessOrder = false;}public LinkedHashMap() { super(); accessOrder = false;}public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) { super(); accessOrder = false; putMapEntries(m, false);}public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder;}前四个构造方法accessOrder都等于false，说明双向链表是按插入顺序存储元素。最后一个构造方法accessOrder从构造方法参数传入，如果传入true，则就实现了按访问顺序存储元素，这也是实现LRU缓存策略的关键。afterNodeInsertion(boolean evict)方法在节点插入之后做些什么，在HashMap中的putVal()方法中被调用，可以看到HashMap中这个方法的实现为空。1234567891011void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); }}protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return false;}evict，驱逐的意思。如果evict为true，且头节点不为空，且确定移除最老的元素，那么就调用HashMap.removeNode()把头节点移除（这里的头节点是双向链表的头节点，而不是某个桶中的第一个元素）；HashMap.removeNode()从HashMap中把这个节点移除之后，会调用afterNodeRemoval()方法；afterNodeRemoval()方法在LinkedHashMap中也有实现，用来在移除元素后修改双向链表，见下文；默认removeEldestEntry()方法返回false，也就是不删除元素。afterNodeAccess(Node&lt;K,V&gt; e)方法在节点访问之后被调用，主要在put()已经存在的元素或get()时被调用，如果accessOrder为true，调用这个方法把访问到的节点移动到双向链表的末尾。123456789101112131415161718192021222324252627282930void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; // 如果accessOrder为true，并且访问的节点不是尾节点 if (accessOrder &amp;&amp; (last = tail) != e) { LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 把p节点从双向链表中移除 p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; // 把p节点放到双向链表的末尾 if (last == null) head = p; else { p.before = last; last.after = p; } // 尾节点等于p tail = p; ++modCount; }}如果accessOrder为true，并且访问的节点不是尾节点；从双向链表中移除访问的节点；把访问的节点加到双向链表的末尾；（末尾为最新访问的元素）afterNodeRemoval(Node&lt;K,V&gt; e)方法在节点被删除之后调用的方法。1234567891011121314void afterNodeRemoval(Node&lt;K,V&gt; e) { // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 把节点p从双向链表中删除。 p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b;}经典的把节点从双向链表中删除的方法。get(Object key)方法获取元素。12345678public V get(Object key) { Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;}如果查找到了元素，且accessOrder为true，则调用afterNodeAccess()方法把访问的节点移到双向链表的末尾。总结LinkedHashMap继承自HashMap，具有HashMap的所有特性；LinkedHashMap内部维护了一个双向链表存储所有的元素；如果accessOrder为false，则可以按插入元素的顺序遍历元素；如果accessOrder为true，则可以按访问元素的顺序遍历元素；LinkedHashMap的实现非常精妙，很多方法都是在HashMap中留的钩子（Hook），直接实现这些Hook就可以实现对应的功能了，并不需要再重写put()等方法；默认的LinkedHashMap并不会移除旧元素，如果需要移除旧元素，则需要重写removeEldestEntry()方法设定移除策略；LinkedHashMap可以用来实现LRU缓存淘汰策略；彩蛋LinkedHashMap如何实现LRU缓存淘汰策略呢？首先，我们先来看看LRU是个什么鬼。LRU，Least Recently Used，最近最少使用，也就是优先淘汰最近最少使用的元素。如果使用LinkedHashMap，我们把accessOrder设置为true是不是就差不多能实现这个策略了呢？答案是肯定的。请看下面的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.coolcoding.code;import java.util.LinkedHashMap;import java.util.Map;/** * @author: tangtong * @date: 2019/3/18 */public class LRUTest { public static void main(String[] args) { // 创建一个只有5个元素的缓存 LRU&lt;Integer, Integer&gt; lru = new LRU&lt;&gt;(5, 0.75f); lru.put(1, 1); lru.put(2, 2); lru.put(3, 3); lru.put(4, 4); lru.put(5, 5); lru.put(6, 6); lru.put(7, 7); System.out.println(lru.get(4)); lru.put(6, 666); // 输出: {3=3, 5=5, 7=7, 4=4, 6=666} // 可以看到最旧的元素被删除了 // 且最近访问的4被移到了后面 System.out.println(lru); }}class LRU&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; { // 保存缓存的容量 private int capacity; public LRU(int capacity, float loadFactor) { super(capacity, loadFactor, true); this.capacity = capacity; } /** * 重写removeEldestEntry()方法设置何时移除旧元素 * @param eldest * @return */ @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) { // 当元素个数大于了缓存的容量, 就移除元素 return size() &gt; this.capacity; }}","link":"/2016/07/11/LinkedHashMap内部原理/"},{"title":"HashSet内部原理","text":"问题集合（Collection）和集合（Set）有什么区别？HashSet怎么保证添加元素不重复？HashSet是否允许null元素？HashSet是有序的吗？HashSet是同步的吗？什么是fail-fast？实战另见 HashSet简介集合，这个概念有点模糊。广义上来讲，java中的集合是指java.util包下面的容器类，包括和Collection及Map相关的所有类。中义上来讲，我们一般说集合特指java集合中的Collection相关的类，不包含Map相关的类。狭义上来讲，数学上的集合是指不包含重复元素的容器，即集合中不存在两个相同的元素，在java里面对应Set。具体怎么来理解还是要看上下文环境。比如，面试别人让你说下java中的集合，这时候肯定是广义上的。再比如，下面我们讲的把另一个集合中的元素全部添加到Set中，这时候就是中义上的。HashSet是Set的一种实现方式，底层主要使用HashMap来确保元素不重复。源码分析属性12345// 内部使用HashMapprivate transient HashMap&lt;E,Object&gt; map;// 虚拟对象，用来作为value放到map中private static final Object PRESENT = new Object();构造方法123456789101112131415161718192021public HashSet() { map = new HashMap&lt;&gt;();}public HashSet(Collection&lt;? extends E&gt; c) { map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c);}public HashSet(int initialCapacity, float loadFactor) { map = new HashMap&lt;&gt;(initialCapacity, loadFactor);}public HashSet(int initialCapacity) { map = new HashMap&lt;&gt;(initialCapacity);}// 非public，主要是给LinkedHashSet使用的HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);}构造方法都是调用HashMap对应的构造方法。最后一个构造方法有点特殊，它不是public的，意味着它只能被同一个包或者子类调用，这是LinkedHashSet专属的方法。添加元素直接调用HashMap的put()方法，把元素本身作为key，把PRESENT作为value，也就是这个map中所有的value都是一样的。123public boolean add(E e) { return map.put(e, PRESENT)==null;}删除元素直接调用HashMap的remove()方法，注意map的remove返回是删除元素的value，而Set的remov返回的是boolean类型。这里要检查一下，如果是null的话说明没有该元素，如果不是null肯定等于PRESENT。123public boolean remove(Object o) { return map.remove(o)==PRESENT;}查询元素Set没有get()方法哦，因为get似乎没有意义，不像List那样可以按index获取元素。这里只要一个检查元素是否存在的方法contains()，直接调用map的containsKey()方法。123public boolean contains(Object o) { return map.containsKey(o);}遍历元素直接调用map的keySet的迭代器。123public Iterator&lt;E&gt; iterator() { return map.keySet().iterator();}全部源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165package java.util;import java.io.InvalidObjectException;import sun.misc.SharedSecrets;public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable{ static final long serialVersionUID = -5024744406713321676L; // 内部元素存储在HashMap中 private transient HashMap&lt;E,Object&gt; map; // 虚拟元素，用来存到map元素的value中的，没有实际意义 private static final Object PRESENT = new Object(); // 空构造方法 public HashSet() { map = new HashMap&lt;&gt;(); } // 把另一个集合的元素全都添加到当前Set中 // 注意，这里初始化map的时候是计算了它的初始容量的 public HashSet(Collection&lt;? extends E&gt; c) { map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); } // 指定初始容量和装载因子 public HashSet(int initialCapacity, float loadFactor) { map = new HashMap&lt;&gt;(initialCapacity, loadFactor); } // 只指定初始容量 public HashSet(int initialCapacity) { map = new HashMap&lt;&gt;(initialCapacity); } // LinkedHashSet专用的方法 // dummy是没有实际意义的, 只是为了跟上上面那个操持方法签名不同而已 HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } // 迭代器 public Iterator&lt;E&gt; iterator() { return map.keySet().iterator(); } // 元素个数 public int size() { return map.size(); } // 检查是否为空 public boolean isEmpty() { return map.isEmpty(); } // 检查是否包含某个元素 public boolean contains(Object o) { return map.containsKey(o); } // 添加元素 public boolean add(E e) { return map.put(e, PRESENT)==null; } // 删除元素 public boolean remove(Object o) { return map.remove(o)==PRESENT; } // 清空所有元素 public void clear() { map.clear(); } // 克隆方法 @SuppressWarnings(\"unchecked\") public Object clone() { try { HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; } catch (CloneNotSupportedException e) { throw new InternalError(e); } } // 序列化写出方法 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException { // 写出非static非transient属性 s.defaultWriteObject(); // 写出map的容量和装载因子 s.writeInt(map.capacity()); s.writeFloat(map.loadFactor()); // 写出元素个数 s.writeInt(map.size()); // 遍历写出所有元素 for (E e : map.keySet()) s.writeObject(e); } // 序列化读入方法 private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { // 读入非static非transient属性 s.defaultReadObject(); // 读入容量, 并检查不能小于0 int capacity = s.readInt(); if (capacity &lt; 0) { throw new InvalidObjectException(\"Illegal capacity: \" + capacity); } // 读入装载因子, 并检查不能小于等于0或者是NaN(Not a Number) // java.lang.Float.NaN = 0.0f / 0.0f; float loadFactor = s.readFloat(); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) { throw new InvalidObjectException(\"Illegal load factor: \" + loadFactor); } // 读入元素个数并检查不能小于0 int size = s.readInt(); if (size &lt; 0) { throw new InvalidObjectException(\"Illegal size: \" + size); } // 根据元素个数重新设置容量 // 这是为了保证map有足够的容量容纳所有元素, 防止无意义的扩容 capacity = (int) Math.min(size * Math.min(1 / loadFactor, 4.0f), HashMap.MAXIMUM_CAPACITY); // 再次检查某些东西, 不重要的代码忽视掉 SharedSecrets.getJavaOISAccess() .checkArray(s, Map.Entry[].class, HashMap.tableSizeFor(capacity)); // 创建map, 检查是不是LinkedHashSet类型 map = (((HashSet&lt;?&gt;)this) instanceof LinkedHashSet ? new LinkedHashMap&lt;E,Object&gt;(capacity, loadFactor) : new HashMap&lt;E,Object&gt;(capacity, loadFactor)); // 读入所有元素, 并放入map中 for (int i=0; i&lt;size; i++) { @SuppressWarnings(\"unchecked\") E e = (E) s.readObject(); map.put(e, PRESENT); } } // 可分割的迭代器, 主要用于多线程并行迭代处理时使用 public Spliterator&lt;E&gt; spliterator() { return new HashMap.KeySpliterator&lt;E,Object&gt;(map, 0, -1, 0, 0); }}总结HashSet内部使用HashMap的key存储元素，以此来保证元素不重复；HashSet是无序的，因为HashMap的key是无序的；HashSet中允许有一个null元素，因为HashMap允许key为null；HashSet是非线程安全的；HashSet是没有get()方法的；彩蛋阿里手册上有说，使用java中的集合时要自己指定集合的大小，通过这篇源码的分析，你知道初始化HashMap的时候初始容量怎么传吗？我们发现有下面这个构造方法，很清楚明白地告诉了我们怎么指定容量。假如，我们预估HashMap要存储n个元素，那么，它的容量就应该指定为((n/0.75f) + 1)，如果这个值小于16，那就直接使用16得了。初始化时指定容量是为了减少扩容的次数，提高效率。1234public HashSet(Collection&lt;? extends E&gt; c) { map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c);}什么是fail-fast？fail-fast机制是java集合中的一种错误机制。当使用迭代器迭代时，如果发现集合有修改，则快速失败做出响应，抛出ConcurrentModificationException异常。这种修改有可能是其它线程的修改，也有可能是当前线程自己的修改导致的，比如迭代的过程中直接调用remove()删除元素等。另外，并不是java中所有的集合都有fail-fast的机制。比如，像最终一致性的ConcurrentHashMap、CopyOnWriterArrayList等都是没有fast-fail的。那么，fail-fast是怎么实现的呢？细心的同学可能会发现，像ArrayList、HashMap中都有一个属性叫modCount，每次对集合的修改这个值都会加1，在遍历前记录这个值到expectedModCount中，遍历中检查两者是否一致，如果出现不一致就说明有修改，则抛出ConcurrentModificationException异常。","link":"/2017/02/01/HashSet内部原理/"},{"title":"TreeMap内部原理（三）","text":"删除元素删除元素本身比较简单，就是采用二叉树的删除规则。如果删除的位置有两个叶子节点，则从其右子树中取最小的元素放到删除的位置，然后把删除位置移到替代元素的位置，进入下一步。如果删除的位置只有一个叶子节点（有可能是经过第一步转换后的删除位置），则把那个叶子节点作为替代元素，放到删除的位置，然后把这个叶子节点删除。如果删除的位置没有叶子节点，则直接把这个删除位置的元素删除即可。针对红黑树，如果删除位置是黑色节点，还需要做再平衡。如果有替代元素，则以替代元素作为当前节点进入再平衡。如果没有替代元素，则以删除的位置的元素作为当前节点进入再平衡，平衡之后再删除这个节点。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public V remove(Object key) { // 获取节点 Entry&lt;K,V&gt; p = getEntry(key); if (p == null) return null; V oldValue = p.value; // 删除节点 deleteEntry(p); // 返回删除的value return oldValue;}private void deleteEntry(Entry&lt;K,V&gt; p) { // 修改次数加1 modCount++; // 元素个数减1 size--; if (p.left != null &amp;&amp; p.right != null) { // 如果当前节点既有左子节点，又有右子节点 // 取其右子树中最小的节点 Entry&lt;K,V&gt; s = successor(p); // 用右子树中最小节点的值替换当前节点的值 p.key = s.key; p.value = s.value; // 把右子树中最小节点设为当前节点 p = s; // 这种情况实际上并没有删除p节点，而是把p节点的值改了，实际删除的是p的后继节点 } // 如果原来的当前节点（p）有2个子节点，则当前节点已经变成原来p的右子树中的最小节点了，也就是说其没有左子节点了 // 到这一步，p肯定只有一个子节点了 // 如果当前节点有子节点，则用子节点替换当前节点 Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); if (replacement != null) { // 把替换节点直接放到当前节点的位置上（相当于删除了p，并把替换节点移动过来了） replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; // 将p的各项属性都设为空 p.left = p.right = p.parent = null; // 如果p是黑节点，则需要再平衡 if (p.color == BLACK) fixAfterDeletion(replacement); } else if (p.parent == null) { // 如果当前节点就是根节点，则直接将根节点设为空即可 root = null; } else { // 如果当前节点没有子节点且其为黑节点，则把自己当作虚拟的替换节点进行再平衡 if (p.color == BLACK) fixAfterDeletion(p); // 平衡完成后删除当前节点（与父节点断绝关系） if (p.parent != null) { if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } }}删除再平衡经过上面的处理，真正删除的肯定是黑色节点才会进入到再平衡阶段。因为删除的是黑色节点，导致整颗树不平衡了，所以这里我们假设把删除的黑色赋予当前节点，这样当前节点除了它自已的颜色还多了一个黑色，那么：如果当前节点是根节点，则直接涂黑即可，不需要再平衡；如果当前节点是红+黑节点，则直接涂黑即可，不需要平衡；如果当前节点是黑+黑节点，则我们只要通过旋转把这个多出来的黑色不断的向上传递到一个红色节点即可，这又可能会出现以下四种情况：（假设当前节点为父节点的左子节点）情况策略1）x是黑+黑节点，x的兄弟是红节点（1）将兄弟节点设为黑色；（2）将父节点设为红色；（3）以父节点为支点进行左旋；（4）重新设置x的兄弟节点，进入下一步；2）x是黑+黑节点，x的兄弟是黑节点，且兄弟节点的两个子节点都是黑色（1）将兄弟节点设置为红色；（2）将x的父节点作为新的当前节点，进入下一次循环；3）x是黑+黑节点，x的兄弟是黑节点，且兄弟节点的右子节点为黑色，左子节点为红色（1）将兄弟节点的左子节点设为黑色；（2）将兄弟节点设为红色；（3）以兄弟节点为支点进行右旋；（4）重新设置x的兄弟节点，进入下一步；3）x是黑+黑节点，x的兄弟是黑节点，且兄弟节点的右子节点为红色，左子节点任意颜色（1）将兄弟节点的颜色设为父节点的颜色；（2）将父节点设为黑色；（3）将兄弟节点的右子节点设为黑色；（4）以父节点为支点进行左旋；（5）将root作为新的当前节点（退出循环）；（假设当前节点为父节点的右子节点，正好反过来）情况策略1）x是黑+黑节点，x的兄弟是红节点（1）将兄弟节点设为黑色；（2）将父节点设为红色；（3）以父节点为支点进行右旋；（4）重新设置x的兄弟节点，进入下一步；2）x是黑+黑节点，x的兄弟是黑节点，且兄弟节点的两个子节点都是黑色（1）将兄弟节点设置为红色；（2）将x的父节点作为新的当前节点，进入下一次循环；3）x是黑+黑节点，x的兄弟是黑节点，且兄弟节点的左子节点为黑色，右子节点为红色（1）将兄弟节点的右子节点设为黑色；（2）将兄弟节点设为红色；（3）以兄弟节点为支点进行左旋；（4）重新设置x的兄弟节点，进入下一步；3）x是黑+黑节点，x的兄弟是黑节点，且兄弟节点的左子节点为红色，右子节点任意颜色（1）将兄弟节点的颜色设为父节点的颜色；（2）将父节点设为黑色；（3）将兄弟节点的左子节点设为黑色；（4）以父节点为支点进行右旋；（5）将root作为新的当前节点（退出循环）；让我们来看看TreeMap中的实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/** * 删除再平衡 *（1）每个节点或者是黑色，或者是红色。 *（2）根节点是黑色。 *（3）每个叶子节点（NIL）是黑色。（注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！） *（4）如果一个节点是红色的，则它的子节点必须是黑色的。 *（5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 */private void fixAfterDeletion(Entry&lt;K,V&gt; x) { // 只有当前节点不是根节点且当前节点是黑色时才进入循环 while (x != root &amp;&amp; colorOf(x) == BLACK) { if (x == leftOf(parentOf(x))) { // 如果当前节点是其父节点的左子节点 // sib是当前节点的兄弟节点 Entry&lt;K,V&gt; sib = rightOf(parentOf(x)); // 情况1）如果兄弟节点是红色 if (colorOf(sib) == RED) { // （1）将兄弟节点设为黑色 setColor(sib, BLACK); // （2）将父节点设为红色 setColor(parentOf(x), RED); // （3）以父节点为支点进行左旋 rotateLeft(parentOf(x)); // （4）重新设置x的兄弟节点，进入下一步 sib = rightOf(parentOf(x)); } if (colorOf(leftOf(sib)) == BLACK &amp;&amp; colorOf(rightOf(sib)) == BLACK) { // 情况2）如果兄弟节点的两个子节点都是黑色 // （1）将兄弟节点设置为红色 setColor(sib, RED); // （2）将x的父节点作为新的当前节点，进入下一次循环 x = parentOf(x); } else { if (colorOf(rightOf(sib)) == BLACK) { // 情况3）如果兄弟节点的右子节点为黑色 // （1）将兄弟节点的左子节点设为黑色 setColor(leftOf(sib), BLACK); // （2）将兄弟节点设为红色 setColor(sib, RED); // （3）以兄弟节点为支点进行右旋 rotateRight(sib); // （4）重新设置x的兄弟节点 sib = rightOf(parentOf(x)); } // 情况4） // （1）将兄弟节点的颜色设为父节点的颜色 setColor(sib, colorOf(parentOf(x))); // （2）将父节点设为黑色 setColor(parentOf(x), BLACK); // （3）将兄弟节点的右子节点设为黑色 setColor(rightOf(sib), BLACK); // （4）以父节点为支点进行左旋 rotateLeft(parentOf(x)); // （5）将root作为新的当前节点（退出循环） x = root; } } else { // symmetric // 如果当前节点是其父节点的右子节点 // sib是当前节点的兄弟节点 Entry&lt;K,V&gt; sib = leftOf(parentOf(x)); // 情况1）如果兄弟节点是红色 if (colorOf(sib) == RED) { // （1）将兄弟节点设为黑色 setColor(sib, BLACK); // （2）将父节点设为红色 setColor(parentOf(x), RED); // （3）以父节点为支点进行右旋 rotateRight(parentOf(x)); // （4）重新设置x的兄弟节点 sib = leftOf(parentOf(x)); } if (colorOf(rightOf(sib)) == BLACK &amp;&amp; colorOf(leftOf(sib)) == BLACK) { // 情况2）如果兄弟节点的两个子节点都是黑色 // （1）将兄弟节点设置为红色 setColor(sib, RED); // （2）将x的父节点作为新的当前节点，进入下一次循环 x = parentOf(x); } else { if (colorOf(leftOf(sib)) == BLACK) { // 情况3）如果兄弟节点的左子节点为黑色 // （1）将兄弟节点的右子节点设为黑色 setColor(rightOf(sib), BLACK); // （2）将兄弟节点设为红色 setColor(sib, RED); // （3）以兄弟节点为支点进行左旋 rotateLeft(sib); // （4）重新设置x的兄弟节点 sib = leftOf(parentOf(x)); } // 情况4） // （1）将兄弟节点的颜色设为父节点的颜色 setColor(sib, colorOf(parentOf(x))); // （2）将父节点设为黑色 setColor(parentOf(x), BLACK); // （3）将兄弟节点的左子节点设为黑色 setColor(leftOf(sib), BLACK); // （4）以父节点为支点进行右旋 rotateRight(parentOf(x)); // （5）将root作为新的当前节点（退出循环） x = root; } } } // 退出条件为多出来的黑色向上传递到了根节点或者红节点 // 则将x设为黑色即可满足红黑树规则 setColor(x, BLACK);}删除元素举例假设我们有下面这样一颗红黑树。我们删除6号元素，则从右子树中找到了最小元素7，7又没有子节点了，所以把7作为当前节点进行再平衡。我们看到7是黑节点，且其兄弟为黑节点，且其兄弟的两个子节点都是红色，满足情况4），平衡之后如下图所示。我们再删除7号元素，则从右子树中找到了最小元素8，8有子节点且为黑色，所以8的子节点9是替代节点，以9为当前节点进行再平衡。我们发现9是红节点，则直接把它涂成黑色即满足了红黑树的特性，不需要再过多的平衡了。这次我们来个狠的，把根节点删除，从右子树中找到了最小的元素5，5没有子节点，所以把5作为当前节点进行再平衡。我们看到5是黑节点，且其兄弟为红色，符合情况1），平衡之后如下图所示，然后进入情况2）。对情况2）进行再平衡后如下图所示。然后进入下一次循环，发现不符合循环条件了，直接把x涂为黑色即可，退出这个方法之后会把旧x删除掉（见deleteEntry()方法），最后的结果就是下面这样。","link":"/2016/07/06/TreeMap内部原理(三)/"},{"title":"TreeMap内部原理（二）","text":"插入元素插入元素，如果元素在树中存在，则替换value；如果元素不存在，则插入到对应的位置，再平衡树。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public V put(K key, V value) { Entry&lt;K,V&gt; t = root; if (t == null) { // 如果没有根节点，直接插入到根节点 compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; } // key比较的结果 int cmp; // 用来寻找待插入节点的父节点 Entry&lt;K,V&gt; parent; // 根据是否有comparator使用不同的分支 Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) { // 如果使用的是comparator方式，key值可以为null，只要在comparator.compare()中允许即可 // 从根节点开始遍历寻找 do { parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) // 如果小于0从左子树寻找 t = t.left; else if (cmp &gt; 0) // 如果大于0从右子树寻找 t = t.right; else // 如果等于0，说明插入的节点已经存在了，直接更换其value值并返回旧值 return t.setValue(value); } while (t != null); } else { // 如果使用的是Comparable方式，key不能为null if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; // 从根节点开始遍历寻找 do { parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) // 如果小于0从左子树寻找 t = t.left; else if (cmp &gt; 0) // 如果大于0从右子树寻找 t = t.right; else // 如果等于0，说明插入的节点已经存在了，直接更换其value值并返回旧值 return t.setValue(value); } while (t != null); } // 如果没找到，那么新建一个节点，并插入到树中 Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) // 如果小于0插入到左子节点 parent.left = e; else // 如果大于0插入到右子节点 parent.right = e; // 插入之后的平衡 fixAfterInsertion(e); // 元素个数加1（不需要扩容） size++; // 修改次数加1 modCount++; // 如果插入了新节点返回空 return null;}插入再平衡插入的元素默认都是红色，因为插入红色元素只违背了第4条特性，那么我们只要根据这个特性来平衡就容易多了。根据不同的情况有以下几种处理方式：插入的元素如果是根节点，则直接涂成黑色即可，不用平衡；插入的元素的父节点如果为黑色，不需要平衡；插入的元素的父节点如果为红色，则违背了特性4，需要平衡，平衡时又分成下面三种情况：（如果父节点是祖父节点的左节点）情况策略1）父节点为红色，叔叔节点也为红色（1）将父节点设为黑色；（2）将叔叔节点设为黑色；（3）将祖父节点设为红色；（4）将祖父节点设为新的当前节点，进入下一次循环判断；2）父节点为红色，叔叔节点为黑色，且当前节点是其父节点的右节点（1）将父节点作为新的当前节点；（2）以新当节点为支点进行左旋，进入情况3）；3）父节点为红色，叔叔节点为黑色，且当前节点是其父节点的左节点（1）将父节点设为黑色；（2）将祖父节点设为红色；（3）以祖父节点为支点进行右旋，进入下一次循环判断；（如果父节点是祖父节点的右节点，则正好与上面反过来）情况策略1）父节点为红色，叔叔节点也为红色（1）将父节点设为黑色；（2）将叔叔节点设为黑色；（3）将祖父节点设为红色；（4）将祖父节点设为新的当前节点，进入下一次循环判断；2）父节点为红色，叔叔节点为黑色，且当前节点是其父节点的左节点（1）将父节点作为新的当前节点；（2）以新当节点为支点进行右旋；3）父节点为红色，叔叔节点为黑色，且当前节点是其父节点的右节点（1）将父节点设为黑色；（2）将祖父节点设为红色；（3）以祖父节点为支点进行左旋，进入下一次循环判断；让我们来看看TreeMap中的实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 插入再平衡 *（1）每个节点或者是黑色，或者是红色。 *（2）根节点是黑色。 *（3）每个叶子节点（NIL）是黑色。（注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！） *（4）如果一个节点是红色的，则它的子节点必须是黑色的。 *（5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 */private void fixAfterInsertion(Entry&lt;K,V&gt; x) { // 插入的节点为红节点，x为当前节点 x.color = RED; // 只有当插入节点不是根节点且其父节点为红色时才需要平衡（违背了特性4） while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) { if (parentOf(x) == leftOf(parentOf(parentOf(x)))) { // a）如果父节点是祖父节点的左节点 // y为叔叔节点 Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { // 情况1）如果叔叔节点为红色 // （1）将父节点设为黑色 setColor(parentOf(x), BLACK); // （2）将叔叔节点设为黑色 setColor(y, BLACK); // （3）将祖父节点设为红色 setColor(parentOf(parentOf(x)), RED); // （4）将祖父节点设为新的当前节点 x = parentOf(parentOf(x)); } else { // 如果叔叔节点为黑色 // 情况2）如果当前节点为其父节点的右节点 if (x == rightOf(parentOf(x))) { // （1）将父节点设为当前节点 x = parentOf(x); // （2）以新当前节点左旋 rotateLeft(x); } // 情况3）如果当前节点为其父节点的左节点（如果是情况2）则左旋之后新当前节点正好为其父节点的左节点了） // （1）将父节点设为黑色 setColor(parentOf(x), BLACK); // （2）将祖父节点设为红色 setColor(parentOf(parentOf(x)), RED); // （3）以祖父节点为支点进行右旋 rotateRight(parentOf(parentOf(x))); } } else { // b）如果父节点是祖父节点的右节点 // y是叔叔节点 Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { // 情况1）如果叔叔节点为红色 // （1）将父节点设为黑色 setColor(parentOf(x), BLACK); // （2）将叔叔节点设为黑色 setColor(y, BLACK); // （3）将祖父节点设为红色 setColor(parentOf(parentOf(x)), RED); // （4）将祖父节点设为新的当前节点 x = parentOf(parentOf(x)); } else { // 如果叔叔节点为黑色 // 情况2）如果当前节点为其父节点的左节点 if (x == leftOf(parentOf(x))) { // （1）将父节点设为当前节点 x = parentOf(x); // （2）以新当前节点右旋 rotateRight(x); } // 情况3）如果当前节点为其父节点的右节点（如果是情况2）则右旋之后新当前节点正好为其父节点的右节点了） // （1）将父节点设为黑色 setColor(parentOf(x), BLACK); // （2）将祖父节点设为红色 setColor(parentOf(parentOf(x)), RED); // （3）以祖父节点为支点进行左旋 rotateLeft(parentOf(parentOf(x))); } } } // 平衡完成后将根节点设为黑色 root.color = BLACK;}插入元素举例我们依次向红黑树中插入 4、2、3 三个元素，来一起看看整个红黑树平衡的过程。三个元素都插入完成后，符合父节点是祖父节点的左节点，叔叔节点为黑色，且当前节点是其父节点的右节点，即情况2）。情况2）需要做以下两步处理：将父节点作为新的当前节点；以新当节点为支点进行左旋，进入情况3）；情况3）需要做以下三步处理：将父节点设为黑色；将祖父节点设为红色；以祖父节点为支点进行右旋，进入下一次循环判断；下一次循环不符合父节点为红色了，退出循环，插入再平衡完成。","link":"/2016/07/03/TreeMap内部原理(二)/"},{"title":"WeakHashMap内部原理","text":"简介WeakHashMap是一种弱引用map，内部的key会存储为弱引用，当jvm gc的时候，如果这些key没有强引用存在的话，会被gc回收掉，下一次当我们操作map的时候会把对应的Entry整个删除掉，基于这种特性，WeakHashMap特别适用于缓存处理。继承体系可见，WeakHashMap没有实现Clone和Serializable接口，所以不具有克隆和序列化的特性。存储结构WeakHashMap因为gc的时候会把没有强引用的key回收掉，所以注定了它里面的元素不会太多，因此也就不需要像HashMap那样元素多的时候转化为红黑树来处理了。因此，WeakHashMap的存储结构只有（数组 + 链表）。源码解析属性123456789101112131415161718192021222324252627282930313233343536373839/** * 默认初始容量为16 */private static final int DEFAULT_INITIAL_CAPACITY = 16;/** * 最大容量为2的30次方 */private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 默认装载因子 */private static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 桶 */Entry&lt;K,V&gt;[] table;/** * 元素个数 */private int size;/** * 扩容门槛，等于capacity * loadFactor */private int threshold;/** * 装载因子 */private final float loadFactor;/** * 引用队列，当弱键失效的时候会把Entry添加到这个队列中 */private final ReferenceQueue&lt;Object&gt; queue = new ReferenceQueue&lt;&gt;();容量容量为数组的长度，亦即桶的个数，默认为16，最大为2的30次方，当容量达到64时才可以树化。装载因子装载因子用来计算容量达到多少时才进行扩容，默认装载因子为0.75。引用队列当弱键失效的时候会把Entry添加到这个队列中，当下次访问map的时候会把失效的Entry清除掉。Entry内部类WeakHashMap内部的存储节点, 没有key属性。1234567891011121314151617181920212223242526272829303132333435private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; { // 可以发现没有key, 因为key是作为弱引用存到Referen类中 V value; final int hash; Entry&lt;K,V&gt; next; Entry(Object key, V value, ReferenceQueue&lt;Object&gt; queue, int hash, Entry&lt;K,V&gt; next) { // 调用WeakReference的构造方法初始化key和引用队列 super(key, queue); this.value = value; this.hash = hash; this.next = next; }}public class WeakReference&lt;T&gt; extends Reference&lt;T&gt; { public WeakReference(T referent, ReferenceQueue&lt;? super T&gt; q) { // 调用Reference的构造方法初始化key和引用队列 super(referent, q); }}public abstract class Reference&lt;T&gt; { // 实际存储key的地方 private T referent; /* Treated specially by GC */ // 引用队列 volatile ReferenceQueue&lt;? super T&gt; queue; Reference(T referent, ReferenceQueue&lt;? super T&gt; queue) { this.referent = referent; this.queue = (queue == null) ? ReferenceQueue.NULL : queue; }}从Entry的构造方法我们知道，key和queue最终会传到到Reference的构造方法中，这里的key就是Reference的referent属性，它会被gc特殊对待，即当没有强引用存在时，当下一次gc的时候会被清除。构造方法1234567891011121314151617181920212223242526272829303132public WeakHashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal Initial Capacity: \"+ initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal Load factor: \"+ loadFactor); int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; table = newTable(capacity); this.loadFactor = loadFactor; threshold = (int)(capacity * loadFactor);}public WeakHashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR);}public WeakHashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);}public WeakHashMap(Map&lt;? extends K, ? extends V&gt; m) { this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); putAll(m);}构造方法与HashMap基本类似，初始容量为大于等于传入容量最近的2的n次方，扩容门槛threshold等于capacity * loadFactor。put(K key, V value)方法添加元素的方法。123456789101112131415161718192021222324252627282930public V put(K key, V value) { // 如果key为空，用空对象代替 Object k = maskNull(key); // 计算key的hash值 int h = hash(k); // 获取桶 Entry&lt;K,V&gt;[] tab = getTable(); // 计算元素在哪个桶中，h &amp; (length-1) int i = indexFor(h, tab.length); // 遍历桶对应的链表 for (Entry&lt;K,V&gt; e = tab[i]; e != null; e = e.next) { if (h == e.hash &amp;&amp; eq(k, e.get())) { // 如果找到了元素就使用新值替换旧值，并返回旧值 V oldValue = e.value; if (value != oldValue) e.value = value; return oldValue; } } modCount++; // 如果没找到就把新值插入到链表的头部 Entry&lt;K,V&gt; e = tab[i]; tab[i] = new Entry&lt;&gt;(k, value, queue, h, e); // 如果插入元素后数量达到了扩容门槛就把桶的数量扩容为2倍大小 if (++size &gt;= threshold) resize(tab.length * 2); return null;}计算hash；这里与HashMap有所不同，HashMap中如果key为空直接返回0，这里是用空对象来计算的。另外打散方式也不同，HashMap只用了一次异或，这里用了四次，HashMap给出的解释是一次够了，而且就算冲突了也会转换成红黑树，对效率没什么影响。计算在哪个桶中；遍历桶对应的链表；如果找到元素就用新值替换旧值，并返回旧值；如果没找到就在链表头部插入新元素；HashMap就插入到链表尾部。如果元素数量达到了扩容门槛，就把容量扩大到2倍大小；HashMap中是大于threshold才扩容，这里等于threshold就开始扩容了。resize(int newCapacity)方法扩容方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657void resize(int newCapacity) { // 获取旧桶，getTable()的时候会剔除失效的Entry Entry&lt;K,V&gt;[] oldTable = getTable(); // 旧容量 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } // 新桶 Entry&lt;K,V&gt;[] newTable = newTable(newCapacity); // 把元素从旧桶转移到新桶 transfer(oldTable, newTable); // 把新桶赋值桶变量 table = newTable; /* * If ignoring null elements and processing ref queue caused massive * shrinkage, then restore old table. This should be rare, but avoids * unbounded expansion of garbage-filled tables. */ // 如果元素个数大于扩容门槛的一半，则使用新桶和新容量，并计算新的扩容门槛 if (size &gt;= threshold / 2) { threshold = (int)(newCapacity * loadFactor); } else { // 否则把元素再转移回旧桶，还是使用旧桶 // 因为在transfer的时候会清除失效的Entry，所以元素个数可能没有那么大了，就不需要扩容了 expungeStaleEntries(); transfer(newTable, oldTable); table = oldTable; }}private void transfer(Entry&lt;K,V&gt;[] src, Entry&lt;K,V&gt;[] dest) { // 遍历旧桶 for (int j = 0; j &lt; src.length; ++j) { Entry&lt;K,V&gt; e = src[j]; src[j] = null; while (e != null) { Entry&lt;K,V&gt; next = e.next; Object key = e.get(); // 如果key等于了null就清除，说明key被gc清理掉了，则把整个Entry清除 if (key == null) { e.next = null; // Help GC e.value = null; // \" \" size--; } else { // 否则就计算在新桶中的位置并把这个元素放在新桶对应链表的头部 int i = indexFor(e.hash, dest.length); e.next = dest[i]; dest[i] = e; } e = next; } }}判断旧容量是否达到最大容量；新建新桶并把元素全部转移到新桶中；如果转移后元素个数不到扩容门槛的一半，则把元素再转移回旧桶，继续使用旧桶，说明不需要扩容；否则使用新桶，并计算新的扩容门槛；转移元素的过程中会把key为null的元素清除掉，所以size会变小；get(Object key)方法获取元素。123456789101112131415public V get(Object key) { Object k = maskNull(key); // 计算hash int h = hash(k); Entry&lt;K,V&gt;[] tab = getTable(); int index = indexFor(h, tab.length); Entry&lt;K,V&gt; e = tab[index]; // 遍历链表，找到了就返回 while (e != null) { if (e.hash == h &amp;&amp; eq(k, e.get())) return e.value; e = e.next; } return null;}计算hash值；遍历所在桶对应的链表；如果找到了就返回元素的value值；如果没找到就返回空；remove(Object key)方法移除元素。1234567891011121314151617181920212223242526272829303132public V remove(Object key) { Object k = maskNull(key); // 计算hash int h = hash(k); Entry&lt;K,V&gt;[] tab = getTable(); int i = indexFor(h, tab.length); // 元素所在的桶的第一个元素 Entry&lt;K,V&gt; prev = tab[i]; Entry&lt;K,V&gt; e = prev; // 遍历链表 while (e != null) { Entry&lt;K,V&gt; next = e.next; if (h == e.hash &amp;&amp; eq(k, e.get())) { // 如果找到了就删除元素 modCount++; size--; if (prev == e) // 如果是头节点，就把头节点指向下一个节点 tab[i] = next; else // 如果不是头节点，删除该节点 prev.next = next; return e.value; } prev = e; e = next; } return null;}计算hash；找到所在的桶；遍历桶对应的链表；如果找到了就删除该节点，并返回该节点的value值；如果没找到就返回null；expungeStaleEntries()方法剔除失效的Entry。1234567891011121314151617181920212223242526272829303132private void expungeStaleEntries() { // 遍历引用队列 for (Object x; (x = queue.poll()) != null; ) { synchronized (queue) { @SuppressWarnings(\"unchecked\") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); // 找到所在的桶 Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; // 遍历链表 while (p != null) { Entry&lt;K,V&gt; next = p.next; // 找到该元素 if (p == e) { // 删除该元素 if (prev == e) table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator e.value = null; // Help GC size--; break; } prev = p; p = next; } } }}当key失效的时候gc会自动把对应的Entry添加到这个引用队列中；所有对map的操作都会直接或间接地调用到这个方法先移除失效的Entry，比如getTable()、size()、resize()；这个方法的目的就是遍历引用队列，并把其中保存的Entry从map中移除掉，具体的过程请看类注释；从这里可以看到移除Entry的同时把value也一并置为null帮助gc清理元素，防御性编程。使用案例说了这么多，不举个使用的例子怎么过得去。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.coolcoding.code;import java.util.Map;import java.util.WeakHashMap;public class WeakHashMapTest {public static void main(String[] args) { Map&lt;String, Integer&gt; map = new WeakHashMap&lt;&gt;(3); // 放入3个new String()声明的字符串 map.put(new String(\"1\"), 1); map.put(new String(\"2\"), 2); map.put(new String(\"3\"), 3); // 放入不用new String()声明的字符串 map.put(\"6\", 6); // 使用key强引用\"3\"这个字符串 String key = null; for (String s : map.keySet()) { // 这个\"3\"和new String(\"3\")不是一个引用 if (s.equals(\"3\")) { key = s; } } // 输出{6=6, 1=1, 2=2, 3=3}，未gc所有key都可以打印出来 System.out.println(map); // gc一下 System.gc(); // 放一个new String()声明的字符串 map.put(new String(\"4\"), 4); // 输出{4=4, 6=6, 3=3}，gc后放入的值和强引用的key可以打印出来 System.out.println(map); // key与\"3\"的引用断裂 key = null; // gc一下 System.gc(); // 输出{6=6}，gc后强引用的key可以打印出来 System.out.println(map);}}在这里通过new String()声明的变量才是弱引用，使用”6”这种声明方式会一直存在于常量池中，不会被清理，所以”6”这个元素会一直在map里面，其它的元素随着gc都会被清理掉。总结WeakHashMap使用（数组 + 链表）存储结构；WeakHashMap中的key是弱引用，gc的时候会被清除；每次对map的操作都会剔除失效key对应的Entry；使用String作为key时，一定要使用new String()这样的方式声明key，才会失效，其它的基本类型的包装类型是一样的；WeakHashMap常用来作为缓存使用；彩蛋强、软、弱、虚引用知多少？强引用使用最普遍的引用。如果一个对象具有强引用，它绝对不会被gc回收。如果内存空间不足了，gc宁愿抛出OutOfMemoryError，也不是会回收具有强引用的对象。软引用如果一个对象只具有软引用，则内存空间足够时不会回收它，但内存空间不够时就会回收这部分对象。只要这个具有软引用对象没有被回收，程序就可以正常使用。弱引用如果一个对象只具有弱引用，则不管内存空间够不够，当gc扫描到它时就会回收它。虚引用如果一个对象只具有虚引用，那么它就和没有任何引用一样，任何时候都可能被gc回收。软（弱、虚）引用必须和一个引用队列（ReferenceQueue）一起使用，当gc回收这个软（弱、虚）引用引用的对象时，会把这个软（弱、虚）引用放到这个引用队列中。比如，上述的Entry是一个弱引用，它引用的对象是key，当key被回收时，Entry会被放到queue中。","link":"/2016/08/11/WeakHashMap内部原理/"},{"title":"SparkML 导出 JPMML","text":"提示: 本文是模型部署方案的一部分依赖spark 2.2jpmml-sparkml 1.3.8scala 2.11步骤使用spark ml训练一个决策树模型在控制台验证可以输出后，写到文件中我们开始吧maven依赖1234567891011121314151617181920212223242526272829303132333435&lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.11.12&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt; &lt;spark-core.version&gt;2.2.0&lt;/spark-core.version&gt; &lt;jpmml-sparkml.version&gt;1.3.8&lt;/jpmml-sparkml.version&gt;&lt;/properties&gt;&lt;!--整合jpmml--&gt;&lt;dependency&gt; &lt;groupId&gt;org.jpmml&lt;/groupId&gt; &lt;artifactId&gt;jpmml-sparkml&lt;/artifactId&gt; &lt;version&gt;${jpmml-sparkml.version}&lt;/version&gt;&lt;/dependency&gt;&lt;!--spark mllib--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;version&gt;${spark-core.version}&lt;/version&gt;&lt;/dependency&gt;&lt;!--spark--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_${scala.compat.version}&lt;/artifactId&gt; &lt;version&gt;${spark-core.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;${spark-core.version}&lt;/version&gt;&lt;/dependency&gt;模型训练 &amp; 输出12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package org.houqian.spark.jpmmlimport java.io.Fileimport org.apache.spark.ml.classification.DecisionTreeClassifierimport org.apache.spark.ml.feature.RFormulaimport org.apache.spark.ml.{Pipeline, PipelineStage}import org.apache.spark.sql.SparkSessionimport org.jpmml.model.JAXBUtil/** * @author : houqian * @version : 1.0 * @since : 2018-08-30 */object Pipeline { def main(args: Array[String]): Unit = { val spark = SparkSession .builder .appName(\"Pipeline\") .master(\"local[4]\") .getOrCreate() // 加载训练集 val irisData = spark .read .format(\"csv\") .option(\"header\", \"true\") .load(\"file:///Users/houqian/repo/github/data-notebook/src/main/resources/Iris.csv\") irisData.show() // 特征选择 val formula = new RFormula().setFormula(\"Species ~ .\") // 设置决策树分类器 val classifier = new DecisionTreeClassifier().setLabelCol(formula.getLabelCol).setFeaturesCol(formula.getFeaturesCol) // 组合pipeline val pipeline = new Pipeline().setStages(Array[PipelineStage](formula, classifier)) // 训练 val pipelineModel = pipeline.fit(irisData) import javax.xml.transform.stream.StreamResult import org.jpmml.sparkml.PMMLBuilder val schema = irisData.schema val pmml = new PMMLBuilder(schema, pipelineModel).build // 将pmml以流的形式输出到控制台 JAXBUtil.marshalPMML(pmml, new StreamResult(System.out)) // 将pmml写到文件 new PMMLBuilder(schema, pipelineModel).buildFile(new File(\"/Users/houqian/repo/github/data-notebook/src/main/resources/pipeline.pmml\")) }}运行，控制台输出：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;&lt;PMML xmlns=\"http://www.dmg.org/PMML-4_3\" xmlns:data=\"http://jpmml.org/jpmml-model/InlineTable\" version=\"4.3\"&gt; &lt;Header&gt; &lt;Application name=\"JPMML-SparkML\" version=\"1.5.3\"/&gt; &lt;Timestamp&gt;2018-08-30T14:03:52Z&lt;/Timestamp&gt; &lt;/Header&gt; &lt;DataDictionary&gt; &lt;DataField name=\"Species\" optype=\"categorical\" dataType=\"string\"&gt; &lt;Value value=\"versicolor\"/&gt; &lt;Value value=\"virginica\"/&gt; &lt;Value value=\"setosa\"/&gt; &lt;/DataField&gt; &lt;DataField name=\"Petal_Width\" optype=\"categorical\" dataType=\"string\"&gt; &lt;Value value=\"0.2\"/&gt; &lt;Value value=\"1.3\"/&gt; &lt;Value value=\"1.5\"/&gt; &lt;Value value=\"1.8\"/&gt; &lt;Value value=\"2.3\"/&gt; &lt;Value value=\"1.4\"/&gt; &lt;Value value=\"0.4\"/&gt; &lt;Value value=\"1\"/&gt; &lt;Value value=\"0.3\"/&gt; &lt;Value value=\"2.1\"/&gt; &lt;Value value=\"2\"/&gt; &lt;Value value=\"0.1\"/&gt; &lt;Value value=\"1.9\"/&gt; &lt;Value value=\"1.2\"/&gt; &lt;Value value=\"1.6\"/&gt; &lt;Value value=\"2.4\"/&gt; &lt;Value value=\"1.1\"/&gt; &lt;Value value=\"2.5\"/&gt; &lt;Value value=\"2.2\"/&gt; &lt;Value value=\"1.7\"/&gt; &lt;Value value=\"0.5\"/&gt; &lt;Value value=\"0.6\"/&gt; &lt;/DataField&gt; &lt;/DataDictionary&gt; &lt;TreeModel functionName=\"classification\" missingValueStrategy=\"nullPrediction\" splitCharacteristic=\"multiSplit\"&gt; &lt;MiningSchema&gt; &lt;MiningField name=\"Species\" usageType=\"target\"/&gt; &lt;MiningField name=\"Petal_Width\"/&gt; &lt;/MiningSchema&gt; &lt;Output&gt; &lt;OutputField name=\"pmml(prediction)\" optype=\"categorical\" dataType=\"string\" feature=\"predictedValue\" isFinalResult=\"false\"/&gt; &lt;OutputField name=\"prediction\" optype=\"categorical\" dataType=\"double\" feature=\"transformedValue\"&gt; &lt;MapValues outputColumn=\"data:output\" dataType=\"double\"&gt; &lt;FieldColumnPair field=\"pmml(prediction)\" column=\"data:input\"/&gt; &lt;InlineTable&gt; &lt;row&gt; &lt;data:input&gt;versicolor&lt;/data:input&gt; &lt;data:output&gt;0&lt;/data:output&gt; &lt;/row&gt; &lt;row&gt; &lt;data:input&gt;virginica&lt;/data:input&gt; &lt;data:output&gt;1&lt;/data:output&gt; &lt;/row&gt; &lt;row&gt; &lt;data:input&gt;setosa&lt;/data:input&gt; &lt;data:output&gt;2&lt;/data:output&gt; &lt;/row&gt; &lt;/InlineTable&gt; &lt;/MapValues&gt; &lt;/OutputField&gt; &lt;OutputField name=\"probability(versicolor)\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"versicolor\"/&gt; &lt;OutputField name=\"probability(virginica)\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"virginica\"/&gt; &lt;OutputField name=\"probability(setosa)\" optype=\"continuous\" dataType=\"double\" feature=\"probability\" value=\"setosa\"/&gt; &lt;/Output&gt; &lt;Node&gt; &lt;True/&gt; &lt;Node score=\"setosa\" recordCount=\"29\"&gt; &lt;SimplePredicate field=\"Petal_Width\" operator=\"equal\" value=\"0.2\"/&gt; &lt;ScoreDistribution value=\"versicolor\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"virginica\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"setosa\" recordCount=\"29.0\"/&gt; &lt;/Node&gt; &lt;Node score=\"versicolor\" recordCount=\"13\"&gt; &lt;SimplePredicate field=\"Petal_Width\" operator=\"equal\" value=\"1.3\"/&gt; &lt;ScoreDistribution value=\"versicolor\" recordCount=\"13.0\"/&gt; &lt;ScoreDistribution value=\"virginica\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"setosa\" recordCount=\"0.0\"/&gt; &lt;/Node&gt; &lt;Node score=\"setosa\" recordCount=\"7\"&gt; &lt;SimplePredicate field=\"Petal_Width\" operator=\"equal\" value=\"0.4\"/&gt; &lt;ScoreDistribution value=\"versicolor\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"virginica\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"setosa\" recordCount=\"7.0\"/&gt; &lt;/Node&gt; &lt;Node score=\"setosa\" recordCount=\"7\"&gt; &lt;SimplePredicate field=\"Petal_Width\" operator=\"equal\" value=\"0.3\"/&gt; &lt;ScoreDistribution value=\"versicolor\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"virginica\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"setosa\" recordCount=\"7.0\"/&gt; &lt;/Node&gt; &lt;Node score=\"setosa\" recordCount=\"5\"&gt; &lt;SimplePredicate field=\"Petal_Width\" operator=\"equal\" value=\"0.1\"/&gt; &lt;ScoreDistribution value=\"versicolor\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"virginica\" recordCount=\"0.0\"/&gt; &lt;ScoreDistribution value=\"setosa\" recordCount=\"5.0\"/&gt; &lt;/Node&gt; &lt;Node score=\"virginica\" recordCount=\"89\"&gt; &lt;True/&gt; &lt;ScoreDistribution value=\"versicolor\" recordCount=\"37.0\"/&gt; &lt;ScoreDistribution value=\"virginica\" recordCount=\"50.0\"/&gt; &lt;ScoreDistribution value=\"setosa\" recordCount=\"2.0\"/&gt; &lt;/Node&gt; &lt;/Node&gt; &lt;/TreeModel&gt;&lt;/PMML&gt;我们成功生成的pmml文件：参考https://openscoring.io/blog/2018/07/09/converting_sparkml_pipeline_pmml/https://github.com/jpmml/jpmml-sparkml用到的训练集Iris.csv：https://github.com/jpmml/jpmml-sparkml/blob/1.3.X/src/test/resources/csv/Iris.csv","link":"/2018/08/30/sparkml导出jpmml/"},{"title":"TreeSet内部原理","text":"问题TreeSet真的是使用TreeMap来存储元素的吗？TreeSet是有序的吗？TreeSet和LinkedHashSet有何不同？简介TreeSet底层是采用TreeMap实现的一种Set，所以它是有序的，同样也是非线程安全的。实战另见 TreeSet源码分析经过前面我们学习HashSet和LinkedHashSet，基本上已经掌握了Set实现的套路了。所以，也不废话了，直接上源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243package java.util;// TreeSet实现了NavigableSet接口，所以它是有序的public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable{ // 元素存储在NavigableMap中 // 注意它不一定就是TreeMap private transient NavigableMap&lt;E,Object&gt; m; // 虚拟元素, 用来作为value存储在map中 private static final Object PRESENT = new Object(); // 直接使用传进来的NavigableMap存储元素 // 这里不是深拷贝,如果外面的map有增删元素也会反映到这里 // 而且, 这个方法不是public的, 说明只能给同包使用 TreeSet(NavigableMap&lt;E,Object&gt; m) { this.m = m; } // 使用TreeMap初始化 public TreeSet() { this(new TreeMap&lt;E,Object&gt;()); } // 使用带comparator的TreeMap初始化 public TreeSet(Comparator&lt;? super E&gt; comparator) { this(new TreeMap&lt;&gt;(comparator)); } // 将集合c中的所有元素添加的TreeSet中 public TreeSet(Collection&lt;? extends E&gt; c) { this(); addAll(c); } // 将SortedSet中的所有元素添加到TreeSet中 public TreeSet(SortedSet&lt;E&gt; s) { this(s.comparator()); addAll(s); } // 迭代器 public Iterator&lt;E&gt; iterator() { return m.navigableKeySet().iterator(); } // 逆序迭代器 public Iterator&lt;E&gt; descendingIterator() { return m.descendingKeySet().iterator(); } // 以逆序返回一个新的TreeSet public NavigableSet&lt;E&gt; descendingSet() { return new TreeSet&lt;&gt;(m.descendingMap()); } // 元素个数 public int size() { return m.size(); } // 判断是否为空 public boolean isEmpty() { return m.isEmpty(); } // 判断是否包含某元素 public boolean contains(Object o) { return m.containsKey(o); } // 添加元素, 调用map的put()方法, value为PRESENT public boolean add(E e) { return m.put(e, PRESENT)==null; } // 删除元素 public boolean remove(Object o) { return m.remove(o)==PRESENT; } // 清空所有元素 public void clear() { m.clear(); } // 添加集合c中的所有元素 public boolean addAll(Collection&lt;? extends E&gt; c) { // 满足一定条件时直接调用TreeMap的addAllForTreeSet()方法添加元素 if (m.size()==0 &amp;&amp; c.size() &gt; 0 &amp;&amp; c instanceof SortedSet &amp;&amp; m instanceof TreeMap) { SortedSet&lt;? extends E&gt; set = (SortedSet&lt;? extends E&gt;) c; TreeMap&lt;E,Object&gt; map = (TreeMap&lt;E, Object&gt;) m; Comparator&lt;?&gt; cc = set.comparator(); Comparator&lt;? super E&gt; mc = map.comparator(); if (cc==mc || (cc != null &amp;&amp; cc.equals(mc))) { map.addAllForTreeSet(set, PRESENT); return true; } } // 不满足上述条件, 调用父类的addAll()通过遍历的方式一个一个地添加元素 return super.addAll(c); } // 子set（NavigableSet中的方法） public NavigableSet&lt;E&gt; subSet(E fromElement, boolean fromInclusive, E toElement, boolean toInclusive) { return new TreeSet&lt;&gt;(m.subMap(fromElement, fromInclusive, toElement, toInclusive)); } // 头set（NavigableSet中的方法） public NavigableSet&lt;E&gt; headSet(E toElement, boolean inclusive) { return new TreeSet&lt;&gt;(m.headMap(toElement, inclusive)); } // 尾set（NavigableSet中的方法） public NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) { return new TreeSet&lt;&gt;(m.tailMap(fromElement, inclusive)); } // 子set（SortedSet接口中的方法） public SortedSet&lt;E&gt; subSet(E fromElement, E toElement) { return subSet(fromElement, true, toElement, false); } // 头set（SortedSet接口中的方法） public SortedSet&lt;E&gt; headSet(E toElement) { return headSet(toElement, false); } // 尾set（SortedSet接口中的方法） public SortedSet&lt;E&gt; tailSet(E fromElement) { return tailSet(fromElement, true); } // 比较器 public Comparator&lt;? super E&gt; comparator() { return m.comparator(); } // 返回最小的元素 public E first() { return m.firstKey(); } // 返回最大的元素 public E last() { return m.lastKey(); } // 返回小于e的最大的元素 public E lower(E e) { return m.lowerKey(e); } // 返回小于等于e的最大的元素 public E floor(E e) { return m.floorKey(e); } // 返回大于等于e的最小的元素 public E ceiling(E e) { return m.ceilingKey(e); } // 返回大于e的最小的元素 public E higher(E e) { return m.higherKey(e); } // 弹出最小的元素 public E pollFirst() { Map.Entry&lt;E,?&gt; e = m.pollFirstEntry(); return (e == null) ? null : e.getKey(); } public E pollLast() { Map.Entry&lt;E,?&gt; e = m.pollLastEntry(); return (e == null) ? null : e.getKey(); } // 克隆方法 @SuppressWarnings(\"unchecked\") public Object clone() { TreeSet&lt;E&gt; clone; try { clone = (TreeSet&lt;E&gt;) super.clone(); } catch (CloneNotSupportedException e) { throw new InternalError(e); } clone.m = new TreeMap&lt;&gt;(m); return clone; } // 序列化写出方法 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException { // Write out any hidden stuff s.defaultWriteObject(); // Write out Comparator s.writeObject(m.comparator()); // Write out size s.writeInt(m.size()); // Write out all elements in the proper order. for (E e : m.keySet()) s.writeObject(e); } // 序列化写入方法 private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { // Read in any hidden stuff s.defaultReadObject(); // Read in Comparator @SuppressWarnings(\"unchecked\") Comparator&lt;? super E&gt; c = (Comparator&lt;? super E&gt;) s.readObject(); // Create backing TreeMap TreeMap&lt;E,Object&gt; tm = new TreeMap&lt;&gt;(c); m = tm; // Read in size int size = s.readInt(); tm.readTreeSet(size, s, PRESENT); } // 可分割的迭代器 public Spliterator&lt;E&gt; spliterator() { return TreeMap.keySpliteratorFor(m); } // 序列化id private static final long serialVersionUID = -2479143000061671589L;}源码比较简单，基本都是调用map相应的方法。总结TreeSet底层使用NavigableMap存储元素；TreeSet是有序的；TreeSet是非线程安全的；TreeSet实现了NavigableSet接口，而NavigableSet继承自SortedSet接口；TreeSet实现了SortedSet接口；（彤哥年轻的时候面试被问过TreeSet和SortedSet的区别^^）彩蛋通过之前的学习，我们知道TreeSet和LinkedHashSet都是有序的，那它们有何不同？LinkedHashSet并没有实现SortedSet接口，它的有序性主要依赖于LinkedHashMap的有序性，所以它的有序性是指按照插入顺序保证的有序性；而TreeSet实现了SortedSet接口，它的有序性主要依赖于NavigableMap的有序性，而NavigableMap又继承自SortedMap，这个接口的有序性是指按照key的自然排序保证的有序性，而key的自然排序又有两种实现方式，一种是key实现Comparable接口，一种是构造方法传入Comparator比较器。TreeSet里面真的是使用TreeMap来存储元素的吗？通过源码分析我们知道TreeSet里面实际上是使用的NavigableMap来存储元素，虽然大部分时候这个map确实是TreeMap，但不是所有时候都是TreeMap。因为有一个构造方法是TreeSet(NavigableMap&lt;E,Object&gt; m)，而且这是一个非public方法，通过调用关系我们可以发现这个构造方法都是在自己类中使用的，比如下面这个：123public NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) { return new TreeSet&lt;&gt;(m.tailMap(fromElement, inclusive));}而这个m我们姑且认为它是TreeMap，也就是调用TreeMap的tailMap()方法：12345public NavigableMap&lt;K,V&gt; tailMap(K fromKey, boolean inclusive) { return new AscendingSubMap&lt;&gt;(this, false, fromKey, inclusive, true, null, true);}可以看到，返回的是AscendingSubMap对象，这个类的继承链是怎么样的呢？可以看到，这个类并没有继承TreeMap，不过通过源码分析也可以看出来这个类是组合了TreeMap，也算和TreeMap有点关系，只是不是继承关系。所以，TreeSet的底层不完全是使用TreeMap来实现的，更准确地说，应该是NavigableMap。","link":"/2017/02/08/TreeSet内部原理/"},{"title":"Shell编程实战总结","text":"Vim 批量行首添加字符串123# 以后研究下列模式能否实现vim mode下 :%s/^/del user_item__/gLinux显示彩色字体1234567891011121314151617echo -e \"\\033[30m 黑色字 \\033[0m\"echo -e \"\\033[31m 红色字 \\033[0m\"echo -e \"\\033[32m 绿色字 \\033[0m\"echo -e \"\\033[33m 黄色字 \\033[0m\"echo -e \"\\033[34m 蓝色字 \\033[0m\"echo -e \"\\033[35m 紫色字 \\033[0m\"echo -e \"\\033[36m 天蓝字 \\033[0m\"echo -e \"\\033[37m 白色字 \\033[0m\" echo -e \"\\033[40;37m 黑底白字 \\033[0m\"echo -e \"\\033[41;37m 红底白字 \\033[0m\"echo -e \"\\033[42;37m 绿底白字 \\033[0m\"echo -e \"\\033[43;37m 黄底白字 \\033[0m\"echo -e \"\\033[44;37m 蓝底白字 \\033[0m\"echo -e \"\\033[45;37m 紫底白字 \\033[0m\"echo -e \"\\033[46;37m 天蓝底白字 \\033[0m\"echo -e \"\\033[47;30m 白底黑字 \\033[0m\"隔一秒检查MySQL在执行的DML语句1234while true; do sudo mysql --default-character-set=utf8 -h ${hostname} -P 3306 -u${username} -p${password} -e \"show processlist\" | grep Query; sleep 1; done以逗号分隔每一行，取第一列1awk -F, '{print $1}' doufen_uid_name查看jar包内容1jar -tf xxxx.jar统计一个文件的行数123456# 方法1：先用wc -l算出行数，awk取第一个字段wc -l filename | awk '{print $1}'# 方法2：巧用END函数和内置变量NR直接输出行数# NR变量可以输出当前行号，END函数是awk读取完文件之后执行的操作awk 'END{pint NR}' filename如何获取当前脚本的名称？当前路径文件的名称？12# shell scriptbasename $0Linux下的进程管理12345678910111213141516171819# 1. bg,fg,&amp;,ctrl+c, ctrl+z, jobs# 查看后台进程任务bg# 将一个后台任务唤醒start并切到前台fg# 将一个任务转为daemon进程在后台运行&amp;# 停止当前正在运行的进程ctrl+c# 将当前进程放到后天并挂起[stop]ctrl+z# 查看当前用户下的所有后台任务jobs# 工作流实践# 1.vim 打开一个log文件# 2.ctrl + z在后台挂起# 3.jobs# 4.fg + 编号唤醒相应的进程实时的监控网卡流量1iptraf网络状态工具SS取出中括号内的内容1234567echo 123[321]123 | cut -d \"[\" -f2 | cut -d \"]\" -f1output:321# cut语法理解：cut -d 会根据指定的分隔符切分字符串（或者从文件中读取），-f{$num}的num是按照该分隔符分割后取哪一块内容（也就是分割后是一个字符串数组，你可以根据下标获取期望的子串，只不过该数组下标从）使用VIM去除段落内行尾的空格与tab1:%s#\\s\\+$##过滤并高亮1grep --color -C30 'RESPONSE' test.logosx下利用xargs传参123ls | grep json |grep -v '(' | xargs -I % cp % test/# %%内包起来要执行的命令即可，不需要{}占位符# 注意：OSX的xargs与Linux不同查看系统中的内存占用1ps -A --sort -rss -o pid,comm,pmem,rss | less查看进程打开的文件句柄数1for p in $(pidof java); do echo \"PID # $p has $(lsof -n -a -p $p|wc -l) fd opened.\"; doneLinux根据PID查找执行路径及其他详情1ll /proc/PID","link":"/2017/08/25/shell编程实战/"},{"title":"JDK源码阅读心得","text":"简介最近写了一些 java 核心技术的文章放在 WikiNotes里，有些体会也想在这里记录一下。 这篇文章主要讲述jdk本身的源码该如何阅读，关于各种框架的源码阅读我们后面再一起探讨。笔者认为阅读源码主要包括下面几个步骤。设定目标凡事皆有目的，阅读源码也是一样。从大的方面来说，我们阅读源码的目的是为了提升自己的技术能力，运用到工作中，遇到问题快速定位，升职加薪等等。从小的方面来说，阅读某一段源码的目的就是要搞清楚它的原理，就是死磕，就是那种探索真相的固执。目的是抽象的，目标是具体的，我们阅读源码之前一定要给自己设定一个目标。比如，下一章我们将要一起学习的ConcurrentHashMap，我们可以设定以下目标：熟悉ConcurrentHashMap的存储结构；熟悉ConcurrentHashMap中主要方法的实现过程；探索ConcurrentHashMap中出现的新技术；提出问题有了目标之后，我们要试着提出一些问题。还是以ConcurrentHashMap为例，笔者提出了以下这些问题：ConcurrentHashMap与HashMap的数据结构是否一样？HashMap在多线程环境下何时会出现并发安全问题？ConcurrentHashMap是怎么解决并发安全问题的？ConcurrentHashMap使用了哪些锁？ConcurrentHashMap的扩容是怎么进行的？ConcurrentHashMap是否是强一致性的？ConcurrentHashMap不能解决哪些问题？ConcurrentHashMap除了并发安全，还有哪些与HashMap不同的地方，为什么要那么实现？ConcurrentHashMap中有哪些不常见的技术值得学习？如何提出问题很多人会说，我也知道要提出问题，但是该怎么提出问题呢？这确实是很困难的一件事，笔者认为主要是三点：问自己把自己当成面试官问自己，往死里问的那种。如果问自己问不出几个问题，也不要紧，请看下面。问互联网很多问题可能自己也想不到，那就需要上网大概查一下相关的博客，看人家有没有提出什么问题。或者，查询相关面试题。比如，笔者学习ConcurrentHashMap这个类时，上网一查很多都是基于jdk7的，那这时候就可以提出一个问题，jdk8与jdk7中ConcurrentHashMap这个类的实现方式有何不同？jdk8对jdk7作了哪些优化？不断发现问题在源码阅读的过程中，可能看着看着就遇到个问题，这是非常常见的，这种问题也应该保留下来研究研究。比如，ConcurrentHashMap中size()方法是怎么实现的？@sun.misc.Contended这玩意是什么鬼东西？然后上网一查，说是为了避免伪共享，我X，伪共享又是啥？然后你再查一下伪共享，又出来了CPU多级缓存？学完CPU多级缓存，是不是觉得跟jvm的内存模型很像？问完这一连串问题，是不是感觉世界都清晰了？^_^看吧，问题是源源不断地被发现的。所以，一开始提不出几个问题也不要紧，关键是要看，看了才能发现更多的问题。带着问题阅读源码，忽略不必要的细节，死磕重要的细节首先，一定要带着问题阅读源码。其次，一定要忽略不必要的细节。再次，一定要死磕重要的细节。乍一看，后面两步似乎有所矛盾，其实不然，忽略不必要的细节是为了不迷失在源码的世界中，死磕重要的细节是为了弄清楚源码的真相。这里的细节是忽略还是死磕，主要是看跟问题的相关性。jdk源码还是比较好阅读的，如果后面看spring的源码，做不到忽略不必要的细节，真的是会迷失的，先埋个伏笔哈~~举个例子，之前阅读过ArrayList的序列化相关的代码中的readObject()方法。s.readInt();这行是干嘛的？省略行不行？这时候就要去了解序列化相关的知识，然后看看writeObject()里面的实现，这就是要死磕的代码。SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity);这行又是干嘛的？乍一看，好像是跟权限相关的代码，跟我们的问题“序列化”无关，忽略之，如果实在想知道，先打个标记，等把序列化的问题解决了再来研究这个东西。12345678910111213141516171819202122232425private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { // 声明为空数组 elementData = EMPTY_ELEMENTDATA; // 读入非transient非static属性（会读取size属性） s.defaultReadObject(); // 读入元素个数，没什么用，只是因为写出的时候写了size属性，读的时候也要按顺序来读 s.readInt(); if (size &gt; 0) { // 计算容量 int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); // 检查是否需要扩容 ensureCapacityInternal(size); Object[] a = elementData; // 依次读取元素到数组中 for (int i=0; i&lt;size; i++) { a[i] = s.readObject(); } }}多做比较在阅读jdk源码的时候，还有很重要的一点，就是要多做比较，比较也可以分为横向比较和纵向比较。横向比较就是与相似的类做比较。比如，集合模块中，基本都是各种插入、查询、删除元素，那这时候可以从数据结构、时间复杂度等维度进行比较，这就是横向比较。纵向比较可以从集合发展的历史进行比较。比如，HashMap的发展史，从（单个数组）实现（没错，可以直接用一个数组实现HashMap），到（多数组+链表）实现，再到jdk8中的（多数组+链表+红黑树）实现，这就是纵向比较。多做实验最后一步，最最最最重要的就是要多做实验。比如，ConcurrentHashMap是不是强一致性的？可以启动多个线程去不断调用get()、put()、size()方法，看看是不是强一致性的。","link":"/2016/09/02/如何阅读JDK源码/"},{"title":"加载模型文件","text":"提示: 本文是模型部署方案的一部分依赖java8pmml-model 1.4.2步骤加载反序列化模型文件为PMML对象å优化模型，并写到新模型文件我们开始吧maven依赖12345678910111213&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;pmml-model.version&gt;1.4.2&lt;/pmml-model.version&gt;&lt;/properties&gt;&lt;!--pmml模型--&gt;&lt;dependency&gt; &lt;groupId&gt;org.jpmml&lt;/groupId&gt; &lt;artifactId&gt;pmml-model&lt;/artifactId&gt; &lt;version&gt;${pmml-model.version}&lt;/version&gt;&lt;/dependency&gt;加载模型 &amp; 更新模型1234567891011121314151617181920212223242526272829303132333435363738package org.houqian.jpmml;import com.fasterxml.jackson.core.JsonProcessingException;import org.dmg.pmml.PMML;import org.jpmml.model.PMMLUtil;import org.jpmml.model.visitors.LocatorNullifier;import org.xml.sax.SAXException;import javax.xml.bind.JAXBException;import java.io.*;/** * @author : houqian * @version : 1.0 * @since : 2018-08-30 */public class Load { public static void main(String[] args) throws FileNotFoundException, JAXBException, SAXException, JsonProcessingException { // 这里的pmml是之前sklearn、sparkml导出的文件，任选其一即可 FileInputStream srcModelIs = new FileInputStream(new File(\"src/main/resources/pipeline.pmml.xml\")); PMML pmml = PMMLUtil.unmarshal(srcModelIs); optimize(pmml); FileOutputStream newModelOs = new FileOutputStream(new File(\"src/main/resources/new-pipeline.pmml.xml\")); store(pmml, newModelOs); } public static void optimize(PMML pmml){ LocatorNullifier nullifier = new LocatorNullifier(); nullifier.applyTo(pmml); } public static void store(PMML pmml, OutputStream os) throws JAXBException { org.jpmml.model.PMMLUtil.marshal(pmml, os); }}笔者没有找到如何验证以上操作和模型正确性的关系，现在姑且先让它这样不报错跑通吧。123456789101112131415161718➜ src git:(master) ✗ tree -L 3 .├── main│ ├── java│ │ ├── com│ │ └── org│ └── resources│ ├── application.properties│ ├── application.yml│ ├── import.sql│ ├── kafka│ ├── new-pipeline.pmml.xml &lt;--生成的新模型文件│ └── pipeline.pmml.xml &lt;--之前sklearn输出的模型文件└── test └── java └── org9 directories, 5 files参考https://github.com/jpmml/jpmml-model","link":"/2018/08/30/加载模型文件/"},{"title":"机器学习模型部署方案","text":"背景目前，我们组反欺诈业务已经进行到了较为后期的阶段，初步有了基础的数据平台、计算平台。此时，算法同事也升级了过往基于统计分布的简单算法实现，而采用了更为强大的机器学习模型，目前已知的将会使用以下几类算法：离群点检测算法、树类算法等（笔者仅了解常见算法，描述不准确还望见谅。）问题算法同学日常使用sk-learn或者spark ml居多，其中前者使用python后者使用scala作为日常开发语言。笔者目前所做的属于算法工程化，需要对接线上实时数据流、离线数据实现对算法同学产出的模型的上线。目前，已知的其他团队的方案有：使用python语言实现backend，并对接实时、离线数据使用Java重新实现一遍算法逻辑，然后实现web api，并对接实时、离线数据分析一下这两种方案：对于1，他们这么做的理由是算法同学自己的工程能力非常强，可以自己用python一站式搞定模型训练、上线，并且他们的业务离线居多，对实时性的要求不高。对于2，他们这么做的理由是算法同学工程能力较弱，自己无法搞定一站式。使用线性模型比较多，后端Java同学实现起来还是比较简单的，性能也非常好。然而，这两种方案目前看来都不能满足我们组的需求。我们的模型比较复杂，用Java实现一遍显的复杂性不可控，验证实现正确性的成本也很高。主要是实时、数据量较少的场景，对实时性要求比较高（查询四五个数据源，加上模型逻辑，需要在1秒内给业务方返回）希望做到模型的部署和训练接耦，算法同学和后端同学各自专注自己的事情方案针对如上特征，笔者进行了调查。发现使用模型训练—&gt;导出模型文件—&gt;服务端加载模型文件，对接数据源这种模式比较适合我们。目前业界使用的比较多的模型文件标准是PMML，以下是对该标准的简单描述：PMML 是一种事实标准语言，用于呈现数据挖掘模型。PMML 允许您在不同的应用程序之间轻松共享预测分析模型。因此，您可以在一个系统中定型一个模型，在 PMML 中对其进行表达，然后将其移动到另一个系统中。PMML 是数据挖掘群组的产物，该群组是一个由供应商领导的委员会，由各种商业和开放源码分析公司组成。因此，现在的大部分领先数据挖掘工具都可以导出或导入 PMML。作为一个已发展 10 多年的成熟标准，PMML 既可以呈现用于从数据中了解模型的统计技术（如人工神经网络和决策树），也可以呈现原始输入数据的预处理以及模型输出的后处理。[1]可以看到，PMML作为一个10几年的标准，在数据挖掘领域是很成熟的。笔者在Github也找到了该标准的Java实现[2]。可以看到其子项目分类非常清晰：jpmml-evaluatorjava加载jpmml格式文件API，这个也是我们服务端重点关注的jpmml-modeljpmml文件对应的Java类jpmml-sparkml与sparkml整合以导出jpmml文件jpmml-sklearn与sklearn整合以导出jpmml文件jpmml-rjpmml-xgboost如果使用这种方案，改动量：算法同学需要整合jpmml-sparkml、jpmml-sklearn后端工程同学需要整合jpmml-evaluator、jpmml-model改动量还是比较大的，目前也主要是笔者自己进行相关的调研，接下来笔者尝试跑通模型训练—&gt;导出模型文件—&gt;服务端加载模型文件，对接数据源这个流程。实现算法侧sklearn导出jpmmlsparkml导出jpmml工程侧加载模型文件调用模型总结该方案的优势解耦使用PMML作为中间层，解耦了模型训练、部署这两个环节，使得算法同学专注效果、后端同学专注性能成熟度很高已经有10几年的历史该方案的缺点故障追踪相对困难增加了一层模型文件，引入了几个子项目，出了问题定位起来相对以前要难上一些。不过，如果满足我们的需求，相信随着对源码的逐步掌握，我们可以在关键节点做埋点日志，结合已有的tracing系统，这个问题也不大心得这次总共花了1天多的时间研究更好的模型上线方案，初步确定了JPMML这一生成中间文件的方案。目前，已经跑通了从算法模型训练（覆盖sklearn、spark ml的jpmml文件输出）到使用jpmml模型文件部署的整个流程。过程中还算比较顺利，github上的文档虽少但足够精炼，已经覆盖了主要流程。但并没有覆盖到算法同学的复杂模型case，因此还需要和算法同学细化使用场景。如果该方案最终被采用，笔者会撰写更为深入的源码分析系列文章。Reference1.何为PMML？https://www.ibm.com/developerworks/cn/opensource/ind-PMML1/index.html ↩2.Jpmml Github https://github.com/jpmml ↩3.携程模型部署-小记 http://ytluck.github.io/program/my-program-post-22.html ↩4.干货，机器学习算法线上部署方法 https://zhuanlan.zhihu.com/p/23382412 ↩","link":"/2018/08/29/机器学习模型部署方案/"},{"title":"监控调研","text":"背景性能压测后，逐个埋点添加各种StopWatch。两个缺点：侵入了业务逻辑，降低可读性耗时，工作量大期望：【注解】 基于配置或注解可以精确控制要添加性能监控的method或class【指标维度全面】监控指标全面，除了常见的QPS、Req Cost Time，还要有TPS, P90, P95 ,P99等指标【可视化】最好有图形化的界面可以实时看到各个method的性能状况方案调研商业OneAPM、NewRelic等基于java agent实现无侵入监控性能指标。开源【一站式】open-tracing【定制化】metrics + aop + AMQP中间件 + InfluxDB + Grafana分析根据目前的需求，需要灵活的定制化监控项，可控的监控数据流，因此原则定制化开源项。方案细节组件调研监控指标采集MetricsAOP指标数据传输AMQP选型指标数据落地时间序列数据库InfluxDB指标数据展示Grafana方案打通测试上线ReferenceMetrics —— JVM上的实时监控类库","link":"/2017/08/29/监控调研/"},{"title":"调用模型","text":"提示: 本文是模型部署方案的一部分依赖java8pmml-evaluator 1.4.2pmml-evaluator-extension 1.4.2步骤使用JPMML执行引擎反序列化pmml文件模型校验（此处具体校验的问题，笔者以后另开一篇）我们开始吧maven依赖：123456789101112131415&lt;properties&gt; &lt;pmml-evaluator.version&gt;1.4.2&lt;/pmml-evaluator.version&gt;&lt;/properties&gt;&lt;!--运行模型--&gt;&lt;dependency&gt; &lt;groupId&gt;org.jpmml&lt;/groupId&gt; &lt;artifactId&gt;pmml-evaluator&lt;/artifactId&gt; &lt;version&gt;${pmml-evaluator.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.jpmml&lt;/groupId&gt; &lt;artifactId&gt;pmml-evaluator-extension&lt;/artifactId&gt; &lt;version&gt;${pmml-evaluator.version}&lt;/version&gt;&lt;/dependency&gt;一网打尽1234567891011121314151617181920212223242526272829303132333435363738394041424344package org.houqian.jpmml;import org.jpmml.evaluator.*;import org.xml.sax.SAXException;import javax.xml.bind.JAXBException;import java.io.File;import java.io.IOException;import java.util.List;/** * @author : houqian * @version : 1.0 * @since : 2018-08-30 */public class Execute { public static void main(String[] args) throws JAXBException, SAXException, IOException { // 从pmml文件实例化模型执行引擎 Evaluator evaluator = new LoadingModelEvaluatorBuilder() .setLocatable(false) .setVisitors(new DefaultVisitorBattery()) //.setOutputFilter(OutputFilters.KEEP_FINAL_RESULTS) .load(new File(\"src/main/resources/pipeline.pmml.xml\")) .build(); // 校验模型合法性 evaluator.verify(); // 输入变量 List&lt;? extends InputField&gt; inputFields = evaluator.getInputFields(); System.out.println(\"Input fields: \" + inputFields); // 标签 List&lt;? extends TargetField&gt; targetFields = evaluator.getTargetFields(); System.out.println(\"Target field(s): \" + targetFields); // 输出变量 List&lt;? extends OutputField&gt; outputFields = evaluator.getOutputFields(); System.out.println(\"Output fields: \" + outputFields); // 释放引用，优化GC evaluator = null; }}","link":"/2018/08/30/调用模型/"},{"title":"一张图澄清架构设计模式","text":"","link":"/2019/06/01/一张图澄清架构设计模式/"},{"title":"看起来简单的通知推送服务，真的简单吗","text":"背景目前OneAlert提供短信、邮件、电话、APP四种通知通道，其中前三种的使用量最高(90%以上的用户)，因此靠谱的第三方推送提供商至关重要。经过对各种三方推送服务的公司调研，目前锁定了阿里大鱼[2]、容联云[3]、云片[4]、云之讯[5]、SendCloud[6]，这5家平台提供商。首先我们来分析一下接入三方后的通讯模型注：配额这里是指三方服务商的发送限制，比如每小时最多每个电话拨打几次我们作为一个服务提供商，必然要确保用户的告警可以准确、准时、不漏的投递给用户，这个问题看起来一目了然，没什么难度，其实不然，我们分析下以上通信模型中的几种情况：s1:发送成功最好的情况（这里由于无法真正的监控第三方是否真正投递到了用户，不过根据以往的工单经验，丢失的概率很小）s2:超出配额换个重发超出配额，那就换个重发呗？这是下意识的解决思路，我们看看有什么问题：独立状态服务很直观的发现，重试的次数有可能会很多，这非常影响推送实时性。马上，我们又会想到可以对每次超出配额的情况进行缓存，提炼一个状态服务，如下：问题到此为止，一切美好已经发生… 遗憾的是，该状态服务几乎是不可用的。我们可以进一步思考：当第一次配额超出的情况发生时，按照上面的设计，该状态服务会缓存下来；当第二次推送来临时，会首先请求状态服务，拿到配额超出的那个服务商，排除掉它，使用其他服务商发送当第三次推送来临时，会首先请求状态服务，拿到配额超出的那个服务商，排除掉它，使用其他服务商发送…当第n次推送…此时问题一目了然，某个三方服务商第一次超出配额后就不再被请求了，这显然不是我们要的。如果我们试图及时、恰当的让某超出配额的状态失效掉，问题是我们如何知道什么时间或者哪一次请求时让其失效呢？小结目前看来，针对简单的请求-重试模型是很难解决我们的问题的。换个思路上面所有的方案每次发送都是无状态的，如果我们对每个时间段每次发送用的哪个服务商记录下来，结合配额限制，这样是不是可以做到避免处罚超出配额的问题呢？我们来分析下：嗯… 此时貌似已经解决了问题。不过从设计来看，NotifySender职责太多了，既要负责发送消息，又得记录发送状态，不符合单一职责原则。如果将来对三方服务商做增加、下线操作，还要动NotifySender，显然是不合理的。小结通过有状态的发送，解决了配额问题，但NotifySender职责太多，不利于拓展，需要再次设计。最后方案事实上，我们需要一个这样的服务，它能根据三方服务商目前的配额情况，自动的给我们的消息路由到合适的服务商。实质上，对于每次发送「计数」这件事，其实可以换一个角度思考，我们只要确保在单位时间内，按照配额作为权重将消息分发给服务商即可，这样也能确保不会触发超出配额异常。由于篇幅关系，这里不对轮询算法进行展开讨论，而且本身算法不是重点，此类算法网上一搜一大堆，笔者也不想赘述，重要的是我们对业务场景进行实质分析。ok，直接给出结论：加权轮询算法（如果读者有兴趣，可以给我发邮件，我们一起讨论: P）对调度服务的补充解释：我们选择将服务商信息存放在配置中心里，主要考虑了以下几点：如果服务商将来由于各种原因维护，我们只需要从配置中心去掉这个服务商即可，NotifySender不需要做任何改动如果服务商将来更改了配额，我们只需要在配置中心重新配置一下这个服务商的配额信息即可，NotifySender也不需要做任何改动Reference1.https://www.onealert.com OneAlert是OneAPM旗下的一站式运维事件管理平台 ↩2.https://dayu.aliyun.com 阿里大鱼 ↩3.https://www.yuntongxun.com/ 容联云 ↩4.https://www.yunpian.com 云片 ↩5.https://www.ucpaas.com 云之讯 ↩6.https://sendcloud.sohu.com/ SendCloud ↩","link":"/2016/05/31/看起来简单的通知推送服务，真的简单吗/"},{"title":"ArrayList内部原理","text":"简介ArrayList是一种以数组实现的List，与数组相比，它具有动态扩展的能力，因此也可称之为动态数组。实战另见 ArrayList实战继承体系ArrayList实现了List, RandomAccess, Cloneable, java.io.Serializable等接口。ArrayList实现了List，提供了基础的添加、删除、遍历等操作。ArrayList实现了RandomAccess，提供了随机访问的能力。ArrayList实现了Cloneable，可以被克隆。ArrayList实现了Serializable，可以被序列化。源码解析属性123456789101112131415161718192021222324/** * 默认容量 */private static final int DEFAULT_CAPACITY = 10;/** * 空数组，如果传入的容量为0时使用 */private static final Object[] EMPTY_ELEMENTDATA = {};/** * 空数组，传传入容量时使用，添加第一个元素的时候会重新初始为默认容量大小 */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};/** * 存储元素的数组 */transient Object[] elementData; // non-private to simplify nested class access/** * 集合中元素的个数 */private int size;DEFAULT_CAPACITY默认容量为10，也就是通过new ArrayList()创建时的默认容量。EMPTY_ELEMENTDATA空的数组，这种是通过new ArrayList(0)创建时用的是这个空数组。DEFAULTCAPACITY_EMPTY_ELEMENTDATA也是空数组，这种是通过new ArrayList()创建时用的是这个空数组，与EMPTY_ELEMENTDATA的区别是在添加第一个元素时使用这个空数组的会初始化为DEFAULT_CAPACITY（10）个元素。elementData真正存放元素的地方，使用transient是为了不序列化这个字段。至于没有使用private修饰，后面注释是写的“为了简化嵌套类的访问”，但是楼主实测加了private嵌套类一样可以访问。private表示是类私有的属性，只要是在这个类内部都可以访问，嵌套类或者内部类也是在类的内部，所以也可以访问类的私有成员。size真正存储元素的个数，而不是elementData数组的长度。ArrayList(int initialCapacity)构造方法传入初始容量，如果大于0就初始化elementData为对应大小，如果等于0就使用EMPTY_ELEMENTDATA空数组，如果小于0抛出异常。123456789101112public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) { // 如果传入的初始容量大于0，就新建一个数组存储元素 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { // 如果传入的初始容量等于0，使用空数组EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; } else { // 如果传入的初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \" + initialCapacity); }}ArrayList()构造方法不传初始容量，初始化为DEFAULTCAPACITY_EMPTY_ELEMENTDATA空数组，会在添加第一个元素的时候扩容为默认的大小，即10。12345public ArrayList() { // 如果没有传入初始容量，则使用空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA // 使用这个数组是在添加第一个元素的时候会扩容到默认大小10 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;}ArrayList(Collection&lt;? extends E&gt; c)构造方法传入集合并初始化elementData，这里会使用拷贝把传入集合的元素拷贝到elementData数组中，如果元素个数为0，则初始化为EMPTY_ELEMENTDATA空数组。123456789101112131415/*** 把传入集合的元素初始化到ArrayList中*/public ArrayList(Collection&lt;? extends E&gt; c) { // 集合转数组 elementData = c.toArray(); if ((size = elementData.length) != 0) { // 检查c.toArray()返回的是不是Object[]类型，如果不是，重新拷贝成Object[].class类型 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // 如果c的空集合，则初始化为空数组EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; }}为什么c.toArray();返回的有可能不是Object[]类型呢？请看下面的代码：12345678910111213141516171819202122232425262728public class ArrayTest { public static void main(String[] args) { Father[] fathers = new Son[]{}; // 打印结果为class [Lcom.coolcoding.code.Son; System.out.println(fathers.getClass()); List&lt;String&gt; strList = new MyList(); // 打印结果为class [Ljava.lang.String; System.out.println(strList.toArray().getClass()); }}class Father {}class Son extends Father {}class MyList extends ArrayList&lt;String&gt; { /** * 子类重写父类的方法，返回值可以不一样 * 但这里只能用数组类型，换成Object就不行 * 应该算是java本身的bug */ @Override public String[] toArray() { // 为了方便举例直接写死 return new String[]{\"1\", \"2\", \"3\"}; }}add(E e)方法添加元素到末尾，平均时间复杂度为O(1)。1234567891011121314151617181920212223242526272829303132333435363738394041public boolean add(E e) { // 检查是否需要扩容 ensureCapacityInternal(size + 1); // 把元素插入到最后一位 elementData[size++] = e; return true;}private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));}private static int calculateCapacity(Object[] elementData, int minCapacity) { // 如果是空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA，就初始化为默认大小10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity;}private void ensureExplicitCapacity(int minCapacity) { modCount++; if (minCapacity - elementData.length &gt; 0) // 扩容 grow(minCapacity);}private void grow(int minCapacity) { int oldCapacity = elementData.length; // 新容量为旧容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 如果新容量发现比需要的容量还小，则以需要的容量为准 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量已经超过最大容量了，则使用最大容量 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 以新容量拷贝出来一个新数组 elementData = Arrays.copyOf(elementData, newCapacity);}检查是否需要扩容；如果elementData等于DEFAULTCAPACITY_EMPTY_ELEMENTDATA则初始化容量大小为DEFAULT_CAPACITY；新容量是老容量的1.5倍（oldCapacity + (oldCapacity &gt;&gt; 1)），如果加了这么多容量发现比需要的容量还小，则以需要的容量为准；创建新容量的数组并把老数组拷贝到新数组；add(int index, E element)方法添加元素到指定位置，平均时间复杂度为O(n)。123456789101112131415161718public void add(int index, E element) { // 检查是否越界 rangeCheckForAdd(index); // 检查是否需要扩容 ensureCapacityInternal(size + 1); // 将inex及其之后的元素往后挪一位，则index位置处就空出来了 System.arraycopy(elementData, index, elementData, index + 1, size - index); // 将元素插入到index的位置 elementData[index] = element; // 大小增1 size++;}private void rangeCheckForAdd(int index) { if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));}检查索引是否越界；检查是否需要扩容；把插入索引位置后的元素都往后挪一位；在插入索引位置放置插入的元素；大小加1；addAll(Collection&lt;? extends E&gt; c)方法求两个集合的并集。12345678910111213141516/*** 将集合c中所有元素添加到当前ArrayList中*/public boolean addAll(Collection&lt;? extends E&gt; c) { // 将集合c转为数组 Object[] a = c.toArray(); int numNew = a.length; // 检查是否需要扩容 ensureCapacityInternal(size + numNew); // 将c中元素全部拷贝到数组的最后 System.arraycopy(a, 0, elementData, size, numNew); // 大小增加c的大小 size += numNew; // 如果c不为空就返回true，否则返回false return numNew != 0;}拷贝c中的元素到数组a中；检查是否需要扩容；把数组a中的元素拷贝到elementData的尾部；get(int index)方法获取指定索引位置的元素，时间复杂度为O(1)。123456789101112131415public E get(int index) { // 检查是否越界 rangeCheck(index); // 返回数组index位置的元素 return elementData(index);}private void rangeCheck(int index) { if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));}E elementData(int index) { return (E) elementData[index];}检查索引是否越界，这里只检查是否越上界，如果越上界抛出IndexOutOfBoundsException异常，如果越下界抛出的是ArrayIndexOutOfBoundsException异常。返回索引位置处的元素；remove(int index)方法删除指定索引位置的元素，时间复杂度为O(n)。12345678910111213141516171819public E remove(int index) { // 检查是否越界 rangeCheck(index); modCount++; // 获取index位置的元素 E oldValue = elementData(index); // 如果index不是最后一位，则将index之后的元素往前挪一位 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); // 将最后一个元素删除，帮助GC elementData[--size] = null; // clear to let GC do its work // 返回旧值 return oldValue;}检查索引是否越界；获取指定索引位置的元素；如果删除的不是最后一位，则其它元素往前移一位；将最后一位置为null，方便GC回收；返回删除的元素。可以看到，ArrayList删除元素的时候并没有缩容。remove(Object o)方法删除指定元素值的元素，时间复杂度为O(n)。12345678910111213141516171819202122232425262728293031public boolean remove(Object o) { if (o == null) { // 遍历整个数组，找到元素第一次出现的位置，并将其快速删除 for (int index = 0; index &lt; size; index++) // 如果要删除的元素为null，则以null进行比较，使用== if (elementData[index] == null) { fastRemove(index); return true; } } else { // 遍历整个数组，找到元素第一次出现的位置，并将其快速删除 for (int index = 0; index &lt; size; index++) // 如果要删除的元素不为null，则进行比较，使用equals()方法 if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false;}private void fastRemove(int index) { // 少了一个越界的检查 modCount++; // 如果index不是最后一位，则将index之后的元素往前挪一位 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); // 将最后一个元素删除，帮助GC elementData[--size] = null; // clear to let GC do its work}找到第一个等于指定元素值的元素；快速删除；fastRemove(int index)相对于remove(int index)少了检查索引越界的操作，可见jdk将性能优化到极致。retainAll(Collection&lt;?&gt; c)方法求两个集合的交集。12345678910111213141516171819202122232425262728293031323334353637383940414243444546public boolean retainAll(Collection&lt;?&gt; c) { // 集合c不能为null Objects.requireNonNull(c); // 调用批量删除方法，这时complement传入true，表示删除不包含在c中的元素 return batchRemove(c, true);}/*** 批量删除元素* complement为true表示删除c中不包含的元素* complement为false表示删除c中包含的元素*/private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) { final Object[] elementData = this.elementData; // 使用读写两个指针同时遍历数组 // 读指针每次自增1，写指针放入元素的时候才加1 // 这样不需要额外的空间，只需要在原有的数组上操作就可以了 int r = 0, w = 0; boolean modified = false; try { // 遍历整个数组，如果c中包含该元素，则把该元素放到写指针的位置（以complement为准） for (; r &lt; size; r++) if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; } finally { // 正常来说r最后是等于size的，除非c.contains()抛出了异常 if (r != size) { // 如果c.contains()抛出了异常，则把未读的元素都拷贝到写指针之后 System.arraycopy(elementData, r, elementData, w, size - r); w += size - r; } if (w != size) { // 将写指针之后的元素置为空，帮助GC for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w; // 新大小等于写指针的位置（因为每写一次写指针就加1，所以新大小正好等于写指针的位置） size = w; modified = true; } } // 有修改返回true return modified;}遍历elementData数组；如果元素在c中，则把这个元素添加到elementData数组的w位置并将w位置往后移一位；遍历完之后，w之前的元素都是两者共有的，w之后（包含）的元素不是两者共有的；将w之后（包含）的元素置为null，方便GC回收；removeAll(Collection&lt;?&gt; c)求两个集合的单方向差集，只保留当前集合中不在c中的元素，不保留在c中不在当前集体中的元素。123456public boolean removeAll(Collection&lt;?&gt; c) { // 集合c不能为空 Objects.requireNonNull(c); // 同样调用批量删除方法，这时complement传入false，表示删除包含在c中的元素 return batchRemove(c, false);}与retainAll(Collection&lt;?&gt; c)方法类似，只是这里保留的是不在c中的元素。总结ArrayList内部使用数组存储元素，当数组长度不够时进行扩容，每次加一半的空间，ArrayList不会进行缩容；ArrayList支持随机访问，通过索引访问元素极快，时间复杂度为O(1)；ArrayList添加元素到尾部极快，平均时间复杂度为O(1)；ArrayList添加元素到中间比较慢，因为要搬移元素，平均时间复杂度为O(n)；ArrayList从尾部删除元素极快，时间复杂度为O(1)；ArrayList从中间删除元素比较慢，因为要搬移元素，平均时间复杂度为O(n)；ArrayList支持求并集，调用addAll(Collection&lt;? extends E&gt; c)方法即可；ArrayList支持求交集，调用retainAll(Collection&lt;? extends E&gt; c)方法即可；ArrayList支持求单向差集，调用removeAll(Collection&lt;? extends E&gt; c)方法即可；彩蛋elementData设置成了transient，那ArrayList是怎么把元素序列化的呢？12345678910111213141516171819202122232425262728293031323334353637383940414243444546private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // 防止序列化期间有修改 int expectedModCount = modCount; // 写出非transient非static属性（会写出size属性） s.defaultWriteObject(); // 写出元素个数 s.writeInt(size); // 依次写出元素 for (int i=0; i&lt;size; i++) { s.writeObject(elementData[i]); } // 如果有修改，抛出异常 if (modCount != expectedModCount) { throw new ConcurrentModificationException(); }}private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { // 声明为空数组 elementData = EMPTY_ELEMENTDATA; // 读入非transient非static属性（会读取size属性） s.defaultReadObject(); // 读入元素个数，没什么用，只是因为写出的时候写了size属性，读的时候也要按顺序来读 s.readInt(); if (size &gt; 0) { // 计算容量 int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); // 检查是否需要扩容 ensureCapacityInternal(size); Object[] a = elementData; // 依次读取元素到数组中 for (int i=0; i&lt;size; i++) { a[i] = s.readObject(); } }}查看writeObject()方法可知，先调用s.defaultWriteObject()方法，再把size写入到流中，再把元素一个一个的写入到流中。一般地，只要实现了Serializable接口即可自动序列化，writeObject()和readObject()是为了自己控制序列化的方式，这两个方法必须声明为private，在java.io.ObjectStreamClass#getPrivateMethod()方法中通过反射获取到writeObject()这个方法。在ArrayList的writeObject()方法中先调用了s.defaultWriteObject()方法，这个方法是写入非static非transient的属性，在ArrayList中也就是size属性。同样地，在readObject()方法中先调用了s.defaultReadObject()方法解析出了size属性。elementData定义为transient的优势，自己根据size序列化真实的元素，而不是根据数组的长度序列化元素，减少了空间占用。","link":"/2016/03/04/ArrayList内部原理/"},{"title":"TreeMap内部原理(一)","text":"简介TreeMap使用红黑树存储元素，可以保证元素按key值的大小进行遍历。实战另见 TreeMap继承体系TreeMap实现了Map、SortedMap、NavigableMap、Cloneable、Serializable等接口。SortedMap规定了元素可以按key的大小来遍历，它定义了一些返回部分map的方法。1234567891011121314151617181920public interface SortedMap&lt;K,V&gt; extends Map&lt;K,V&gt; { // key的比较器 Comparator&lt;? super K&gt; comparator(); // 返回fromKey（包含）到toKey（不包含）之间的元素组成的子map SortedMap&lt;K,V&gt; subMap(K fromKey, K toKey); // 返回小于toKey（不包含）的子map SortedMap&lt;K,V&gt; headMap(K toKey); // 返回大于等于fromKey（包含）的子map SortedMap&lt;K,V&gt; tailMap(K fromKey); // 返回最小的key K firstKey(); // 返回最大的key K lastKey(); // 返回key集合 Set&lt;K&gt; keySet(); // 返回value集合 Collection&lt;V&gt; values(); // 返回节点集合 Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet();}NavigableMap是对SortedMap的增强，定义了一些返回离目标key最近的元素的方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface NavigableMap&lt;K,V&gt; extends SortedMap&lt;K,V&gt; { // 小于给定key的最大节点 Map.Entry&lt;K,V&gt; lowerEntry(K key); // 小于给定key的最大key K lowerKey(K key); // 小于等于给定key的最大节点 Map.Entry&lt;K,V&gt; floorEntry(K key); // 小于等于给定key的最大key K floorKey(K key); // 大于等于给定key的最小节点 Map.Entry&lt;K,V&gt; ceilingEntry(K key); // 大于等于给定key的最小key K ceilingKey(K key); // 大于给定key的最小节点 Map.Entry&lt;K,V&gt; higherEntry(K key); // 大于给定key的最小key K higherKey(K key); // 最小的节点 Map.Entry&lt;K,V&gt; firstEntry(); // 最大的节点 Map.Entry&lt;K,V&gt; lastEntry(); // 弹出最小的节点 Map.Entry&lt;K,V&gt; pollFirstEntry(); // 弹出最大的节点 Map.Entry&lt;K,V&gt; pollLastEntry(); // 返回倒序的map NavigableMap&lt;K,V&gt; descendingMap(); // 返回有序的key集合 NavigableSet&lt;K&gt; navigableKeySet(); // 返回倒序的key集合 NavigableSet&lt;K&gt; descendingKeySet(); // 返回从fromKey到toKey的子map，是否包含起止元素可以自己决定 NavigableMap&lt;K,V&gt; subMap(K fromKey, boolean fromInclusive, K toKey, boolean toInclusive); // 返回小于toKey的子map，是否包含toKey自己决定 NavigableMap&lt;K,V&gt; headMap(K toKey, boolean inclusive); // 返回大于fromKey的子map，是否包含fromKey自己决定 NavigableMap&lt;K,V&gt; tailMap(K fromKey, boolean inclusive); // 等价于subMap(fromKey, true, toKey, false) SortedMap&lt;K,V&gt; subMap(K fromKey, K toKey); // 等价于headMap(toKey, false) SortedMap&lt;K,V&gt; headMap(K toKey); // 等价于tailMap(fromKey, true) SortedMap&lt;K,V&gt; tailMap(K fromKey);}存储结构TreeMap只使用到了红黑树，所以它的时间复杂度为O(log n)，我们再来回顾一下红黑树的特性。每个节点或者是黑色，或者是红色。根节点是黑色。每个叶子节点（NIL）是黑色。（注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！）如果一个节点是红色的，则它的子节点必须是黑色的。从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。源码解析属性12345678910111213141516171819/** * 比较器，如果没传则key要实现Comparable接口 */private final Comparator&lt;? super K&gt; comparator;/** * 根节点 */private transient Entry&lt;K,V&gt; root;/** * 元素个数 */private transient int size = 0;/** * 修改次数 */private transient int modCount = 0;（1）comparator按key的大小排序有两种方式，一种是key实现Comparable接口，一种方式通过构造方法传入比较器。（2）root根节点，TreeMap没有桶的概念，所有的元素都存储在一颗树中。Entry内部类存储节点，典型的红黑树结构。12345678static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color = BLACK;}构造方法123456789101112131415161718192021222324252627282930313233/** * 默认构造方法，key必须实现Comparable接口 */public TreeMap() { comparator = null;}/** * 使用传入的comparator比较两个key的大小 */public TreeMap(Comparator&lt;? super K&gt; comparator) { this.comparator = comparator;} /** * key必须实现Comparable接口，把传入map中的所有元素保存到新的TreeMap中 */public TreeMap(Map&lt;? extends K, ? extends V&gt; m) { comparator = null; putAll(m);}/** * 使用传入map的比较器，并把传入map中的所有元素保存到新的TreeMap中 */public TreeMap(SortedMap&lt;K, ? extends V&gt; m) { comparator = m.comparator(); try { buildFromSorted(m.size(), m.entrySet().iterator(), null, null); } catch (java.io.IOException cannotHappen) { } catch (ClassNotFoundException cannotHappen) { }}构造方法主要分成两类，一类是使用comparator比较器，一类是key必须实现Comparable接口。其实，笔者认为这两种比较方式可以合并成一种，当没有传comparator的时候，可以用以下方式来给comparator赋值，这样后续所有的比较操作都可以使用一样的逻辑处理了，而不用每次都检查comparator为空的时候又用Comparable来实现一遍逻辑。123// 如果comparator为空，则key必须实现Comparable接口，所以这里肯定可以强转// 这样在构造方法中统一替换掉，后续的逻辑就都一致了comparator = (k1, k2) -&gt; ((Comparable&lt;? super K&gt;)k1).compareTo(k2);get(Object key)方法获取元素，典型的二叉查找树的查找方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public V get(Object key) { // 根据key查找元素 Entry&lt;K,V&gt; p = getEntry(key); // 找到了返回value值，没找到返回null return (p==null ? null : p.value);}final Entry&lt;K,V&gt; getEntry(Object key) { // 如果comparator不为空，使用comparator的版本获取元素 if (comparator != null) return getEntryUsingComparator(key); // 如果key为空返回空指针异常 if (key == null) throw new NullPointerException(); // 将key强转为Comparable @SuppressWarnings(\"unchecked\") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; // 从根元素开始遍历 Entry&lt;K,V&gt; p = root; while (p != null) { int cmp = k.compareTo(p.key); if (cmp &lt; 0) // 如果小于0从左子树查找 p = p.left; else if (cmp &gt; 0) // 如果大于0从右子树查找 p = p.right; else // 如果相等说明找到了直接返回 return p; } // 没找到返回null return null;} final Entry&lt;K,V&gt; getEntryUsingComparator(Object key) { @SuppressWarnings(\"unchecked\") K k = (K) key; Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) { // 从根元素开始遍历 Entry&lt;K,V&gt; p = root; while (p != null) { int cmp = cpr.compare(k, p.key); if (cmp &lt; 0) // 如果小于0从左子树查找 p = p.left; else if (cmp &gt; 0) // 如果大于0从右子树查找 p = p.right; else // 如果相等说明找到了直接返回 return p; } } // 没找到返回null return null;}从root遍历整个树；如果待查找的key比当前遍历的key小，则在其左子树中查找；如果待查找的key比当前遍历的key大，则在其右子树中查找；如果待查找的key与当前遍历的key相等，则找到了该元素，直接返回；从这里可以看出是否有comparator分化成了两个方法，但是内部逻辑一模一样，因此可见笔者comparator = (k1, k2) -&gt; ((Comparable&lt;? super K&gt;)k1).compareTo(k2);这种改造的必要性。我是一条美丽的分割线，前方高能，请做好准备。特性再回顾每个节点或者是黑色，或者是红色。根节点是黑色。每个叶子节点（NIL）是黑色。（注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！）如果一个节点是红色的，则它的子节点必须是黑色的。从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。左旋左旋，就是以某个节点为支点向左旋转。整个左旋过程如下：将 y的左节点 设为 x的右节点，即将 β 设为 x的右节点；将 x 设为 y的左节点的父节点，即将 β的父节点 设为 x；将 x的父节点 设为 y的父节点；如果 x的父节点 为空节点，则将y设置为根节点；如果x是它父节点的左（右）节点，则将y设置为x父节点的左（右）节点；将 x 设为 y的左节点；将 x的父节点 设为 y；让我们来看看TreeMap中的实现：12345678910111213141516171819202122232425262728293031323334353637/** * 以p为支点进行左旋 * 假设p为图中的x */private void rotateLeft(Entry&lt;K,V&gt; p) { if (p != null) { // p的右节点，即y Entry&lt;K,V&gt; r = p.right; // （1）将 y的左节点 设为 x的右节点 p.right = r.left; // （2）将 x 设为 y的左节点的父节点（如果y的左节点存在的话） if (r.left != null) r.left.parent = p; // （3）将 x的父节点 设为 y的父节点 r.parent = p.parent; // （4）... if (p.parent == null) // 如果 x的父节点 为空，则将y设置为根节点 root = r; else if (p.parent.left == p) // 如果x是它父节点的左节点，则将y设置为x父节点的左节点 p.parent.left = r; else // 如果x是它父节点的右节点，则将y设置为x父节点的右节点 p.parent.right = r; // （5）将 x 设为 y的左节点 r.left = p; // （6）将 x的父节点 设为 y p.parent = r; }}右旋右旋，就是以某个节点为支点向右旋转。整个右旋过程如下：将 x的右节点 设为 y的左节点，即 将 β 设为 y的左节点；将 y 设为 x的右节点的父节点，即 将 β的父节点 设为 y；将 y的父节点 设为 x的父节点；如果 y的父节点 是 空节点，则将x设为根节点；如果y是它父节点的左（右）节点，则将x设为y的父节点的左（右）节点；将 y 设为 x的右节点；将 y的父节点 设为 x；让我们来看看TreeMap中的实现：123456789101112131415161718192021222324252627282930313233343536/** * 以p为支点进行右旋 * 假设p为图中的y */private void rotateRight(Entry&lt;K,V&gt; p) { if (p != null) { // p的左节点，即x Entry&lt;K,V&gt; l = p.left; // （1）将 x的右节点 设为 y的左节点 p.left = l.right; // （2）将 y 设为 x的右节点的父节点（如果x有右节点的话） if (l.right != null) l.right.parent = p; // （3）将 y的父节点 设为 x的父节点 l.parent = p.parent; // （4）... if (p.parent == null) // 如果 y的父节点 是 空节点，则将x设为根节点 root = l; else if (p.parent.right == p) // 如果y是它父节点的右节点，则将x设为y的父节点的右节点 p.parent.right = l; else // 如果y是它父节点的左节点，则将x设为y的父节点的左节点 p.parent.left = l; // （5）将 y 设为 x的右节点 l.right = p; // （6）将 y的父节点 设为 x p.parent = l; }}","link":"/2016/07/01/TreeMap内部原理(一)/"},{"title":"小议 Docker：Docker 基础","text":"Linux 平台是 Docker 原生支持的平台，在 Linux 上使用 Docker 可以得到最佳的用户体验。Docker 安装Docker 是一个轻量级虚拟化技术，它具备传统虚拟机无可比拟的优势。它更简易的安装和使用方式、更快的速度、服务集成与开发流程自动化，都使Docker 被广大技术爱好者青睐。安装 Docker 的基本要求Docker 只支持 64 位 CPU 架构的计算机，目前不支持 32 位 CPU建议系统的 Linux 内核版本为 3.10 及以上Linux 需开启 cgroups 和 namespace 功能Docker 操作参数解读Docker 的命令行工具是我们日常使用的重点，本节将有选择的介绍其部分功能。为了了解命令行工具的概括，我们可以使用 docker 命令或docker help命令来获取 Docker 的命令清单。12345678910➜ ~ dockerUsage: docker [OPTIONS] COMMAND...... unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codesRun 'docker COMMAND --help' for more information on a command.随着 Docker 的不断发展，docker 的子命令已经达到 41 个，其中核心子命令还有复杂的可选执行参数。对于每一个特定的子命令，用户可以使用 docker COMMNAD —help命令来查看该子命令的详细信息，包括子命令的使用方法及可用的操作参数。以下这个例子使用docker start --help命令获取子命令 start 的详细信息。12345678910111213➜ ~ docker start --helpUsage: docker start [OPTIONS] CONTAINER [CONTAINER...]Start one or more stopped containersOptions: -a, --attach Attach STDOUT/STDERR and forward signals --checkpoint string Restore from this checkpoint --checkpoint-dir string Use a custom checkpoint storage directory --detach-keys string Override the key sequence for detaching a container -i, --interactive Attach container's STDIN下面是根据命令的用途对其进行分类：子命令分类子命令Docker 环境信息info、version容器生命周期管理create、exec、kill、pause、restart、rm、run、start、stop、unpause镜像仓库命令login、logout、pull、push、search镜像管理build、images、import、load、rmi、save、tag、commit容器运维操作attach、export、inspect、port、ps、rename、stats、top、wait、cp、diff、update容器资源管理volume、network系统日志信息events、history、logs从 Docker 命令使用出发，梳理出如下的命令结构图，以更进一步了解 Docker 命令行工具[1]。根据子命令分类清单，选取每个功能分类中常用的子命令进行用法和操作参数的解读。Docker 环境信息docker info命令用于检测 Docker 是否正确安装。如果安装正确，该命令会输出 Docker 的配置信息。docker info命令一般结合docker version命令使用，这样能够提取到足够详细的 Docker 环境信息。1234567891011121314➜ ~ docker infoContainers: 14 Running: 0 Paused: 0 Stopped: 14Images: 11Server Version: 18.09.1Storage Driver: overlay2 Backing Filesystem: extfs...Kernel Version: 4.9.125-linuxkitOperating System: Docker for MacOSType: linux...12345678910111213➜ ~ docker versionClient: Docker Engine - Community Version: 18.09.1 API version: 1.39 Go version: go1.10.6...Server: Docker Engine - Community Engine: Version: 18.09.1 API version: 1.39 (minimum version 1.12) Go version: go1.10.6...容器生命周期管理容器生命周期管理涉及容器启动、停止等功能，下面选取最常用的 docker run命令和负责启动停止的docker start/stop/restart命令举例。docker run 命令docker run 命令使用方法如下：1docker run [OPTIONS] IMAGE [COMMAND] [ARG...]docker run 命令是 Docker 的核心命令之一，用户可以选用的选项近100 个（目前 92 个），所有选项的说明可以通过 docker run —help命令查看。docker run命令用来基于特定的镜像创建一个容器，并依据选项来控制该容器。具体使用示例如下：12➜ ~ docker run ubuntu echo \"hello word\"hello word这是docker run命令最基本的使用方法，改名了从 Ubuntu 镜像启动一个容器，并执行 echo 命令打印出 hello word。执行完 echo 命令后，容器将停止运行。docker run命令启动的容器会随记分配一个容器 ID（container id），用以标识该容器。在选取启动容器的镜像时，可以在镜像名后添加 tag 来区分同名的镜像，如 ubuntu:latest、ubuntu:13.04、ubunt:14.04。如在选取镜像启动容器时，用户未指定具体 tag，Docker 将默认选取 tag 为 latest 的镜像。掌握了基本用法后，结合 docker run命令丰富的选项，能够实现更加复杂的功能。来看示例：12➜ ~ docker run -i -t --name mytest ubuntu:latest /bin/bashroot@e471b4225043:/#上例中，docker run命令启动一个容器，并为它分配一个伪终端执行 /bin/bash 命令，用户可以在该伪终端与容器进行交互。其中：-i 选项表示使用交互模式，始终保持输入流开放-t 选项表示分配一个伪终端，一般两个参数结合时使用 -it，即可在容器中利用打开的伪终端进行交互操作-- name 选项可以指定 docker run命令启动的容器的名字，若无此选项，Docker 将为容器随机分配一个名字除了上面示例中使用到的这些，docker run命令还有其他常用的选项：-c 选项用于给运行在容器中的所有进程分配 CPU 的 shares 值，这是一个相对权重，实际的处理速度还与宿主机的 CPU 有关-m 选项用于限制为容器中所有进程分配的内存总量，以 B、K、M、G 为单位-v 选项用于挂载一个 volume，可以用多个-v 参数同时挂载多个 volume。volume 的格式为[host-dir]:[container-dir]:[rw|ro]-p 选项用于将容器的端口暴露给宿主机的端口，其常用格式为 hostport:container-port。通过端口的暴露，可以让外部主机通过宿主机暴露的端口来访问容器内的应用。其中，前三个选项对于 Docker 的资源管理的作用非常显著，在后面有更详细的解释。docker start/stop/restart 命令docker run命令可以新建一个容器来运行，而对于已经存在的容器，可以通过 docker start/stop/restart/ 命令来启动、停止和重启。利用docker run命令新建一个容器时，Docker 将自动为每个新容器分配唯一的 ID 作为标识。docker start/stop/restart 命令一般利用容器 ID 标识确定具体容器，在一写情况下，也使用容器名来确定容器。docker start 命令使用 -i 选项来开启交互模式，始终保持输入流开发。使用 -a 选项来附加标准输入、输出或错误输出。此外，docker stop 和 docker restart 命令使用 -t 选项来设定容器停止前的等待时间。Docker registryDocker registry 是存储容器镜像的仓库，用户可以通过 Docker client 与 Docker registry 进行通信，以此来完成镜像的搜索、下载和上传等相关操作。Docker Hub 是由 Docker 公司在互联网上提供的一个镜像仓库，提供镜像的共有与私有存储服务，它是用户最主要的镜像来源。除了 Docker Hub 以外，用户还可以自行搭建私有服务器来实现镜像仓库的功能。下面选取最常用的 docker pull 和 push 命令举例。docker pull 命令docker pull 命令是 Docker 中的常用命令，主要用于从 Docker registry 中拉取 image 或 repository。在 Docker 官方仓库 Docker Hub 中有许多即拿即用的镜像资源，通过docker pull 命令可以有效的利用他们，这也体现了 Docker「一次编译，到处运行」的特性。同时，当镜像被拉取到本地后，用户可以在其现有的基础上做出自身的更改操作，这也大大加快了应用的开发进程。该命令的使用方法如下：1docker pull [OPTIONS] NAME[:TAG @DIGEST]示例：12345678# 从官方 Hub 拉取 ubuntu:latest 镜像docker pull ubuntu# 从官方 Hub 拉取指明 \"ubuntu 12.04\" tag 的镜像docker pull ubuntu:ubuntu12.04# 从特定的仓库拉取 ubuntu 镜像docker pull SEL/ubuntu# 从其他服务器拉取镜像docker pull 192.168.100.1:5000/sshddocker push 命令与 docker pull 命令向对应的 docker push 命令，可以将本地的 image 或repository 推送到 Docker Hub 的公共或私有镜像库，以及私有服务器。使用方法如下：1docker push [OPTIONS] NAME[:TAG]示例：1docker push SEL/ubuntu镜像管理docker images 命令通过 docker images 命令可以列出主机上的镜像，默认只列出最顶层的镜像，可以使用 -a 选项显示所有镜像。使用方法：1docker images [OPTIONS] [REPOSITORY[:TAG]]示例：12345➜ ~ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 4c108a37151f 2 days ago 64.2MBzendesk/maxwell latest 4e899bf758ed 2 months ago 1.05GBcanal/canal-server latest b9c3d95520a5 2 months ago 831MB上例中，从 REPOSITORY 属性可以判断出是来自于官方镜像、私人仓库还是私有服务器。docker rmi 和 docker rm 命令这两个子命令的功能都是删除，其中 docker rmi 命令用于删除镜像， docker rm 命令用于删除容器。他们可同时删除多个镜像或容器，也可按条件来删除。使用方法：12docker rm [OPTIONS] CONTAINER [CONTAINER...]docker rmi [OPTIONS] IMAGE [IMAGE...]需要注意的是，使用 rmi 命令删除镜像时，如果已有基于该镜像启动的容器存在，则无法直接删除，需要首先删除容器。当然，这两个子命令读提供了 -f 选项，可强制删除存在容器的镜像或启动中的容器。容器运维操作作为 Docker 的核心，容器的操作是重中之重，Docker 为用户提供了丰富的容器运维操作命令，以常用的 attach、inspect 及 ps 子命令举例。docker attach 命令docker attach 命令对于开发者来说十分有用，它可以连接到正在运行的容器，观察该容器的运行情况，或与容器的主进程进行交互。使用方法：1docker attach [OPTIONS] CONTAINERdocker inspect 命令docker inspect 命令可以查看镜像和容器的详细信息，默认会列出全部信息，可以通过 --format 参数来指定输出的模板格式，以便输出特定信息。使用方法：1docker inspect [OPTINOS] CONTAINER|IMAGE [CONTAINER|IMAGE...]示例：123# 查看容器内部 IP➜ ~ docker inspect --format='{{.NetworkSettings.IPAddress}}' c5b3f7901fdf172.17.0.3docker ps 命令docker ps 命令可以查看容器的相关信息，默认只显示正在运行的容器的信息。可以查看到的信息包括 CONTAINER ID、NAMES、IMAGE、STATUS、容器启动后执行的 COMMAND、创建时间 CREATED 和绑定开启的端口 PORTS。docker ps 命令最常用的功能就是查看容器的 CONTAINER ID，以便对特定容器进行操作。使用方法：1docker ps [OPTIONS]docker ps 命令常用的选项有 -a 和 -l。-a 参数可以查看所有容器，包括停止的容器；-l 参数则只查看最新创建的容器，包括停止的容器。其他子命令除了上述命令外，Docker 还有一系列非常有用的子命令，如固话容器为镜像的 commit 命令等。下面以一些较为常用的子命令举例。docker commit 命令commit 命令可以将一个容器固化为一个新的镜像。当需要制作特定的镜像时，会进行修改容器的配置，如在容器中安装特定工具等，通过 commit 命令可以讲这些修改保存起来，使其不会因为容器的停止而丢失。使用方法：1docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]提交保存时，只能选用正在运行的容器（即可以通过 docker ps 命令看到的容器）来制作新的镜像。但在制作镜像时， commit 命令只是一个临时性的辅助命令，不推荐使用。官方建议通过 docker build 命令集合 Dockerfile 创建和管理镜像。events、history 和 logs 命令这三个命令用于查看 Docker 的系统日志信息。events 命令会打印出实时的系统事件；history 命令会打印出指定镜像的历史版本信息，即构建该镜像的每一层镜像的命令记录；logs 命令会打印出容器中进程的运行日志。使用方法：123docker events [OPTIONS]docker history [OPTIONS] IMAGEdocker tags [OPTIONS] CONTAINER参考资料1.https://blog.octo.com/en/docker-registry-first-steps/ ↩","link":"/2017/11/22/小议Docker-Docker基础/"},{"title":"小议 Docker：容器、容器云","text":"从 2013 年发布至今，Docker 一直保持着旺盛的生命力。Docker 选择容器作为核心和基础，依靠容器技术支撑的 Docker 迅速成为国内外各大云计算厂商和开发者手中的至宝。云计算平台云计算时代蕴育出了众多的云计算平台，虽然在服务类型后平台功能上有所差异，但他们的本质上如出一辙，都与 NIST 对于云计算平台的定义有着密切的关系。云计算是一种资源服务模式，能便捷按需地将资源（网络、服务器、存储、应用及服务）快速供应并释放。经典云计算架构包括 IaaS（Infrastructure as a Service，基础设施即服务）、PaaS（Platform as a Service，平台即服务）、SaaS（Software as a Service， 软件即服务）三层。IaaS 层为基础设施运维人员服务，提供计算、存储、网络及其他基础资源，云平台使用者可以在上面部署和运行包括操作系统和应用程序在内的任意软件，无需再为基础设施的管理而分心。PaaS 层为应用开发人员服务，提供支撑应用运行所需的软件运行环境、相关工具与服务，如数据库服务、日志服务、监控服务、服务发现等，让应用开发者可以专注于核心业务的开发。SaaS 层为一般用户服务，提供了一套完整可用的软件系统，让一般用户无需关注技术细节，只需要通过浏览器、应用客户端等方式就能使用部署在云上的应用服务。同时，随着计算机技术推陈出新，应用的规模愈发庞大，逻辑愈发复杂，迭代更新愈发频繁，应用开发所需的统一规范和原有开发模式杂乱无章成了追求进步的主要障碍。在尖锐的矛盾中，云时代应用生命周期管理机制（Application Lifecycle Management，ALM）和十二要素应用规范[1] （The Twelve-Factor App）应运而生。所有理论设计和预想一定是美好的。IaaS 的发展主要以虚拟机为最小粒度的资源调度单位，出现了资源利用率低、调度分发缓慢、软件栈环境不统一等一系列问题。PaaS 在 IaaS 基础上发展而来，众多 PaaS 已经意识到可以利用容器技术解决资源利用率问题，但是 PaaS 通常在应用架构选择、支持的软件环境服务方面有较大的限制，这带来了应用于平台无法解耦、应用运行时环境局限性强、运维人员控制力下降的问题。可见不论 IaaS 还是 PaaS 都有各自适用的场景，但依旧存在诸多缺陷，人们亟需一个真正可用的解决方案。容器，新的革命每一场革命背后都有着深刻的历史背景和矛盾冲突，新陈代谢是历史的必然结果，新生取代陈旧得益于理念的飞跃和对时代发展需求的契合，很显然 Docker 抓住了这个契机。Docker 是什么？根据官方的定义，Docker 是以 Docker容器为资源分隔和调度的基本单位，封装整个软件运行时环境，为开发者和系统管理员设计的，用于构建、发布和运行分布式应用的平台。它是一个跨平台、可移植并且简单易用的容器解决方案。Docker 的源代码托管在Github 上，基于 Go 语言开发并遵从 Apache 2.0 协议。Docker 可以在容器内部快速自动化的部署应用，并通过操作系统内核技术（namespace、cgroups 等）为容器提供资源隔离与安全保障。比较容器和虚拟机[2]容器容器是应用层的抽象，它将代码和依赖关系打包在一起。多个容器可以在同一台机器上运行，并与其他容器共享操作系统内核，每个容器在用户空间中作为独立进程运行。容器占用的空间比 VM 少（容器映像的大小通常为几十 MB），可以处理更多的应用程序，并且需要更少的 VM 和操作系统。虚拟机虚拟机（VM）是物理硬件的抽象，将一台服务器转变为多台服务器。管理程序允许多台 VM 在单台机器上运行。每个 VM 都包含操作系统的完整副本，应用程序，必要的二进制文件和库，这带来体积巨大（占用数十 GB是常态）；在启动速度上，虚拟机也可能很慢。容器 x 虚拟机现在，将容器和虚拟机结合起来使用， 为部署和管理应用程序提供了极大的灵活性。容器技术的好处容器技术的生态系统自下而上分别覆盖了 IaaS 层和 PaaS 层所涉及的各类问题，包括资源调度、编排、部署、监控、配置管理、存储网络管理、安全、容器化应用支撑平台等。除了基于容器技术解决构建分布式平台无法回避的经典问题，容器技术主要带来了以下几点好处：持续部署与测试容器消除了线上线下环境差异，保证了应用生命周期的环境一致性和标准化。开发人员使用镜像实现标准开发环境的构建，开发完成后通过封装着完整环境和应用的镜像进行迁移，由此，测试和运维人员可以直接部署软件镜像来进行测试和发布，大大简化了持续集成、测试和发布的过程。跨云平台支持容器带来的最大好处之一就是其适配性，越来越多的云平台都支持容器，用户再也无需担心受到云平台的捆绑，同时也让应用多平台混合部署成为可能。目前支持容器的国外 IaaS 云平台包括但不限于亚马逊云平台（AWS）、Google 云平台（GCP）、微软云平台（Azure）、OpenStack 等，国内 IaaS 云台包括但不限于阿里云、腾讯云、华为云、金山云等等云厂商。还包括如 Chef、Puppet、Ansible 等配置管理工具。环境标准化和版本控制基于容器提供的环境一致性和标准化，你可以使用 Git 等工具对容器镜像进行版本控制，相比基于代码的版本控制来说，你还能够对整个应用运行环境实现版本控制，一旦出现故障可以快速回滚。相比以前的虚拟机镜像，容器压缩和备份速度更快，镜像启动也像启动一个普通进程一样快速。高资源利用率与隔离容器没有管理程序的额外开销，与底层共享操作系统，性能更加优良，系统负载更低，在同等条件下可以运行更多的应用实例，可以更充分的利用系统资源。同时，容器拥有不错的资源隔离与限制能力，可以精确的对应用分配 CPU、内存等资源，保证了应用见不会相互影响。容器跨平台性与镜像Linux 容器虽然早在 Linux2.6 版本内核已经存在，但是缺少容器的跨平台性，难以推广。容器在原有 Linux 容器的基础上进行大胆革新，为容器设定了一整套标准化的配置方法，将应用及其依赖的运行环境打包成镜像，真正实现了「构建一次，到处运行 」的理念，大大提高了容器的跨平台性。易于理解且易用Docker 的英文原意是处理集装箱的码头工人，标志是鲸鱼运送一大堆集装箱，集装箱就是容器，生动好记，易于理解。一个开发者可以在 15 分钟之内入门 Docker 并进行安装和部署，这是容器使用史的一次飞跃。因为他的易用性，有更多的人开始关注容器技术，加速了容器标准化的步伐。应用镜像仓库Docker 官方构建了一个镜像仓库，组织和管理形式类似于 Github，其上已经累积了成千上万的镜像。因为 Docker 的跨平台适配性，相当于为用户提供了一个非常有用的应用商店，所有人都可以自由的下载微服务组件，这位开发者提供了巨大便利。进化从容器到容器云容器为用户打开了一扇通往新世界的大门，真正进入这个容器的世界后，却发现新的生态系统如此庞大。在生产和使用中，不论是个人还是企业，都会提出更复杂的需求。这时，我们需要众多跨主机的容器协同工作，需要支持各种类型的工作负载，企业级应用开发更是需要基于容器技术，实现支持多人协作的持续集成、持续交付平台。即使 Docker 只需要一条命令便可启动一个容器，一旦试图将其推广到软件开发和生产环境中，麻烦便层出不穷，容器相关的网络、存储、集群、高可用等就是不得不面对的问题。从容器到容器云的进化应运而生。什么是容器云容器云以容器为资源分割和调度的基本单位，封装整个软件运行时环境，为开发者和系统管理员提供用于构建、发布和运行分布式应用的平台。当容器云专注于资源共享与隔离、容器编排与部署时，它更接近传统的 IaaS；当容器云渗透到应用支撑与运行时环境时，它更接近传统的 PaaS。容器云并不仅限于 Docker，基于 rkt 容器的 CoreOS 项目也是容器云。Docker 的出现让人们意识到了容器的价值，使得一直以来长期存在但并未被重视的轻量级虚拟化技术得到快速的发展和应用。参考资料1.十二要素应用规范 https://12factor.net/zh_cn/ ↩2.容器、虚拟机 https://www.docker.com/resources/what-container ↩","link":"/2017/11/02/小议Docker-容器和容器云/"},{"title":"JVM Internals","text":"本文系转载，并在转载译文基础上根据笔者经验略作修改。原文链接在文末一、前言本文将介绍JVM内部架构。下图展示符合Java7规范的JVM内部主要组件。下面我们将上述组件分为线程相关和线程独立两种类型来介绍。二、ThreadJVM允许进程包含多个并发的线程。Hotspot JVM中的Java线程与OS线程是一一对应的。当线程工作存储区（thread-local storage）、配置缓存（allocation buffers）、同步对象（synchronized objects）、栈和本地栈（stacks）和程序计数器（pragram counter）等Java线程相关的状态均准备好后，就会启动OS线程并有OS线程执行run函数。OS负责线程的调度。当以正常方式或异常抛出的方式退出run函数，OS线程均会判断当前Java线程的终止是否会导致进程的终止（进程的工作线程是否都终止了？），若要终止进程的化，则释放Java线程和OS线程所占的资源，否则就释放Java线程的资源，并回收OS线程。1、JVM System Threads若你用过jconsole或其他调试工具，你会发现除了主线程外还存在数个有JVM创建的系统线程。Hotspot JVM的系统线程有这5个：2、VM thread（虚拟机线程）VM thread 用于为一些需要防止堆变化操作提供执行环境，当要执行防止堆变化的操作时，就是要求JVM启动安全点（safe-point）,此时将会暂停GC、线程栈操作、线程恢 复和偏向锁解除。3、Periodic task thread（周期性任务线程）Periodic task thread负责定时事件（如interrupts），用于周期性执行计划任务4、GC threads（垃圾回收线程）GC threads 负责不同类型垃圾回收活动。5、Compiler threads（编译器线程）Compiler threads用于在运行时将字节码编译为CPU本地代码。6、Signal dispatcher thread（信号量分发线程）Singal dispatcher thread用于接收发送给JVM的信号量，并将其分发到合适的JVM方法来处理。三、Per Thread每个线程的执行环境均有以下的组件。1、Program Counter(PC)（程序计数器）用于存放当前指令（或操作码）的地址，若该指令为本地指令那么PC为undefined。当执行完当前指令后PC会自增（根据当前指令的定义自增1或N）从而指向下一个指令的地 址，那么JVM就可以知道接下来要执行哪个指令了。事实上PC存放的是方法区（Method Area）中的内存地址。2、Stack（堆栈）每个线程有自定独立的堆栈用于存放在该线程执行的方法。堆栈是一个后进先出（LIFO）的数据结构，元素称为栈帧（frame）。当将要在线程上执行某方法时，则需要将代表 该方法的栈帧压栈，当方法执行完毕后（正常退出或抛出未处理的异常）则将栈帧弹栈。栈帧可能分配在堆上（heap），而堆栈并不需要连续的存储空间。3、Native Stack（本地堆栈）不是每种JVM都支持本地方法，对于支持本地方法的JVM它门会提供线程本地堆栈。若JVM实现了通过C链接模型（C-linkage Model）来实现JNI，那么本地堆栈实质就是C堆 栈（入参顺序和返回值均与C程序一致）。本地方法一般都可以调用Java方法，此时会在Java的堆栈中压入一个栈帧并按执行Java方法的流程处理。Stack Restrictions（堆栈约束）：堆栈的容量有动态和固定两种。当栈帧数量大于堆栈容量时就会抛出StackOverflowError；当堆中没有足够内存来分配新栈帧时则抛出OutOfMemoryError。4、Frame（堆栈的元素——栈帧）4.1、Local Varibles Array（局部变量表）局部变量表用于存放方法执行过程中this引用、方法入参和局部变量。对于静态方法而言方法参数从局部变量表的第一位开始（下标为0），对于实例方法而言方法参数从局部变量表的第二位开始（下标为1，第一位是this引用）。局部变量表内可包含以下类型数据，boolean/byte/char/long/short/int/float/double/reference/returnAddress。局部变量表的每个元素占32bit，每32bit称为1个slot。上述所支持的类型中除了long和double外均占1个slot，而它俩就占2个slot。4.2、Operand Stack（操作数栈）在执行方法内部的字节码指令时需要使用操作数栈，大多数JVM的字节码指令是用于操作操作数栈（压栈、弹栈、赋值栈帧、栈帧互换位置或执行方法操作栈帧），实现数据在操作数栈和局部变量表之间频繁移动。示例如下：123456//java codeint i;// bytecode0: iconst_0 // 将0压栈1: istore_1 // 弹栈并将值赋值到局部变量表的第二个Slot槽中4.3、Dynamic Linking（动态链接）每个栈帧均包含一个指向运行时常量池（runtime constant pool）的引用。通过这个运行时常量池来实现动态链接。C/C++的代码会被编译成一个一个独立的对象文件，并通过静态链接将对多个对象文件生成一个执行文件或dll类库。在链接阶段所有的符号引用会被直接引用取代，而直接引用则为相对于可执行文件的进程入口地址的相对地址。而Java的链接阶段是在运行时动态发生的。当将Java类编译成字节码时，所有对变量和方法的引用将被保存为常量池表中的一条条符号引用表项，这些符号引用为逻辑引用而不是指向物理内存地址的引用。JVM可以选择不同的时刻将符号引用转换为直接引用。一种是当class文件加载并验证通过后，这种称为静态处理（eager or static resolution）；另一种是在使用时才转换为直接引用，这种称为懒处理（lazy or late resolution）。对于字段通过绑定来处理，对于对象或类则通过将符号引用转换直接引用来识别，动态链接后原有的符号引用将被直接引用替换，因此对于同一个符号引用，动态链接的操作仅发生一次。假如直接引用的类还未加载，则会加载该类。而直接引用所包含的地址相对于变量和方法在运行时的地址。Shared Between Threads四、Heap（堆）堆用于在运行时分配对象和数组。由于栈帧的容量是固定的，因此无法将对象和数组等容量可变的数据存放到堆栈中，而是将对象和数组在堆中的地址存放在栈帧中从而操作对象和数组。由于对象和数组是存放在堆，因此需要通过垃圾回收器来回收它们所占的内存空间。垃圾回收机制将堆分成3部分：新生代（再细分为初生空间和幸存空间）老年代永久代（译者语：永久代不在堆上）五、Memory Management（内存管理）对象和数组不能被显式地释放，必须通过垃圾回收器来自动回收。一般的工作步骤如下：新创建的对象和数组被存放在新生代;次垃圾回收将会对新生代作操作，存活下来的将从初生空间移至幸存空间;主垃圾回收（一般会导致应用的其他所有线程挂起），会将新生代的对象爱嗯挪动到老年代;每次回收老年代对象时均会回收永久代的对象。当他们满的时候就会触发回收操作。六、Non-Heap Memory（非堆内存）非堆内存包含下列这些:永久代方法区字符串区代码缓存用于存放被JIT编译器编译为本地代码的方法。七、Just In Time (JIT) Compilation（JIT编译）Java的字节码是解析执行的，速度比CPU本地代码差远了。为了提高Java程序的执行效率，Oracle的Hotspot虚拟机将需要经常执行的字节码编译成本地代码并存放在代码缓存当中。Hotspot虚拟机会自动权衡解析执行字节码和将字节码编译成本地代码再执行之间的效率，然后选择最优方案。八、Method Area（方法区）方法区存放每个类的信息，具体如下：类加载器引用运行时常量池1. 数字常量字段引用方法引用属性字段数据，每个字段包含以下信息名称类型修饰符属性方法数据，每个方法包含以下信息名称返回值类型入参的数据类型（保持入参的次序）修饰符属性方法代码，每个方法包含以下信息字节码操作数栈容量局部变量表容量局部变量表异常表，每个异常表项包含以下信息起始地址结束地址异常处理代码的地址异常类在常量池的地址所有线程均访问同一个方法区，因此方法区的数据访问和动态链接操作必须是线程安全才行。假如两个线程试图访问某个未加载的类的字段或方法时，则会先挂起这两个线程，等该类加载完后继续执行。九、 Class File Structure（Class文件结构）123456789101112131415161718ClassFile { u4 magic; u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info contant_pool[constant_pool_count – 1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count];}magic、minor_version、major_version：用于声明JDK版本constant_pool：类似符号表，但包含更多的信息access_flags：存放该类的描述符列表this_class：指向constant_pool中CONSTANT_Class_info类型常量的索引，该常量存放的是符号引用到当前类（如org/jamesdbloom/foo/bar）super_class：指向constant_pool中CONSTANT_Class_info类型常量的索引，该常量存放的是符号引用到超类（如java/lang/Object）interfaces：一组指向constant_pool中CONSTANT_Class_info类型常量的索引，该类常量存放的是符号引用到接口fields：字段表，一个表项代表一个字段，表项的子项信息均有constant_pool提供。methods：方法表，一个表项代表一个方法，表项的子项信息均有constant_pool提供。attributes：属性表，表项用于类提供额外的信息。java代码中通过注解（约束为RetentionPolicy.CLASS或RetentionPolicy.RUNTIME的annotation）提供通过javap命令我们可以查看解析后的字节码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// javapackage org.jvminternals;public class SimpleClass { public void sayHello() { System.out.println(&quot;Hello&quot;); }}// shell or cmdjavap -v -p -s -sysinfo -constants classes/org/jvminternals/SimpleClass.class// Bytecodespublic class org.jvminternals.SimpleClass SourceFile: &quot;SimpleClass.java&quot; minor version: 0 major version: 51 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #6.#17 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #18.#19 // java/lang/System.out:Ljava/io/PrintStream; #3 = String #20 // &quot;Hello&quot; #4 = Methodref #21.#22 // java/io/PrintStream.println:(Ljava/lang/String;)V #5 = Class #23 // org/jvminternals/SimpleClass #6 = Class #24 // java/lang/Object #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 Lorg/jvminternals/SimpleClass; #14 = Utf8 sayHello #15 = Utf8 SourceFile #16 = Utf8 SimpleClass.java #17 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #18 = Class #25 // java/lang/System #19 = NameAndType #26:#27 // out:Ljava/io/PrintStream; #20 = Utf8 Hello #21 = Class #28 // java/io/PrintStream #22 = NameAndType #29:#30 // println:(Ljava/lang/String;)V #23 = Utf8 org/jvminternals/SimpleClass #24 = Utf8 java/lang/Object #25 = Utf8 java/lang/System #26 = Utf8 out #27 = Utf8 Ljava/io/PrintStream; #28 = Utf8 java/io/PrintStream #29 = Utf8 println #30 = Utf8 (Ljava/lang/String;)V{ public org.jvminternals.SimpleClass(); Signature: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 3: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lorg/jvminternals/SimpleClass; public void sayHello(); Signature: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #3 // String &quot;Hello&quot; 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 6: 0 line 7: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lorg/jvminternals/SimpleClass;}字节码显示三个主要的区域：常量池、构造函数和sayHello方法。常量池：提供类似于符号表的信息。方法：每个方法均含四个区域签名和访问标志；方法体的字节码；行号表：为调试器提供Java代码与字节码的行号映射关系信息。局部变量表：罗列当且当前方法的所有局部变量名。（译者语：由于后续内容为对字节码指令的讲解，没什么必要翻译了所以…………..）十、Classloader（类加载器）JVM启动时通过bootstrap classloader加载初始类。在执行 public static void main(String[]) 方法前，这个类需要经过链接、初始化操作。然后在执行这个方法时就会触发其他类和接口的加载、链接和初始化操作。加载，通过特定的名称搜索类或接口文件，并将其内容加载为字节数组。（译者语：这里加载的工作已经完成了，后面内容是加载+链接的内容）然后字节数组被解析为符合Java版本号的类对象（如Object.class），而该类或接口的直接父类和直接父接口也会被加载。链接，由验证Class文件合法性、准备和可选的解析三个步骤组成。1. 验证，就是要根据Java和JVM规范对类或接口字节码的格式和语义进行校验。下面罗列部分校验项：1.1. 符号表具有一致和合法的格式；1.2. 不可更改的方法和类没有被重写；1.3. 方法含有效的访问控制关键字；1.4. 方法含有效的入参类型和数目；1.5. 字节码没有对操作数栈进行非法操作；1.6. 变量先初始化后使用；1.7. 变量值与变量类型匹配。在类加载阶段进行验证虽然会减慢加载速度，但可以减少运行时对同一类或接口进行重复验证。2. 准备，为静态字段、静态方法和如方法表等JVM使用的数据分配内存空间，并对静态字段进行初始化。但这个时候该类或接口的构造函数、静态构造函数和方法均没有被执行。3. 解析（可选项），检查符号引用并加载所引用的类或接口（加载直接父类和直接接口）。当没有执行这一步骤时，则在运行时中调用这个类或接口时在执行。\\初始化**，执行类的静态构造函数。JVM中有多个不同类型的类加载器。bootstrap classloader是顶层的类加载器，其他类加载器均继承自它。1. Bootstrap Classloader，由于在JVM加载时初始化，因此Bootstrap Classloader是用C++编写的。用于加载Java的核心API，如rt.jar等位于boot类路径的高信任度的类，而这些类在链接时需要的校验步骤比一般类要少不止一点点。2. Extenson Classloader，用于加载Java的扩展APIs。3. System Classloader，默认的应用类加载器，用于从classpath中加载应用的类。4. User Defined Classloaders，应用内部按一定的需求将对类分组加载或对类进行重新加载。十一、Faster Class Loading（更快的类加载）从HotSpot5.0开始引入了共享类数据（CDS）特性。在安装JVM时则会将如rt.jar中的类加载到一个内存映射共享文档中。然后各JVM实例启动时直接读取该内存中的类，提高JVM的启动速度。十二、 Where Is The Method Area（方法区在哪？）《Java Virtual Machine Specification Java SE 7 Edition》明确声明：“虽然方法区逻辑上位于堆中，简单的实现方式应该是被垃圾回收。”矛盾的是Oracle JVM的jconsole告知我们方法区和代码缓存是位于非堆内存空间中的。而OpenJDK则将代码缓存设置为虚拟机外的ObjectHeap中。十三、Classloader Reference（类加载器引用）每个类都持有一个指向加载它的类加载器指针，同样每个类加载都持有一组由它加载的类的指引。十四、Run Time Constant Pool（运行时常量池）每个类都对应一个运行时常量池（有Class文件中的常量池生成）。运行时常量池与符号表类似但包含更多的信息。字节码指令中需要对数据进行操作，但由于数据太大无法直接存放在字节码指令当中，于是通过将数据存放在常量池，而字节码指令存放数据位于常量池的索引值来实现指令对数据的操作。动态链接也是通过运行时常量池来实现的。运行时常量池包含以下的类型的数据：数字字面量；字符串字面量；类引用；字段引用；方法引用。举个栗子：1234567// javaObject foo = new Object();// bytecodes0: new #2 // Class java/lang/Object1: dup2: invokespecial #3 // Method java/ lang/Object &quot;&lt;init&gt;&quot;()Vnew操作码后的#2操作数就是常量池第2项的索引，该项为类型引用，内含一个缩略UTF8类型的常量来存放类的全限定名（java/lang/Object）。在进行动态符号链接时则通过该名称来查找类对象java.lang.Object。而new操作码会创建一个类的实例、初始化实例的字段，并将该对象压入操作数栈。dup复制栈顶元素并压栈，然后invokespecial则弹出操作数栈顶的一个元素执行对象的构造函数。再举个栗子：12345678910111213141516171819202122232425262728293031323334353637383940414243// javapackage org.jvminternals;public class SimpleClass { public void sayHello() { System.out.println(&quot;Hello&quot;); }}// BytecodesConstant pool: #1 = Methodref #6.#17 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #18.#19 // java/lang/System.out:Ljava/io/PrintStream; #3 = String #20 // &quot;Hello&quot; #4 = Methodref #21.#22 // java/io/PrintStream.println:(Ljava/lang/String;)V #5 = Class #23 // org/jvminternals/SimpleClass #6 = Class #24 // java/lang/Object #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 Lorg/jvminternals/SimpleClass; #14 = Utf8 sayHello #15 = Utf8 SourceFile #16 = Utf8 SimpleClass.java #17 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #18 = Class #25 // java/lang/System #19 = NameAndType #26:#27 // out:Ljava/io/PrintStream; #20 = Utf8 Hello #21 = Class #28 // java/io/PrintStream #22 = NameAndType #29:#30 // println:(Ljava/lang/String;)V #23 = Utf8 org/jvminternals/SimpleClass #24 = Utf8 java/lang/Object #25 = Utf8 java/lang/System #26 = Utf8 out #27 = Utf8 Ljava/io/PrintStream; #28 = Utf8 java/io/PrintStream #29 = Utf8 println #30 = Utf8 (Ljava/lang/String;)VClass的常量池包含以下类型：Integer 一个4bytes的整型常量Long 一个8bytes的长整型常量Float 一个4bytes的浮点型常量Double 一个4bytes的双精度浮点型常量String 字符串引用，指向一个缩略Utf8常量Utf8 缩略Utf8编码的字符串Class 类型引用，指向一个缩略Utf8常量，存放类全限定名（用于动态链接）NameAndType 存放两个引用，一个指向用于存放字段或方法名的缩略Utf8常量，一个指向存放字段数据类型或方法返回值类型和入参的缩略Utf8常量Fieldref, 存放两个引用，一个指向表示所属类或接口的Class常量，一个指向描述字段、方法名称和描述符的NameAndType常量Methodref,InterfaceMethodref十五、Exception Table（异常表）异常表的每一项表示一项异常处理，表项字段如下：起始位置、结束位置、处理代码的起始位置和指向常量池Class常量的位置索引。只要Java代码中出现try-catch或try-finally的异常处理时，就会创建异常表，异常表的表项用于存放try语句块在字节码指令集中的范围、捕捉的异常类和相应的字节码处理指令的起始位置。（译者注：try-finally所创建的表项的异常类引用为0）当发生异常并没有被捕获处理，则会从线程栈的当前栈帧抛出并触发弹栈操作，再栈顶栈帧接收，直到异常被某个栈帧捕获处理或该线程栈为空并退出线程然后异常有系统异常处理机制捕获。finally语句块的代码无论是否抛出异常均会执行。十六、Symbol Table（符号表）HotSpot虚拟机在永久代中增加了符号表。该表为哈希表用于将直接引用与运行时常量池的符号引用作映射。另外每个表项还有个引用计数器，用来记录有多少个符号引用指向同一个直接引用。假如某个类被卸载了那么类中的所有符号引用将无效，则对应的符号表表项的引用计数器减1，当计数器为0时则将该表项移除。十七、 Interned Strings (String Table)（字符串表）Java语言说明中要求字符串字面量必须唯一，一样的字符串字面量必须为同一个String实例。HotSport虚拟机通过字符串表来实现。字符串表位于永久代中，表项为String实例地址与字符串字面量的映射关系信息。加载类时成功执行链接的准备阶段时，Class文件常量池下的CONSTANT_String_info常量的信息均加载到字符串表中。而执行阶段可以通过String#intern()方法将字符串字面量加入到字符串表中。如：12new String(&quot;jvm&quot;) == &quot;jvm&quot;; // false(new String(&quot;jvm&quot;)).intern() == &quot;jvm&quot;; // trueString#intern()，会先去字符串表查找字面量相同的表项，有则返回对应的对象引用，没有则先将新的字符串对象和字面量添加到表中，然后再返回对象引用。总结本文对JVM内存模型做了概要的说明，让初次接触JVM的朋友对它有一个初步的big photo，在此感谢作者的分享。转载链接尊重原创，转载请注明来自：http://www.cnblogs.com/fsjohnhuang/p/4260417.html ^_^肥仔John原文地址：http://blog.jamesdbloom.com/JVMInternals.html","link":"/2017/02/11/JVM-Internals/"},{"title":"HashMap内部原理","text":"简介HashMap采用key/value存储结构，每个key对应唯一的value，查询和修改的速度都很快，能达到O(1)的平均时间复杂度。它是非线程安全的，且不保证元素存储的顺序；实战另见 HashMap继承体系HashMap实现了Cloneable，可以被克隆HashMap实现了Serializable，可以被序列化HashMap继承自AbstractMap，实现了Map接口，具有Map的所有功能存储结构在Java中，HashMap的实现采用了（数组 + 链表 + 红黑树）的复杂结构，数组的一个元素又称作桶。在添加元素时，会根据hash值算出元素在数组中的位置，如果该位置没有元素，则直接把元素放置在此处，如果该位置有元素了，则把元素以链表的形式放置在链表的尾部。当一个链表的元素个数达到一定的数量（且数组的长度达到一定的长度）后，则把链表转化为红黑树，从而提高效率。数组的查询效率为O(1)，链表的查询效率是O(k)，红黑树的查询效率是O(log k)，k为桶中的元素个数，所以当元素数量非常多的时候，转化为红黑树能极大地提高效率。源码解析属性1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * 默认的初始容量为16 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;/** * 最大的容量为2的30次方 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 默认的装载因子 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 当一个桶中的元素个数大于等于8时进行树化 */static final int TREEIFY_THRESHOLD = 8;/** * 当一个桶中的元素个数小于等于6时把树转化为链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * 当桶的个数达到64的时候才进行树化 */static final int MIN_TREEIFY_CAPACITY = 64;/** * 数组，又叫作桶（bucket） */transient Node&lt;K,V&gt;[] table;/** * 作为entrySet()的缓存 */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;/** * 元素的数量 */transient int size;/** * 修改次数，用于在迭代的时候执行快速失败策略 */transient int modCount;/** * 当桶的使用数量达到多少时进行扩容，threshold = capacity * loadFactor */int threshold;/** * 装载因子 */final float loadFactor;容量容量为数组的长度，亦即桶的个数，默认为16，最大为2的30次方，当容量达到64时才可以树化。装载因子装载因子用来计算容量达到多少时才进行扩容，默认装载因子为0.75。树化树化，当容量达到64且链表的长度达到8时进行树化，当链表的长度小于6时反树化。Node内部类Node是一个典型的单链表节点，其中，hash用来存储key计算得来的hash值。123456static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next;}TreeNode内部类这是一个神奇的类，它继承自LinkedHashMap中的Entry类，关于LInkedHashMap.Entry这个类我们后面再讲。TreeNode是一个典型的树型节点，其中，prev是链表中的节点，用于在删除元素的时候可以快速找到它的前置节点。12345678910111213141516// 位于HashMap中static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; { TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red;}// 位于LinkedHashMap中，典型的双向链表节点static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); }}HashMap()构造方法空参构造方法，全部使用默认值。123public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted}HashMap(int initialCapacity)构造方法调用HashMap(int initialCapacity, float loadFactor)构造方法，传入默认装载因子。123public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR);}HashMap(int initialCapacity)构造方法判断传入的初始容量和装载因子是否合法，并计算扩容门槛，扩容门槛为传入的初始容量往上取最近的2的n次方。1234567891011121314151617181920212223242526public HashMap(int initialCapacity, float loadFactor) { // 检查传入的初始容量是否合法 if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 检查装载因子是否合法 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; // 计算扩容门槛 this.threshold = tableSizeFor(initialCapacity);}static final int tableSizeFor(int cap) { // 扩容门槛为传入的初始容量往上取最近的2的n次方 int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;}put(K key, V value)方法添加元素的入口。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public V put(K key, V value) { // 调用hash(key)计算出key的hash值 return putVal(hash(key), key, value, false, true);}static final int hash(Object key) { int h; // 如果key为null，则hash值为0，否则调用key的hashCode()方法 // 并让高16位与整个hash异或，这样做是为了使计算出的hash更分散 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);}final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, i; // 如果桶的数量为0，则初始化 if ((tab = table) == null || (n = tab.length) == 0) // 调用resize()初始化 n = (tab = resize()).length; // (n - 1) &amp; hash 计算元素在哪个桶中 // 如果这个桶中还没有元素，则把这个元素放在桶中的第一个位置 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 新建一个节点放在桶中 tab[i] = newNode(hash, key, value, null); else { // 如果桶中已经有元素存在了 Node&lt;K, V&gt; e; K k; // 如果桶中第一个元素的key与待插入元素的key相同，保存到e中用于后续修改value值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 如果第一个元素是树节点，则调用树节点的putTreeVal插入元素 e = ((TreeNode&lt;K, V&gt;) p).putTreeVal(this, tab, hash, key, value); else { // 遍历这个桶对应的链表，binCount用于存储链表中元素的个数 for (int binCount = 0; ; ++binCount) { // 如果链表遍历完了都没有找到相同key的元素，说明该key对应的元素不存在，则在链表最后插入一个新节点 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 如果插入新节点后链表长度大于8，则判断是否需要树化，因为第一个元素没有加到binCount中，所以这里-1 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 如果待插入的key在链表中找到了，则退出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } // 如果找到了对应key的元素 if (e != null) { // existing mapping for key // 记录下旧值 V oldValue = e.value; // 判断是否需要替换旧值 if (!onlyIfAbsent || oldValue == null) // 替换旧值为新值 e.value = value; // 在节点被访问后做点什么事，在LinkedHashMap中用到 afterNodeAccess(e); // 返回旧值 return oldValue; } } // 到这里了说明没有找到元素 // 修改次数加1 ++modCount; // 元素数量加1，判断是否需要扩容 if (++size &gt; threshold) // 扩容 resize(); // 在节点插入后做点什么事，在LinkedHashMap中用到 afterNodeInsertion(evict); // 没找到元素返回null return null;}计算key的hash值；如果桶（数组）数量为0，则初始化桶；如果key所在的桶没有元素，则直接插入；如果key所在的桶中的第一个元素的key与待插入的key相同，说明找到了元素，转后续流程（9）处理；如果第一个元素是树节点，则调用树节点的putTreeVal()寻找元素或插入树节点；如果不是以上三种情况，则遍历桶对应的链表查找key是否存在于链表中；如果找到了对应key的元素，则转后续流程（9）处理；如果没找到对应key的元素，则在链表最后插入一个新节点并判断是否需要树化；如果找到了对应key的元素，则判断是否需要替换旧值，并直接返回旧值；如果插入了元素，则数量加1并判断是否需要扩容；resize()方法扩容方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102final Node&lt;K, V&gt;[] resize() { // 旧数组 Node&lt;K, V&gt;[] oldTab = table; // 旧容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 旧扩容门槛 int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { // 如果旧容量达到了最大容量，则不再进行扩容 threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 如果旧容量的两倍小于最大容量并且旧容量大于默认初始容量（16），则容量扩大为两部，扩容门槛也扩大为两倍 newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold // 使用非默认构造方法创建的map，第一次插入元素会走到这里 // 如果旧容量为0且旧扩容门槛大于0，则把新容量赋值为旧门槛 newCap = oldThr; else { // zero initial threshold signifies using defaults // 调用默认构造方法创建的map，第一次插入元素会走到这里 // 如果旧容量旧扩容门槛都是0，说明还未初始化过，则初始化容量为默认容量，扩容门槛为默认容量*默认装载因子 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int) (DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { // 如果新扩容门槛为0，则计算为容量*装载因子，但不能超过最大容量 float ft = (float) newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float) MAXIMUM_CAPACITY ? (int) ft : Integer.MAX_VALUE); } // 赋值扩容门槛为新门槛 threshold = newThr; // 新建一个新容量的数组 @SuppressWarnings({\"rawtypes\", \"unchecked\"}) Node&lt;K, V&gt;[] newTab = (Node&lt;K, V&gt;[]) new Node[newCap]; // 把桶赋值为新数组 table = newTab; // 如果旧数组不为空，则搬移元素 if (oldTab != null) { // 遍历旧数组 for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K, V&gt; e; // 如果桶中第一个元素不为空，赋值给e if ((e = oldTab[j]) != null) { // 清空旧桶，便于GC回收 oldTab[j] = null; // 如果这个桶中只有一个元素，则计算它在新桶中的位置并把它搬移到新桶中 // 因为每次都扩容两倍，所以这里的第一个元素搬移到新桶的时候新桶肯定还没有元素 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 如果第一个元素是树节点，则把这颗树打散成两颗树插入到新桶中去 ((TreeNode&lt;K, V&gt;) e).split(this, newTab, j, oldCap); else { // preserve order // 如果这个链表不止一个元素且不是一颗树 // 则分化成两个链表插入到新的桶中去 // 比如，假如原来容量为4，3、7、11、15这四个元素都在三号桶中 // 现在扩容到8，则3和11还是在三号桶，7和15要搬移到七号桶中去 // 也就是分化成了两个链表 Node&lt;K, V&gt; loHead = null, loTail = null; Node&lt;K, V&gt; hiHead = null, hiTail = null; Node&lt;K, V&gt; next; do { next = e.next; // (e.hash &amp; oldCap) == 0的元素放在低位链表中 // 比如，3 &amp; 4 == 0 if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { // (e.hash &amp; oldCap) != 0的元素放在高位链表中 // 比如，7 &amp; 4 != 0 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 遍历完成分化成两个链表了 // 低位链表在新桶中的位置与旧桶一样（即3和11还在三号桶中） if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 高位链表在新桶中的位置正好是原来的位置加上旧容量（即7和15搬移到七号桶了） if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab;}如果使用是默认构造方法，则第一次插入元素时初始化为默认值，容量为16，扩容门槛为12；如果使用的是非默认构造方法，则第一次插入元素时初始化容量等于扩容门槛，扩容门槛在构造方法里等于传入容量向上最近的2的n次方；如果旧容量大于0，则新容量等于旧容量的2倍，但不超过最大容量2的30次方，新扩容门槛为旧扩容门槛的2倍；创建一个新容量的桶；搬移元素，原链表分化成两个链表，低位链表存储在原来桶的位置，高位链表搬移到原来桶的位置加旧容量的位置；TreeNode.putTreeVal(…)方法插入元素到红黑树中的方法。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768final TreeNode&lt;K, V&gt; putTreeVal(HashMap&lt;K, V&gt; map, Node&lt;K, V&gt;[] tab, int h, K k, V v) { Class&lt;?&gt; kc = null; // 标记是否找到这个key的节点 boolean searched = false; // 找到树的根节点 TreeNode&lt;K, V&gt; root = (parent != null) ? root() : this; // 从树的根节点开始遍历 for (TreeNode&lt;K, V&gt; p = root; ; ) { // dir=direction，标记是在左边还是右边 // ph=p.hash，当前节点的hash值 int dir, ph; // pk=p.key，当前节点的key值 K pk; if ((ph = p.hash) &gt; h) { // 当前hash比目标hash大，说明在左边 dir = -1; } else if (ph &lt; h) // 当前hash比目标hash小，说明在右边 dir = 1; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) // 两者hash相同且key相等，说明找到了节点，直接返回该节点 // 回到putVal()中判断是否需要修改其value值 return p; else if ((kc == null &amp;&amp; // 如果k是Comparable的子类则返回其真实的类，否则返回null (kc = comparableClassFor(k)) == null) || // 如果k和pk不是同样的类型则返回0，否则返回两者比较的结果 (dir = compareComparables(kc, k, pk)) == 0) { // 这个条件表示两者hash相同但是其中一个不是Comparable类型或者两者类型不同 // 比如key是Object类型，这时可以传String也可以传Integer，两者hash值可能相同 // 在红黑树中把同样hash值的元素存储在同一颗子树，这里相当于找到了这颗子树的顶点 // 从这个顶点分别遍历其左右子树去寻找有没有跟待插入的key相同的元素 if (!searched) { TreeNode&lt;K, V&gt; q, ch; searched = true; // 遍历左右子树找到了直接返回 if (((ch = p.left) != null &amp;&amp; (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.find(h, k, kc)) != null)) return q; } // 如果两者类型相同，再根据它们的内存地址计算hash值进行比较 dir = tieBreakOrder(k, pk); } TreeNode&lt;K, V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) { // 如果最后确实没找到对应key的元素，则新建一个节点 Node&lt;K, V&gt; xpn = xp.next; TreeNode&lt;K, V&gt; x = map.newTreeNode(h, k, v, xpn); if (dir &lt;= 0) xp.left = x; else xp.right = x; xp.next = x; x.parent = x.prev = xp; if (xpn != null) ((TreeNode&lt;K, V&gt;) xpn).prev = x; // 插入树节点后平衡 // 把root节点移动到链表的第一个节点 moveRootToFront(tab, balanceInsertion(root, x)); return null; } }}寻找根节点；从根节点开始查找；比较hash值及key值，如果都相同，直接返回，在putVal()方法中决定是否要替换value值；根据hash值及key值确定在树的左子树还是右子树查找，找到了直接返回；如果最后没有找到则在树的相应位置插入元素，并做平衡；treeifyBin()方法如果插入元素后链表的长度大于等于8则判断是否需要树化。123456789101112131415161718192021222324252627final void treeifyBin(Node&lt;K, V&gt;[] tab, int hash) { int n, index; Node&lt;K, V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 如果桶数量小于64，直接扩容而不用树化 // 因为扩容之后，链表会分化成两个链表，达到减少元素的作用 // 当然也不一定，比如容量为4，里面存的全是除以4余数等于3的元素 // 这样即使扩容也无法减少链表的长度 resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K, V&gt; hd = null, tl = null; // 把所有节点换成树节点 do { TreeNode&lt;K, V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); // 如果进入过上面的循环，则从头节点开始树化 if ((tab[index] = hd) != null) hd.treeify(tab); }}TreeNode.treeify()方法真正树化的方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445final void treeify(Node&lt;K, V&gt;[] tab) { TreeNode&lt;K, V&gt; root = null; for (TreeNode&lt;K, V&gt; x = this, next; x != null; x = next) { next = (TreeNode&lt;K, V&gt;) x.next; x.left = x.right = null; // 第一个元素作为根节点且为黑节点，其它元素依次插入到树中再做平衡 if (root == null) { x.parent = null; x.red = false; root = x; } else { K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; // 从根节点查找元素插入的位置 for (TreeNode&lt;K, V&gt; p = root; ; ) { int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); // 如果最后没找到元素，则插入 TreeNode&lt;K, V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) { x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; // 插入后平衡，默认插入的是红节点，在balanceInsertion()方法里 root = balanceInsertion(root, x); break; } } } } // 把根节点移动到链表的头节点，因为经过平衡之后原来的第一个元素不一定是根节点了 moveRootToFront(tab, root);}从链表的第一个元素开始遍历；将第一个元素作为根节点；其它元素依次插入到红黑树中，再做平衡；将根节点移到链表第一元素的位置（因为平衡的时候根节点会改变）；get(Object key)方法1234567891011121314151617181920212223242526272829303132public V get(Object key) { Node&lt;K, V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;}final Node&lt;K, V&gt; getNode(int hash, Object key) { Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; first, e; int n; K k; // 如果桶的数量大于0并且待查找的key所在的桶的第一个元素不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { // 检查第一个元素是不是要查的元素，如果是直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { // 如果第一个元素是树节点，则按树的方式查找 if (first instanceof TreeNode) return ((TreeNode&lt;K, V&gt;) first).getTreeNode(hash, key); // 否则就遍历整个链表查找该元素 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null;}计算key的hash值；找到key所在的桶及其第一个元素；如果第一个元素的key等于待查找的key，直接返回；如果第一个元素是树节点就按树的方式来查找，否则按链表方式查找；TreeNode.getTreeNode(int h, Object k)方法12345678910111213141516171819202122232425262728293031323334353637383940final TreeNode&lt;K, V&gt; getTreeNode(int h, Object k) { // 从树的根节点开始查找 return ((parent != null) ? root() : this).find(h, k, null);}final TreeNode&lt;K, V&gt; find(int h, Object k, Class&lt;?&gt; kc) { TreeNode&lt;K, V&gt; p = this; do { int ph, dir; K pk; TreeNode&lt;K, V&gt; pl = p.left, pr = p.right, q; if ((ph = p.hash) &gt; h) // 左子树 p = pl; else if (ph &lt; h) // 右子树 p = pr; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) // 找到了直接返回 return p; else if (pl == null) // hash相同但key不同，左子树为空查右子树 p = pr; else if (pr == null) // 右子树为空查左子树 p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) // 通过compare方法比较key值的大小决定使用左子树还是右子树 p = (dir &lt; 0) ? pl : pr; else if ((q = pr.find(h, k, kc)) != null) // 如果以上条件都不通过，则尝试在右子树查找 return q; else // 都没找到就在左子树查找 p = pl; } while (p != null); return null;}经典二叉查找树的查找过程，先根据hash值比较，再根据key值比较决定是查左子树还是右子树。remove(Object key)方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public V remove(Object key) { Node&lt;K, V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;}final Node&lt;K, V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, index; // 如果桶的数量大于0且待删除的元素所在的桶的第一个元素不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) { Node&lt;K, V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 如果第一个元素正好就是要找的元素，赋值给node变量后续删除使用 node = p; else if ((e = p.next) != null) { if (p instanceof TreeNode) // 如果第一个元素是树节点，则以树的方式查找节点 node = ((TreeNode&lt;K, V&gt;) p).getTreeNode(hash, key); else { // 否则遍历整个链表查找元素 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } // 如果找到了元素，则看参数是否需要匹配value值，如果不需要匹配直接删除，如果需要匹配则看value值是否与传入的value相等 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) { if (node instanceof TreeNode) // 如果是树节点，调用树的删除方法（以node调用的，是删除自己） ((TreeNode&lt;K, V&gt;) node).removeTreeNode(this, tab, movable); else if (node == p) // 如果待删除的元素是第一个元素，则把第二个元素移到第一的位置 tab[index] = node.next; else // 否则删除node节点 p.next = node.next; ++modCount; --size; // 删除节点后置处理 afterNodeRemoval(node); return node; } } return null;}先查找元素所在的节点；如果找到的节点是树节点，则按树的移除节点处理；如果找到的节点是桶中的第一个节点，则把第二个节点移到第一的位置；否则按链表删除节点处理；修改size，调用移除节点后置处理等；TreeNode.removeTreeNode(…)方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117final void removeTreeNode(HashMap&lt;K, V&gt; map, Node&lt;K, V&gt;[] tab, boolean movable) { int n; // 如果桶的数量为0直接返回 if (tab == null || (n = tab.length) == 0) return; // 节点在桶中的索引 int index = (n - 1) &amp; hash; // 第一个节点，根节点，根左子节点 TreeNode&lt;K, V&gt; first = (TreeNode&lt;K, V&gt;) tab[index], root = first, rl; // 后继节点，前置节点 TreeNode&lt;K, V&gt; succ = (TreeNode&lt;K, V&gt;) next, pred = prev; if (pred == null) // 如果前置节点为空，说明当前节点是根节点，则把后继节点赋值到第一个节点的位置，相当于删除了当前节点 tab[index] = first = succ; else // 否则把前置节点的下个节点设置为当前节点的后继节点，相当于删除了当前节点 pred.next = succ; // 如果后继节点不为空，则让后继节点的前置节点指向当前节点的前置节点，相当于删除了当前节点 if (succ != null) succ.prev = pred; // 如果第一个节点为空，说明没有后继节点了，直接返回 if (first == null) return; // 如果根节点的父节点不为空，则重新查找父节点 if (root.parent != null) root = root.root(); // 如果根节点为空，则需要反树化（将树转化为链表） // 如果需要移动节点且树的高度比较小，则需要反树化 if (root == null || (movable &amp;&amp; (root.right == null || (rl = root.left) == null || rl.left == null))) { tab[index] = first.untreeify(map); // too small return; } // 分割线，以上都是删除链表中的节点，下面才是直接删除红黑树的节点（因为TreeNode本身即是链表节点又是树节点） // 删除红黑树节点的大致过程是寻找右子树中最小的节点放到删除节点的位置，然后做平衡，此处不过多注释 TreeNode&lt;K, V&gt; p = this, pl = left, pr = right, replacement; if (pl != null &amp;&amp; pr != null) { TreeNode&lt;K, V&gt; s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode&lt;K, V&gt; sr = s.right; TreeNode&lt;K, V&gt; pp = p.parent; if (s == pr) { // p was s's direct parent p.parent = s; s.right = p; } else { TreeNode&lt;K, V&gt; sp = s.parent; if ((p.parent = sp) != null) { if (s == sp.left) sp.left = p; else sp.right = p; } if ((s.right = pr) != null) pr.parent = s; } p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) root = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; } else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) { TreeNode&lt;K, V&gt; pp = replacement.parent = p.parent; if (pp == null) root = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; } TreeNode&lt;K, V&gt; r = p.red ? root : balanceDeletion(root, replacement); if (replacement == p) { // detach TreeNode&lt;K, V&gt; pp = p.parent; p.parent = null; if (pp != null) { if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; } } if (movable) moveRootToFront(tab, r);}TreeNode本身既是链表节点也是红黑树节点；先删除链表节点；再删除红黑树节点并做平衡；总结HashMap是一种散列表，采用（数组 + 链表 + 红黑树）的存储结构；HashMap的默认初始容量为16（1&lt;&lt;4），默认装载因子为0.75f，容量总是2的n次方；HashMap扩容时每次容量变为原来的两倍；当桶的数量小于64时不会进行树化，只会扩容；当桶的数量大于64且单个桶中元素的数量大于8时，进行树化；当单个桶中元素数量小于6时，进行反树化；HashMap是非线程安全的容器；HashMap查找添加元素的时间复杂度都为O(1)；彩蛋红黑树知多少？红黑树具有以下5种性质：节点是红色或黑色。根节点是黑色。每个叶节点（NIL节点，空节点）是黑色的。每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。红黑树的时间复杂度为O(log n)，与树的高度成正比。红黑树每次的插入、删除操作都需要做平衡，平衡时有可能会改变根节点的位置，颜色转换，左旋，右旋等。","link":"/2016/03/15/hashmap内部原理/"},{"title":"AQS论文翻译","text":"背景最近对并发底层实现比较有兴趣，正巧看到这篇译文不错，这里记录一下[13]。摘要在J2SE 1.5的java.util.concurrent包（下称j.u.c包）中，大部分的同步器（例如锁，屏障等等）都是基于AbstractQueuedSynchronizer类（下称AQS类），这个简单的框架而构建的。这个框架为同步状态的原子性管理、线程的阻塞和解除阻塞以及排队提供了一种通用的机制。这篇论文主要描述了这个框架基本原理、设计、实现、用法以及性能。1. 背景介绍通过JCP的JSR166规范，Java的1.5版本引入了j.u.c包，这个包提供了一系列支持中等程度并发的类。这些组件是一系列的同步器（抽象数据类型(ADT)）。这些同步器主要维护着以下几个功能：内部同步状态的管理(例如：表示一个锁的状态是获取还是释放)，同步状态的更新和检查操作，且至少有一个方法会导致调用线程在同步状态被获取时阻塞，以及在其他线程改变这个同步状态时解除线程的阻塞。上述的这些的实际例子包括：互斥排它锁的不同形式、读写锁、信号量、屏障、Future、事件指示器以及传送队列等。几乎任一同步器都可以用来实现其他形式的同步器。例如，可以用可重入锁实现信号量或者用信号量实现可重入锁。但是，这样做带来的复杂性，开销，不灵活使其至多只能是个二流工程。且缺乏吸引力。如果任何这样的构造方式不能在本质上比其他形式更简洁，那么开发者就不应该随意地选择其中的某个来构建另一个同步器。取而代之，JSR166建立了一个小框架，AQS类。这个框架为构造同步器提供一种通用的机制，并且被j.u.c包中大部分类使用，同时很多用户也用它来定义自己的同步器。在这篇论文的下面部分会讨论这个框架的需求、设计与实现背后的主要思路、示例用法，以及性能指标的一些测量。2. 需求2.1 功能同步器一般包含两种方法，一种是acquire，另一种是release。acquire操作阻塞调用的线程，直到或除非同步状态允许其继续执行。而release操作则是通过某种方式改变同步状态，使得一或多个被acquire阻塞的线程继续执行。j.u.c包中并没有对同步器的API做一个统一的定义。因此，有一些类定义了通用的接口（如Lock），而另外一些则定义了其专有的版本。因此在不同的类中，acquire和release操作的名字和形式会各有不同。例如：Lock.lock，Semaphore.acquire，CountDownLatch.await和FutureTask.get，在这个框架里，这些方法都是acquire操作。但是，J.U.C为支持一系列常见的使用选项，在类间都有个一致约定。在有意义的情况下，每一个同步器都支持下面的操作：阻塞和非阻塞（例如tryLock）同步。可选的超时设置，让调用者可以放弃等待通过中断实现的任务取消，通常是分为两个版本，一个acquire可取消，而另一个不可以。同步器的实现根据其状态是否独占而有所不同。独占状态的同步器，在同一时间只有一个线程可以通过阻塞点，而共享状态的同步器可以同时有多个线程在执行。一般锁的实现类往往只维护独占状态，但是，例如计数信号量在数量许可的情况下，允许多个线程同时执行。为了使框架能得到广泛应用，这两种模式都要支持。j.u.c包里还定义了Condition接口，用于支持监控形式的await/signal操作，这些操作与独占模式的Lock类有关，且Condition的实现天生就和与其关联的Lock类紧密相关。2.2 性能目标Java内置锁（使用synchronized的方法或代码块）的性能问题一直以来都在被人们关注，并且已经有一系列的文章描述其构造（例如引文[1],[3]）。然而，大部分的研究主要关注的是在单核处理器上大部分时候使用于单线程上下文环境中时，如何尽量降低其空间（因为任何的Java对象都可以当成是锁）和时间的开销。对于同步器来说这些都不是特别重要：程序员仅在需要的时候才会使用同步器，因此并不需要压缩空间来避免浪费，并且同步器几乎是专门用在多线程设计中（特别是在多核处理器上），在这种环境下，偶尔的竞争是在意料之中的。因此，常规的JVM锁优化策略主要是针对零竞争的场景，而其它场景则使用缺乏可预见性的“慢速路径（slow paths）” ，所以常规的JVM锁优化策略并不适用于严重依赖于J.U.C包的典型多线程服务端应用。这里主要的性能目标是可伸缩性，即在大部分情况下，即使，或特别在同步器有竞争的情况下，稳定地保证其效率。在理想的情况下，不管有多少线程正试图通过同步点，通过同步点的开销都应该是个常量。在某一线程被允许通过同步点但还没有通过的情况下，使其耗费的总时间最少，这是主要目标之一。然而，这也必须考虑平衡各种资源，包括总CPU时间的需求，内存负载以及线程调度的开销。例如：获取自旋锁通常比阻塞锁所需的时间更短，但是通常也会浪费CPU时钟周期，并且造成内存竞争，所以使用的并不频繁。实现同步器的这些目标包含了两种不同的使用类型。大部分应用程序是最大化其总的吞吐量，容错性，并且最好保证尽量减少饥饿的情况。然而，对于那些控制资源分配的程序来说，更重要是去维持多线程读取的公平性，可以接受较差的总吞吐量。没有任何框架可以代表用户去决定应该选择哪一个方式，因此，应该提供不同的公平策略。无论同步器的内部实现是多么的精雕细琢，它还是会在某些应用中产生性能瓶颈。因此，框架必须提供相应的监视工具让用户发现和缓和这些瓶颈。至少需要提供一种方式来确定有多少线程被阻塞了。3. 设计与实现同步器背后的基本思想非常简单。acquire操作如下：12345while (同步状态不允许获取) {如果尚未排队,则将当前线程排入队列;可能阻塞当前线程;}如果当前线程在排队则将其取消;release操作如下：123更新同步状态;if (状态允许阻塞线程获取)解锁一个或者多个已经入队的线程;为了实现上述操作，需要下面三个基本组件的相互协作：同步状态的原子性管理；线程的阻塞与解除阻塞；队列的管理；创建一个框架分别实现这三个组件是有可能的。但是，这会让整个框架既难用又没效率。例如：存储在队列节点的信息必须与解除阻塞所需要的信息一致，而暴露出的方法的签名必须依赖于同步状态的特性。同步器框架的核心决策是为这三个组件选择一个具体实现，同时在使用方式上又有大量选项可用。这里有意地限制了其适用范围，但是提供了足够的效率，使得实际上没有理由在合适的情况下不用这个框架而去重新建造一个。3.1 同步状态AQS类使用单个int（32位）来保存同步状态，并暴露出getState、setState以及compareAndSet操作来读取和更新这个状态。这些方法都依赖于j.u.c.atomic包的支持，这个包提供了兼容JSR133中volatile在读和写上的语义，并且通过使用本地的compare-and-swap或load-linked/store-conditional指令来实现compareAndSetState，使得仅当同步状态拥有一个期望值的时候，才会被原子地设置成新值。将同步状态限制为一个32位的整形是出于实践上的考量。虽然JSR166也提供了64位long字段的原子性操作，但这些操作在很多平台上还是使用内部锁的方式来模拟实现的，这会使同步器的性能可能不会很理想。当然，将来可能会有一个类是专门使用64位的状态的。然而现在就引入这么一个类到这个包里并不是一个很好的决定（译者注：JDK1.6中已经包含java.util.concurrent.locks.AbstractQueuedLongSynchronizer类，即使用 long 形式维护同步状态的一个 AbstractQueuedSynchronizer 版本）。目前来说，32位的状态对大多数应用程序都是足够的。在j.u.c包中，只有一个同步器类可能需要多于32位来维持状态，那就是CyclicBarrier类，所以，它用了锁（该包中大多数更高层次的工具亦是如此）。基于AQS的具体实现类必须根据暴露出的状态相关的方法定义tryAcquire和tryRelease方法，以控制acquire和release操作。当同步状态满足时，tryAcquire方法必须返回true，而当新的同步状态允许后续acquire时，tryRelease方法也必须返回true。这些方法都接受一个int类型的参数用于传递想要的状态。例如：可重入锁中，当某个线程从条件等待中返回，然后重新获取锁时，为了重新建立循环计数的场景。很多同步器并不需要这样一个参数，因此忽略它即可。3.2 阻塞在JSR166之前，阻塞线程和解除线程阻塞都是基于Java内置监视器，没有基于Java API可以用来创建同步器。唯一可以选择的是Thread.suspend和Thread.resume，但是它们都有无法解决的竞态问题，所以也没法用：当一个非阻塞的线程在一个正准备阻塞的线程调用suspend前调用了resume，这个resume操作将不会有什么效果。j.u.c包有一个LockSuport类，这个类中包含了解决这个问题的方法。方法LockSupport.park阻塞当前线程除非/直到有个LockSupport.unpark方法被调用（unpark方法被提前调用也是可以的）。unpark的调用是没有被计数的，因此在一个park调用前多次调用unpark方法只会解除一个park操作。另外，它们作用于每个线程而不是每个同步器。一个线程在一个新的同步器上调用park操作可能会立即返回，因为在此之前可能有“剩余的”unpark操作。但是，在缺少一个unpark操作时，下一次调用park就会阻塞。虽然可以显式地消除这个状态（译者注：就是多余的unpark调用），但并不值得这样做。在需要的时候多次调用park会更高效。这个简单的机制与有些用法在某种程度上是相似的，例如Solaris-9的线程库，WIN32中的“可消费事件”，以及Linux中的NPTL线程库。因此最常见的运行Java的平台上都有相对应的有效实现。（但目前Solaris和Linux上的Sun Hotspot JVM参考实现实际上是使用一个pthread的condvar来适应目前的运行时设计的）。park方法同样支持可选的相对或绝对的超时设置，以及与JVM的Thread.interrupt结合 —— 可通过中断来unpark一个线程。3.3 队列整个框架的关键就是如何管理被阻塞的线程的队列，该队列是严格的FIFO队列，因此，框架不支持基于优先级的同步。同步队列的最佳选择是自身没有使用底层锁来构造的非阻塞数据结构，目前，业界对此很少有争议。而其中主要有两个选择：一个是Mellor-Crummey和Scott锁（MCS锁）[5][8][10]的变体。一直以来，CLH锁仅被用于自旋锁。但是，在这个框架中，CLH锁显然比MCS锁更合适。因为CLH锁可以更容易地去实现“取消（cancellation）”和“超时”功能，因此我们选择了CLH锁作为实现的基础。但是最终的设计已经与原来的CLH锁有较大的出入，因此下文将对此做出解释。CLH队列实际上并不那么像队列，因为它的入队和出队操作都与它的用途（即用作锁）紧密相关。它是一个链表队列，通过两个字段head和tail来存取，这两个字段是可原子更新的，两者在初始化时都指向了一个空节点。一个新的节点，node，通过一个原子操作入队：123do { pred = tail;} while(!tail.compareAndSet(pred, node));每一个节点的“释放”状态都保存在其前驱节点中。因此，自旋锁的“自旋”操作就如下：1while (pred.status != RELEASED) ; // spin自旋后的出队操作只需将head字段指向刚刚得到锁的节点：1head = node;CLH锁的优点在于其入队和出队操作是快速、无锁的，以及无障碍的（即使在竞争下，某个线程总会赢得一次插入机会而能继续执行）；且探测是否有线程正在等待也很快（只要测试一下head是否与tail相等）；同时，“释放”状态是分散的（译者注：几乎每个节点都保存了这个状态，当前节点保存了其后驱节点的“释放”状态，因此它们是分散的，不是集中于一块的。），避免了一些不必要的内存竞争。在原始版本的CLH锁中，节点间甚至都没有互相链接。自旋锁中，pred变量可以是一个局部变量。然而，Scott和Scherer证明了通过在节点中显式地维护前驱节点，CLH锁就可以处理“超时”和各种形式的“取消”：如果一个节点的前驱节点取消了，这个节点就可以滑动去使用前面一个节点的状态字段。为了将CLH队列用于阻塞式同步器，需要做些额外的修改以提供一种高效的方式定位某个节点的后继节点。在自旋锁中，一个节点只需要改变其状态，下一次自旋中其后继节点就能注意到这个改变，所以节点间的链接并不是必须的。但在阻塞式同步器中，一个节点需要显式地唤醒（unpark）其后继节点。AQS队列的节点包含一个next链接到它的后继节点。但是，由于没有针对双向链表节点的类似compareAndSet的原子性无锁插入指令，因此这个next链接的设置并非作为原子性插入操作的一部分，而仅是在节点被插入后简单地赋值：1pred.next = node;next链接仅是一种优化。如果通过某个节点的next字段发现其后继结点不存在（或看似被取消了），总是可以使用pred字段从尾部开始向前遍历来检查是否真的有后续节点。第二个对CLH队列主要的修改是将每个节点都有的状态字段用于控制阻塞而非自旋。在同步器框架中，仅在线程调用具体子类中的tryAcquire方法返回true时，队列中的线程才能从acquire操作中返回；而单个“released”位是不够的。但仍然需要做些控制以确保当一个活动的线程位于队列头部时，仅允许其调用tryAcquire；这时的acquire可能会失败，然后（重新）阻塞。这种情况不需要读取状态标识，因为可以通过检查当前节点的前驱是否为head来确定权限。与自旋锁不同，读取head以保证复制时不会有太多的内存竞争（ there is not enough memory contention reading head to warrant replication.）。然而，“取消”状态必须存在于状态字段中。队列节点的状态字段也用于避免没有必要的park和unpark调用。虽然这些方法跟阻塞原语一样快，但在跨越Java和JVM runtime以及操作系统边界时仍有可避免的开销。在调用park前，线程设置一个“唤醒（signal me）”位，然后再一次检查同步和节点状态。一个释放的线程会清空其自身状态。这样线程就不必频繁地尝试阻塞，特别是在锁相关的类中，这样会浪费时间等待下一个符合条件的线程去申请锁，从而加剧其它竞争的影响。除非后继节点设置了“唤醒”位（译者注：源码中为-1），否则这也可避免正在release的线程去判断其后继节点。这反过来也消除了这些情形：除非“唤醒”与“取消”同时发生，否则必须遍历多个节点来处理一个似乎为null的next字段。同步框架中使用的CLH锁的变体与其他语言中的相比，主要区别可能是同步框架中使用的CLH锁需要依赖垃圾回收管理节点的内存，这就避免了一些复杂性和开销。但是，即使依赖GC也仍然需要在确定链接字段不再需要时将其置为null。这往往可以与出队操作一起完成。否则，无用的节点仍然可触及，它们就没法被回收。其它一些更深入的微调，包括CLH队列首次遇到竞争时才需要的初始空节点的延迟初始化等，都可以在J2SE1.5的版本的源代码文档中找到相应的描述。抛开这些细节，基本的acquire操作的最终实现的一般形式如下（互斥，非中断，无超时）：123456789101112f (!tryAcquire(arg)) { node = 创建和入栈一个新的节点; pred = 节点有效的前置节点; while (pred前置节点不是head节点 || !tryAcquire(arg)) { if (设置pred前置节点的信号位) park(); else compareAndSet,设置前置节点信号位为true; pred = 节点有效的前置节点; } head = node;}release操作：1234if(tryRelease(arg) &amp;&amp; head node's signal bit is set) { compareAndSet head's bit to false; unpark head's successor, if one exist}acquire操作的主循环次数依赖于具体实现类中tryAcquire的实现方式。另一方面，在没有“取消”操作的情况下，每一个组件的acquire和release都是一个O(1)的操作，忽略park中发生的所有操作系统线程调度。支持“取消”操作主要是要在acquire循环里的park返回时检查中断或超时。由超时或中断而被取消等待的线程会设置其节点状态，然后unpark其后继节点。在有“取消”的情况下，判断其前驱节点和后继节点以及重置状态可能需要O(n)的遍历（n是队列的长度）。由于“取消”操作，该线程再也不会被阻塞，节点的链接和状态字段可以被快速重建。3.4 条件队列AQS框架提供了一个ConditionObject类，给维护独占同步的类以及实现Lock接口的类使用。一个锁对象可以关联任意数目的条件对象，可以提供典型的管程风格的await、signal和signalAll操作，包括带有超时的，以及一些检测、监控的方法。通过修正一些设计决策，ConditionObject类有效地将条件（conditions）与其它同步操作结合到了一起。该类只支持Java风格的管程访问规则，这些规则中，仅当当前线程持有锁且要操作的条件（condition）属于该锁时，条件操作才是合法的（一些替代操作的讨论参考[[4]）。这样，一个ConditionObject关联到一个ReentrantLock上就表现的跟内置的管程（通过Object.wait等）一样了。两者的不同仅仅在于方法的名称、额外的功能以及用户可以为每个锁声明多个条件。ConditionObject使用了与同步器一样的内部队列节点。但是，是在一个单独的条件队列中维护这些节点的。signal操作是通过将节点从条件队列转移到锁队列中来实现的，而没有必要在需要唤醒的线程重新获取到锁之前将其唤醒。基本的await操作如下：1234创建和添加新的节点到条件队列;释放锁;阻塞,直到节点c处于锁定队列;重新获得锁;signal操作如下：1将第一个节点从条件队列转移到锁定队列;因为只有在持有锁的时候才能执行这些操作，因此他们可以使用顺序链表队列操作来维护条件队列（在节点中用一个nextWaiter字段）。转移操作仅仅把第一个节点从条件队列中的链接解除，然后通过CLH插入操作将其插入到锁队列上。实现这些操作主要复杂在，因超时或Thread.interrupt导致取消了条件等待时，该如何处理。“取消”和“唤醒”几乎同时发生就会有竞态问题，最终的结果遵照内置管程相关的规范。JSR133修订以后，就要求如果中断发生在signal操作之前，await方法必须在重新获取到锁后，抛出InterruptedException。但是，如果中断发生在signal后，await必须返回且不抛异常，同时设置线程的中断状态。为了维护适当的顺序，队列节点状态变量中的一个位记录了该节点是否已经（或正在）被转移。“唤醒”和“取消”相关的代码都会尝试用compareAndSet修改这个状态。如果某次signal操作修改失败，就会转移队列中的下一个节点（如果存在的话）。如果某次“取消”操作修改失败，就必须中止此次转移，然后等待重新获得锁。后面的情况采用了一个潜在的无限的自旋等待。在节点成功的被插到锁队列之前，被“取消”的等待不能重新获得锁，所以必须自旋等待CLH队列插入（即compareAndSet操作）被“唤醒”线程成功执行。这里极少需要自旋，且自旋里使用Thread.yield来提示应该调度某一其它线程，理想情况下就是执行signal的那个线程。虽然有可能在这里为“取消”实现一个帮助策略以帮助插入节点，但这种情况实在太少，找不到合适的理由来增加这些开销。在其它所有的情况下，这个基本的机制都不需要自旋或yield，因此在单处理器上保持着合理的性能。4. 用法AQS类将上述的功能结合到一起，并且作为一种基于“模版方法模式”[6]的基类提供给同步器。子类只需定义状态的检查与更新相关的方法，这些方法控制着acquire和 release操作。然而，将AQS的子类作为同步器ADT并不适合，因为这个类必须提供方法在内部控制acquire和release的规则，这些都不应该被用户所看到。所有java.util.concurrent包中的同步器类都声明了一个私有的继承了AbstractQueuedSynchronizer的内部类，并且把所有同步方法都委托给这个内部类。这样各个同步器类的公开方法就可以使用适合自己的名称。下面是一个最简单的Mutex类的实现，它使用同步状态0表示解锁，1表示锁定。这个类并不需要同步方法中的参数，因此这里在调用的时候使用0作为实参，方法实现里将其忽略。1234567891011121314class Mutex { class Sync extends AbstractQueuedSynchronizer { public boolean tryAcquire(int ignore) { return compareAndSetState(0, 1); } public boolean tryRelease(int ignore) { setState(0); return true; } } private final Sync sync = new Sync(); public void lock() { sync.acquire(0); } public void unlock() { sync.release(0); }}这个例子的一个更完整的版本，以及其它用法指南，可以在J2SE的文档中找到。还可以有一些变体。如，tryAcquire可以使用一种“test-and-test-and-set”策略，即在改变状态值前先对状态进行校验。令人诧异的是，像互斥锁这样性能敏感的东西也打算通过委托和虚方法结合的方式来定义。然而，这正是现代动态编译器一直在重点研究的面向对象设计结构。编译器擅长将这方面的开销优化掉，起码会优化频繁调用同步器的那些代码。AbstractQueuedSynchronizer类也提供了一些方法用来协助策略控制。例如，基础的acquire方法有可超时和可中断的版本。虽然到目前为止，我们的讨论都集中在像锁这样的独占模式的同步器上，但AbstractQueuedSynchronizer类也包含另一组方法（如acquireShared），它们的不同点在于tryAcquireShared和tryReleaseShared方法能够告知框架（通过它们的返回值）尚能接受更多的请求，最终框架会通过级联的signal(cascading signals)唤醒多个线程。虽然将同步器序列化（持久化存储或传输）一般来说没有太大意义，但这些类经常会被用于构造其它类，例如线程安全的集合，而这些集合通常是可序列化的。AbstractQueuedSynchronizer和ConditionObject类都提供了方法用于序列化同步状态，但不会序列化潜在的被阻塞的线程，也不会序列化其它内部暂时性的簿记（bookkeeping）变量。即使如此，在反序列化时，大部分同步器类也只仅将同步状态重置为初始值，这与内置锁的隐式策略一致 —— 总是反序列化到一个解锁状态。这相当于一个空操作，但仍必须显式地支持以便final字段能够反序列化。4.1 公平调度的控制尽管同步器是基于FIFO队列的，但它们并不一定就得是公平的。可以注意到，在基础的acquire算法（3.3节）中，tryAcquire是在入队前被执行的。因此一个新的acquire线程能够“窃取”本该属于队列头部第一个线程通过同步器的机会。可闯入的FIFO策略通常会提供比其它技术更高的总吞吐率。当一个有竞争的锁已经空闲，而下一个准备获取锁的线程又正在解除阻塞的过程中，这时就没有线程可以获取到这个锁，如果使用闯入策略，则可减少这之间的时间间隔。与此同时，这种策略还可避免过分的，无效率的竞争，这种竞争是由于只允许一个（第一个）排队的线程被唤醒然后尝试acquire操作导致的。在只要求短时间持有同步器的场景中，创建同步器的开发者可以通过定义tryAcquire在控制权返回之前重复调用自己若干次，来进一步凸显闯入的效果。可闯入的FIFO同步器只有概率性的公平属性。锁队列头部一个解除了阻塞的线程拥有一次无偏向的机会（译者注：即不会偏向队头的线程也不会偏向闯入的线程）来赢得与闯入的线程之间的竞争，如果竞争失败，要么重新阻塞要么进行重试。然而，如果闯入的线程到达的速度比队头的线程解除阻塞快，那么在队列中的第一个线程将很难赢得竞争，以至于几乎总要重新阻塞，并且它的后继节点也会一直保持阻塞。对于短暂持有的同步器来说，在队列中第一个线程被解除阻塞期间，多处理器上很可能发生过多次闯入（译者注：即闯入的线程的acquire操作）和release了。正如下文所提到的，最终结果就是保持一或多个线程的高进展速度的同时，仍至少在一定概率上避免了饥饿的发生。当有更高的公平性需求时，实现起来也很简单。如果需要严格的公平性，程序员可以把tryAcquire方法定义为，若当前线程不是队列的头节点（可通过getFirstQueuedThread方法检查，这是框架提供的为数不多的几个检测方法之一），则立即失败（返回false）。一个更快，但非严格公平的变体可以这样做，若队列为空（判断的瞬间），仍然允许tryAcquire执行成功。在这种情况下，多个线程同时遇到一个空队列时可能会去竞争以使自己第一个获得锁，这样，通常至少有一个线程是无需入队列的。java.util.concurrent包中所有支持公平模式的同步器都采用了这种策略。尽管公平性设置在实践中很有用，但是它们并没有保障，因为Java Language Specification没有提供这样的调度保证。例如：即使是严格公平的同步器，如果一组线程永远不需要阻塞来达到互相等待，那么JVM可能会决定纯粹以顺序方式运行它们。在实际中，单处理器上，在抢占式上下文切换之前，这样的线程有可能是各自运行了一段时间。如果这样一个线程正持有某个互斥锁，它将很快会被切换回来，仅是为了释放其持有的锁，然后会继续阻塞，因为它知道有另外一个线程需要这把锁，这更增加了同步器可用但没有线程能来获取之间的间隔。同步器公平性设置在多处理器上的影响可能会更大，因为在这种环境下会产生更多的交错，因此一个线程就会有更多的机会发现锁被另一个线程请求。在高竞争下，当保护的是短暂持有锁的代码体时，尽管性能可能会较差，但公平锁仍然能有效地工作。例如，当公平性锁保护的是相对长的代码体和/或有着相对长的锁间(inter-lock)间隔，在这种情况下，闯入只能带来很小的性能优势，但却可能会大大增加无限等待的风险。同步器框架将这些工程决策留给用户来确定。4.2 同步器下面是java.util.concurrent包中同步器定义方式的概述：ReentrantLock类使用AQS同步状态来保存锁（重复）持有的次数。当锁被一个线程获取时，ReentrantLock也会记录下当前获得锁的线程标识，以便检查是否是重复获取，以及当错误的线程（译者注：如果线程不是锁的持有者，在此线程中执行该锁的unlock操作就是非法的）试图进行解锁操作时检测是否存在非法状态异常。ReentrantLock也使用了AQS提供的ConditionObject，还向外暴露了其它监控和监测相关的方法。ReentrantLock通过在内部声明两个不同的AbstractQueuedSynchronizer实现类（提供公平模式的那个禁用了闯入策略）来实现可选的公平模式，在创建ReentrantLock实例的时候根据设置（译者注：即ReentrantLock构造方法中的fair参数）使用相应的AbstractQueuedSynchronizer实现类。ReentrantReadWriteLock类使用AQS同步状态中的16位来保存写锁持有的次数，剩下的16位用来保存读锁的持有次数。WriteLock的构建方式同ReentrantLock。ReadLock则通过使用acquireShared方法来支持同时允许多个读线程。Semaphore类（计数信号量）使用AQS同步状态来保存信号量的当前计数。它里面定义的acquireShared方法会减少计数，或当计数为非正值时阻塞线程；tryRelease方法会增加计数，可能在计数为正值时还要解除线程的阻塞。CountDownLatch类使用AQS同步状态来表示计数。当该计数为0时，所有的acquire操作（译者注：acquire操作是从aqs的角度说的，对应到CountDownLatch中就是await方法）才能通过。FutureTask类使用AQS同步状态来表示某个异步计算任务的运行状态（初始化、运行中、被取消和完成）。设置（译者注：FutureTask的set方法）或取消（译者注：FutureTask的cancel方法）一个FutureTask时会调用AQS的release操作，等待计算结果的线程的阻塞解除是通过AQS的acquire操作实现的。SynchronousQueues类（一种CSP（Communicating Sequential Processes）形式的传递）使用了内部的等待节点，这些节点可以用于协调生产者和消费者。同时，它使用AQS同步状态来控制当某个消费者消费当前一项时，允许一个生产者继续生产，反之亦然。java.util.concurrent包的使用者当然也可以为自定义的应用定义自己的同步器。例如，那些曾考虑到过的，但没有采纳进这个包的同步器包括提供WIN32事件各种风格的语义类，二元信号量，集中管理的锁以及基于树的屏障。5. 性能虽然AQS框架除了支持互斥锁外，还支持其它形式的同步方式，但锁的性能是最容易测量和比较的。即使如此，也还存在许多不同的测量方式。这里的实验主要是设计来展示锁的开销和吞吐量。在每个测试中，所有线程都重复的更新一个伪随机数，该随机数由nextRandom(int seed)方法计算：12int t = (seed % 127773) * 16807 – (seed / 127773) * 2836;return (t &gt; 0)? t : t + 0x7fffffff;在每次迭代中，线程以概率S在一个互斥锁下更新共享的生成器，否则（译者注：概率为1-S）更新其自己局部的生成器，此时是不需要锁的。如此，锁占用区域的耗时是短暂的，这就使线程持有锁期间被抢占时的外界干扰降到了最小。这个函数的随机性主要是为了两个目的：确定是否需要使用锁（这个生成器足以应付这里的需求），以及使循环中的代码不可能被轻易地优化掉。这里比较了四种锁：内置锁，用的是synchronized块；互斥锁，用的是像第四节例子中的那样简单的Mutex类；可重入锁，用的是ReentrantLock；以及公平锁，用的是ReentrantLock的公平模式。所有测试都运行在J2SE1.5 JDK build46（大致与beta2相同）的server模式下。在收集测试数据前，测试程序先运行20次非竞争的测试，以排除JVM“预热”（译者注：更多关于“预热”的内容，参见：Java 理论与实践: 动态编译与性能测量）过程的影响。除了公平模式下的测试只跑了一百万次迭代，其它每个线程中的测试都运行了一千万次迭代。该测试运行在四个X86机器和四个UltraSparc机器上。所有X86机器都运行的是RedHat基于NPTL 2.4内核和库的Linux系统。所有的UltraSparc机器都运行的是Solaris-9。测试时所有系统的负载都很轻。根据该测试的特征，并不要求系统完全空闲（译者注：即测试时操作系统上有其它较轻的负载也不会影响本次测试的结果。）。“4P”这个名字反映出双核超线程的Xeon更像是4路机器，而不是2路机器。这里没有将测试数据规范化。如下所示，同步的相对开销与处理器的数量、类型、速度之间不具备简单的关系。表1 测试的平台名字处理器数量类型速度(Mhz)1P1Pentium39002P2Pentium314002A2Athlon20004P2HTPentium4/Xeon24001U1UltraSparc26504U4UltraSparc24508U8UltraSparc375024U24UltraSparc37505.1 开销无竞争情况下的开销是通过仅运行一个线程，将概率S为1时的每次迭代时间减去概率S为0（访问共享内存的概率为0）时的每次迭代时间得到的（译者注：这里的“概率S”即前文提到的“概率S”，概率为0时是没有锁操作的，概率为1时是每次都有锁操作，因此将概率为1时的耗时减去概率为0时的耗时就是整个锁操作的开销。）。表2以纳秒为单位显示了非竞争场景下每次锁操作的开销。Mutex类最接近于框架的基本耗时，可重入锁的额外开销是记录当前所有者线程和错误检查的耗时，对于公平锁来说还包含开始时检查队列是否为空的耗时。表格2也展示与内置锁的“快速路径（fast path）”对比，tryAcquire的耗时。这里的差异主要反映出了各锁和机器中使用的不同的原子指令以及内存屏障的耗时。在多处理器上，这些指令常常是完全优于所有其它指令的。内置锁和同步器类之间的主要差别，显然是由于Hotspot锁在锁定和解锁时都使用了一次compareAndSet，而同步器的acquire操作使用了一次compareAndSet，但release操作用的是一次volatile写（即，多处理器上的一次内存屏障以及所有处理器上的重排序限制）。每个锁的绝对的和相对耗时因机器的不同而不同。表2 无竞争时的单锁开销（单位：纳秒）机器内置互斥可重入公平可重入1P18931372P587177812A132131304P116951091171U904058674U122821001158U1608310312324U16184108119从另一个极端看，表3展示了概率S为1，运行256个并发线程时产生了大规模的锁竞争下每个锁的开销。在完全饱和的情况下，可闯入的FIFO锁比内置锁的开销少了一个数量级（也就是更大的吞吐量），比公平锁更是少了两个数量级。这表现出即使有着极大的竞争，在维持线程进展方面可闯入FIFO策略的效率。表3也说明了即使在内部开销比较低的情况下，公平锁的性能也完全是由上下文切换的时间所决定的。列出的时间大致上都与各平台上线程阻塞和解除线程阻塞的时间相称。此外，后面增加的一个实验（仅使用机器4P）表明，对于这里用到的短暂持有的锁，公平参数的设置在总差异中的影响很小。这里将线程终止时间间的差异记录成一个粗粒度的离散量数。在4P的机器上，公平锁的时间度量的标准差平均为0.7%，可重入锁平均为6.0%。作为对比，为模拟一个长时间持有锁的场景，测试中使每个线程在持有锁的情况下计算了16K次随机数。这时，总运行时间几乎是相同的（公平锁：9.79s，可重入锁：9.72s）。公平模式下的差异依然很小，标准差平均为0.1%，而可重入锁上升到了平均29.5%。表格3 饱和时的单锁开销（单位：纳秒）机器内置互斥可重入公平可重入1P521466783272P930108132149672A7487984339104P1146188247153281U879153177413944U2590347368300048U12741571743108424U1983160182322915.2 吞吐量大部分同步器都是用于无竞争和极大竞争之间的。这可以用实验在两个方面进行检查，通过修改固定个线程的竞争概率，和/或通过往拥有固定竞争概率的线程集合里增加更多的线程。为了说明这些影响，测试运行在不同的竞争概率和不同的线程数目下，都用的是可重入锁。附图使用了一个slowdown度量标准。这里，t是总运行时间，b是一个线程在没有竞争或同步下的基线时间，n是线程数，p是处理器数，S是共享访问的比例（译者注：即前面的竞争概率S）。计算结果是实际执行时间与理想执行时间（通常是无法得到的）的比率，理想执行时间是通过使用Amdahl’s法则计算出来的。理想时间模拟了一次没有同步开销，没有因锁争用而导致线程阻塞的执行过程。即使这样，在很低的竞争下，相比理想时间，有一些测试结果却表现出了很小的速度增长，大概是由于基线和测试之间的优化、流水线等方面有着轻微的差别。图中用以2为底的对数为比例进行了缩放。例如，值为1表示实际时间是理想时间的两倍，4表示慢16倍。使用对数就不需要依赖一个随意的基线时间（这里指的是计算随机数的时间），因此，基于不同底数计算的结果表现出的趋势应该是类似的。这些测试使用的竞争概率从1/128（标识为“0.008”）到1，以2的幂为步长，线程的数量从1到1024，以2的幂的一半为步长。在单处理器（1P和1U）上，性能随着竞争的上升而下降，但不会随着线程数的增加而下降。多处理器在遭遇竞争时，性能下降的更快。根据多处理器相关的图表显示，开始出现的峰值处虽然只有几个线程的竞争，但相对性能通常却最差。这反映出了一个性能的过渡区域，在这里闯入的线程和被唤醒的线程都准备获取锁，这会让它们频繁的迫使对方阻塞。在大部分时候，过渡区域后面会紧接着一个平滑区域，因为此时几乎没有空闲的锁，所以会与单处理器上顺序执行的模式差不多；在多处理器机器上会较早进入平滑区域。例如，请注意，在满竞争（标识为“1.000”）下这些值表示，在处理器越少的机器上，会有更糟糕的相对速度下降。根据这些结果，可以针对阻塞（park/unpark）做进一步调优以减少上下文切换和相关的开销，这会给本框架带来小但显著的提升。此外，在多处理器上为短时间持有的但高竞争的锁采用某种形式的适应性自旋，可以避免这里看到的一些波动，这对同步器类大有裨益。虽然在跨不同上下文时适应性自旋很难很好的工作，但可以使用本框架为遇到这类使用配置的特定应用构建一个自定义形式的锁。6. 总结本文撰写之时，java.util.concurrent包中的同步器框架还太新所以还不能在实践中使用。因此在J2SE 1.5最终版本发布之前都很难看到其大范围的使用，并且，它的设计，API实现以及性能肯定还有无法预料的后果。但是，此时，这个框架明显能胜任其基本的目标，即为创建新的同步器提供一个高效的基础。7. 致谢Thanks to Dave Dice for countless ideas and advice during the development of this framework, to Mark Moir and Michael Scott for urging consideration of CLH queues, to David Holmes for critiquing early versions of the code and API, to Victor Luchangco and Bill Scherer for reviewing previous incarnations of the source code, and to the other members of the JSR166 Expert Group (Joe Bowbeer, Josh Bloch, Brian Goetz, David Holmes, and Tim Peierls) as well as Bill Pugh, for helping with design and specifications and commenting on drafts of this paper. Portions of this work were made possible by a DARPA PCES grant, NSF grant EIA-0080206 (for access to the 24way Sparc) and a Sun Collaborative Research Grant.参考文献1.Agesen, O., D. Detlefs, A. Garthwaite, R. Knippel, Y. S.Ramakrishna, and D. White. An Efficient Meta-lock for Implementing Ubiquitous Synchronization. ACM OOPSLA Proceedings, 1999. ↩2.Andrews, G. Concurrent Programming. Wiley, 1991. ↩3.Bacon, D. Thin Locks: Featherweight Synchronization for Java. ACM PLDI Proceedings, 1998. ↩4.Buhr, P. M. Fortier, and M. Coffin. Monitor Classification,ACM Computing Surveys, March 1995. ↩5.Craig, T. S. Building FIFO and priority-queueing spin locks from atomic swap. Technical Report TR 93-02-02,Department of Computer Science, University of Washington, Feb. 1993. ↩6.Gamma, E., R. Helm, R. Johnson, and J. Vlissides. Design Patterns, Addison Wesley, 1996. ↩7.Holmes, D. Synchronisation Rings, PhD Thesis, Macquarie University, 1999. ↩8.Magnussen, P., A. Landin, and E. Hagersten. Queue locks on cache coherent multiprocessors. 8th Intl. Parallel Processing Symposium, Cancun, Mexico, Apr. 1994. ↩9.Mellor-Crummey, J.M., and M. L. Scott. Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. ACM Trans. on Computer Systems,February 1991 ↩10.M. L. Scott and W N. Scherer III. Scalable Queue-Based Spin Locks with Timeout. 8th ACM Symp. on Principles and Practice of Parallel Programming, Snowbird, UT, June 2001. ↩11.Sun Microsystems. Multithreading in the Solaris Operating Environment. White paper available at http://wwws.sun.com/software/solaris/whitepapers.html 2002. ↩12.Zhang, H., S. Liang, and L. Bak. Monitor Conversion in a Multithreaded Computer System. United States Patent 6,691,304. 2004. ↩13.译文原文链接：原文链接：http://ifeve.com/tag/aqs/ ↩","link":"/2017/08/04/AQS论文翻译/"},{"title":"深入理解 Java 内存模型","text":"说明又发现一本不错的小书《深入理解 Java 内存模型》，趁着昨天周六一口气读完，虽然篇幅只有区区70页，可受益颇多。由于原文短小精悍，为了预留合理的上下文以助于理解，本文保留了大部分原文，并且在关键处使用笔者自己的理解对原文进行再解释（以表达方式再排列、语义扩充等方法，尽量在语义上确保大于等于原意）。好了废话不多说，我们开始吧~基础并发编程的分类在并发编程中，我们需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换 信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。在消息传递的并发模型里，线程之间没有公共状 态，线程之间必须通过明确的发送消息来显式进行通信。同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之 间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前， 因此同步是隐式进行的。Java 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的 Java 程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。Java 内存模型的抽象在java中，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享（本文使用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量 （Local variables），方法定义参数（java语言规范称之为formal method parameters）和异常处理器参数（exception handler parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。Java 线程之间的通信由 Java 内存模型（本文简称为 JMM）控制，JMM 决定一个 线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是 JMM 的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。Java 内存模型的抽象示意图如下：从上图来看，线程 A 与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去然后，线程 B 到主内存中去读取线程 A 之前已经更新过的共享变量举例说明Java线程间通信机制：以上图为背景，线程 A 、B 有主内存中共享变量 x 的副本。假设一开始，这三个内存中的 x 的值都为 0.线程 A 在执行时，把更新后的 x 值（假设值为1）临时存放在自己的本地内存 A 中。当线程 A 和 B 需要通信时，线程 A 首先会把自己本地内存中修改后的 x 值刷新到主内存中，此时主内存中的 x 值变为了 1。随后，线程 B 到主内存中去读线程 A 更新后的 x 值，此时线程 B 的本地内存的 x 值夜变为了 1。整体来看，实质上是线程 A 向 B 发送消息，而且这个通信过程必须要经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 Java 程序员提供内存可见性保证。重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分为三种类型：编译器优化排序编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。指令级并行重排序现代处理器采用了指令级秉性技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应的机器指令的执行顺序。内存系统的重排序由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。从 Java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序：其中 1 属于编译器重排序，2 和 3 属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。因此，为了避免此问题，JMM编译器设置了一些规则：对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel 称之为 memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。处理器重排序与内存屏障指令现代处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多的好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！我们来假设这样一种情况：Processor AProcessor Ba = 1; //A1x = b; //A2b = 2; //B1y = a; //B2显然，我们期望得到的执行结果是: x = 2，y = 1。然而实际上，我们却可能得到 x = y = 0，这是为什么呢？这里处理器 A 和处理器 B 可以同时把共享变量写入自己的缓冲区（A1，B1）然后从内存中读取另一个共享变量（A2，B2）最后才把自己写缓存区保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到 x=y=0从内存操作实际发生的顺序看，直到处理器 A 执行 A3 来刷新自己的写缓存区 A，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1 -&gt; A2，但内存操作实际发生的顺序确是 A2 -&gt; A1。此时，处理器 A 的内存操作顺序被重排序了处理器 B 和 A 的情况是类似的，这里不赘述了。产生这个结果的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作顺序不一致。下面是常见处理器允许的重排序类型的列表：Load-LoadLoad-StoreStore-StoreStore-Load数据依赖sparc- TSONNNYNx86NNNYNia64YYYYNPowerPCYYYYNN 表示处理器不允许两个操作重排序，Y 反之。从上表我们可以看出：常见的处理器都允许 Store-Load 重排序；常见的处理器都不允许对存在数据以来的操作做重排序。特别的，对于 x86 架构的处理器，仅仅会在 Store-Load 这种情况进行重排序。到这里，有同学可能有疑问了，既然 CPU 肯定会对 Store-Load 这种情况进行重排序，那岂不是乱套了，我 Java 代码怎么写？？实际上，为了解决这个问题，Java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM 把内存屏障指令分为以下四类：屏障类型指令示例说明LoadLoad BarriersLoad1; LoadLoad; Load2确保 Load1 数据的装载，之前于Load2 及所有后续装载指令的装载。StoreStore BarriersStore1;StoreStore; Store2确保 Store1 数据对其他处理器可 见（刷新到内存），之前于 Store2 及所有后续存储指令的存储。LoadStore BarriersLoad1;LoadStore; Store2确保 Load1 数据装载，之前于 Store2 及所有后续的存储指令刷 新到内存。StoreLoad BarriersStore1;StoreLoad; Load2确保 Store1 数据对其他处理器变得可见（指刷新到内存），之前于 Load2 及所有后续装载指令的装载。StoreLoad Barriers 会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。现代处理器大多支持 Store-Load Barriers （其他类型的屏障不一定被所有处理器支持）。但是执行该屏障的开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。happens-before从 JDK5 开始，Java 使用新的 JSR-133 内存模型（本文除非特别说明，针对的都是 JSR-133 内存模型）。JSR-133 使用 happens-before 的概念来阐述操作之间的内存可见性。在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同的线程之间。happens-before 规则如下：程序顺序规则一个线程中的每个操作，happens-before 于该线程中的任意后续操作监视器锁规则对一个监视器的解锁，happens-before 于随后对这个监视器的加锁volatile 变量规则对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读传递性如果 A happens-before B，且 B happens-before C，那么 A happens-before C这里，解释一下 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这个定义看起来很微妙，后文具体说明为什么这么定义。总结下happens-before 与 JMM 的关系，如下图所示：如上图所示，一个 happens-before 规则对应一个或多个编译器和处理器重排序规则。对于 Java 程序员来说，happens-before 规则简单易懂，它避免 Java 程序员为了理解 JMM 提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。重排序数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时，这两个操作之间就存在数据依赖性。数据依赖分下列三种类型：名称代码示例说明写后读a = 1;b = a;写一个变量之后，再读这个位置。写后写a = 1;a = 2;写一个变量之后，再写这个变量。读后写a = b;b = 1;读一个变量之后，再写这个变量。很容易对这三种情况得出如下结论：上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。也就是说，上面三种情况，不会发生处理器重排序。不过，这里需要注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑（这种情况参考前面中的例子）as-if-serial 语义不管怎么重排序（编译器和处理器为了提升并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守 as-if-serial 语义。为了遵守 as-if-serial 语义，编译器和处理器不会对存在 数据依赖关系 的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之前不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。我们举例说明：123double pi = 3.14; // Adouble r = 1.0; // Bdouble area = pi * r * r; // C上面三个操作的数据依赖关系如下图所示：如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（因为这种情况下程序结果将被改变）。但 A 和 B 之间是没有数据依赖关系的，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序：as-if-serial 语义把单线程程序保护了起来，遵守 as-if-serial 语义的编译器、runtime和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序时按程序的顺序来执行的。as-if-serial 语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。程序顺序规则根据 happens-before 的程序顺序规则，上面计算圆的面积的示例代码存在三个 happens-before 关系：A happens-before BB happens-before CA happens-before C（根据前两个推导出来的，即传递性）这里 A happens-before B ，但实际执行 B 却可以排在 A 之前执行（看上面那两种重排序后的执行顺序）。在前面提到过，如果 A happens-before B ，JMM 并不要求 A 一定要在 B 之前执行。JMM 仅仅要求前一个操作（执行的结果）对后一个操作可见；而且重排序操作 A 和操作 B 后的执行结果，与操作 A 和 B 按照程序顺序执行的结果一致。在这种情况下，JMM 会认为这种重排序并不非法（not illegal），JMM 允许这种排序。在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从 happens-before 的定义我们可以看出，JMM 同样遵从这一目标。重排序对多线程的影响现在让我们看看，重排序是否会改变多线程程序的执行结果。我们还用一个示例代入：12345678910111213141516class RecorderExample { int a = 0; boolean flag = false; public void writer() { a = 1; // 1 flag = true; // 2 } public void reader() { if (flag) { // 3 int i = a * a; // 4 ...... } }}flag 变量是一个标记，用来标识变量 a 是否已被写入。这里假设有两个线程 A 和 B，A 首先执行 writer()方法，随后 B 线程接着执行 reader()方法。线程 B 在执行操作 4 时，能否看到线程 A 在操作 1 对共享变量 a 的写入？答案是：不一定看到。由于操作 1 和操作 2 没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作 3 和操作 4 没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。让我们先来看看，当操作 1 和操作 2 重排序时，可能会产生什么效果？注：本文统一用红色的虚箭线标识错误的读操作，用绿色的虚箭线标识正确的读操作如上图所示，操作 1 和操作 2 做了重排序。程序执行时，线程 A 首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！现在再让我们看看，当操作 3 和操作 4 重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。如图所示，操作 3 和 4 重排序后，程序的执行时序：在程序中，操作 3 和操作 4 存在控制依赖关系。当代码中存在控制依赖关系时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a*a，然后把计算结果临时保存到另一个名为重排序缓存（record buffer ROB）的硬件缓存中。当接下来操作 3 的条件判断为真时，就把计算结果写入变量 i 中。显然，此种情况下（如上图所示），猜测执行实质上对操作 3 和 4 做了重排序。重排序在这里破坏了多线程程序的语义！在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。顺序一致性数据竞争与顺序一致性保证当程序未正确同步时，就可能会存在数据竞争。Java 内存模型规范对数据竞争的定义如下：一个线程中写一个变量在另一个线程读同一个变量而且写和读没有通过同步来排序当代码中包含数据竞争时，程序的执行旺旺产生违反直觉的结果（前面的示例正是如此）。如果一个多线程程序能正确同步，这个程序将是一个没有数据竞争的程序。JMM 对正确同步的多线程程序的内存一致性做了如下保证：如果程序是正确同步的，程序的执行将具有顺序一致性（sequentially consistent），即程序的执行结果与该程序在 顺序一致性内存模型 中的执行结果相同。马上我们将会看到，这对于程序员来说是一个极强的保证。这里的同步是指广义上的同步，包括对常用同步原语（synchronized，voliatile 和 final）的正确使用。顺序一致性内存模型顺序一一致性模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。其提供两大特性：一个线程中的所有操作必须按照程序的顺序来执行（不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性模型中，每个操作都必须原子执行且立刻对所有线程可见。顺序一致性内存模型为程序员提供的视图如下：在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面的示意图我们可以看出：在任意时间点，最多只有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化（即在顺序一致性模型中，所有操作之间具有全序关系）为了更好的理解，下面我们通过两个示意图来对顺序一致性模型的特性做进一步的说明。假设有两个线程 A 和 B 并发执行。其中 A 线程有三个操作，他们在程序中的顺序是：A1-&gt;A2-&gt;A3。B 线程也有三个操作，他们在程序中的顺序是：B1-&gt;B2-&gt;B3。假设这两个线程使用监视器来正确同步：A 线程的三个操作执行后释放监视器锁，随后 B 线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图所示：现在我们再假设两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图：未同步的程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程 A 和线程 B 看到的执行顺序都是：B &gt;A1-&gt;A2-&gt;B2-&gt;A3-&gt;B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。（注意，该模型的不做同步这种情况下保证，JMM 并没有实施）但是，在 JMM 中就没有这个保证。未同步的程序在 JMM 中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，在还没刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被这个线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其他线程看到的操作执行顺序将不一致。同步程序的顺序一致性效果下面我们对前面的示例程序 RecordExample 用锁来同步，看看正确同步的程序如何具有顺序一致性。请看下面的示例代码：123456789101112131415class SynchronizedExample { int a = 0; Boolean flag = false; public synchronized void writer() { a = 1; flag = true; } // 释放锁 public synchronized void reader() { if(flag) { int i = a; ...... } } // 释放锁}上面示例代码中，假设 A 线程执行 writer() 方法后，B 线程执行 reader()方法。这是一个正确同步的多线程程序。根据 JMM 规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。下面是该程序在两个内存模型中的执行时序对比图：在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在 JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区的代码“逸出”到临界区之外，那样会破坏监视器的语义。）JMM 会在退出临界区和进入临界区这两个关键时间点做一些特别的处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图（具体后文说明）。虽然线程 A 在临界区做了重排序，但由于监视器的互斥执行的特性，这里的线程 B 根本无法“观察”到线程 A 在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。从这里我们可以看到 JMM 在具体实现上的基本方针：在不改变（正确同步）程序执行结果的前提下，尽可能为编译器和处理器的优化打开方便之门。未同步程序的执行特性对于未同步或未正确同步的多线程程序，JMM 只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false），JMM 保证线程读操作读取到的值不会无中生有（out of thin air）的冒出来。为了实现最小安全性，JMM 在堆上分配对象时，首先会清零内存空间，然后才会在上面分配对象（JVM内部会同步这两个操作）。因此，在已清零的内存空间（pre zeroed memory）分配对象时，域的默认初始化已经完成了。JMM 不保证未同步的执行结果与该程序在顺序一致性模型中的执行结果一致（这也符合我们的直觉）。因为如果想要保证执行结果一致，JMM 需要禁止大量的处理器和编译器的优化，这对程序的执行性能会产生很大的影响。而且未同步的程序在顺序一致性模型中执行时，整体时无序的，其执行结果往往无法预知。保证未同步程序在这两个模型中的执行结果一致没什么意义。未同步程序在 JMM 中的执行，整体时无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有下面几个差异：顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一只的操作执行顺序。这一点前面已经讲过了，这里就不再赘述。JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。第三个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一协力步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字节。这里的关键是，总线会同步视试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其他所有的处理器和 I/O 设备执行内存的读/写。下面我们通过一个示意图来说明总线的工作机制：如上图所示，假设处理器 A、B 和 C 同时向总线发起总线事务，这时总线仲裁（bus arbitration）会对竞争作出裁决，这里我们假设总线在仲裁后判定处理器 A 在竞争者获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器 A 继续它的总线事务，而其他两个处理器则要等待处理器 A 的总线事务完成后才能开始再次执行内存访问。假设在处理器 A 执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器 D 向总线发起了总线事务，此时处理器 D 的这个请求会被总线禁止。总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java 语言规范鼓励但不强求 JVM 对 64 位的 long 型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。当单个内存操作不具有原子性，将可能会产生意想不到的后果。请看下面示意图：如上图所示，假设处理器 A 写一个 long 型变量，同时处理器 B 要读这个 long 型变量。处理器 A 中 64 位的写操作被拆分成两个 32 位的写操作，且这两个 32 位的写操作被分配到不同的写事务中执行。同时处理器 B 中的 64 位读操作被分配到单个读事务中执行。当处理器 A 和 B 按上图的时序来执行时，处理器 B 将看到仅仅处理器 A “写了一半” 的无效值。注意，在 JSM-133 之前的旧内存模型中，在 32 位处理器上，一个 64 位 long/double 型变量的读/写操作可以被拆分为两个 32 位的读/写操作来执行。从 JSR-133 内存模型开始（即从 JDK5 开始），仅仅只允许把一个 64 位 long/double 型变量的写操作拆分为两个 32 位的写操作执行，任意的读操作在 JSR-133 中都必须具有原子性（即任意读操作必须要在单个读事务中执行）。volatilevolatile的特性当我们声明变量为 volatile 后，对这个变量的读/写将会很特别。理解volitile 特性的一个好方法是把对 volatile 变量的单个读/写，看成是使用同一个锁对这些单个读/写操作做了同步。下面，我们通过具体的示例来说明，请看下面的示例代码：123456789101112131415class VolatileFeaturesExample { volatile long vl = 0L; //使用 volatile 声明 64 位的 long 型变量 public void set(long l) { vl = l; //单个 volatile 变量的写 } public void getAndIncrement() { vl++; //复合（多个）volatile 变量的读/写 } public long get() { return vl; //单个 volatile 变量的读 } }假设有多个线程分别调用上面程序的三个方法，这个程序在语义上和下面程序等价：1234567891011121314151617class VolatileFeaturesExample { long vl = 0L; // 64 位的 long 型普通变量 public synchronized void set(long l) { //对单个的普通变量的写用同一个锁同步 vl = l; } public void getAndIncrement() { //普通方法调用 long temp = get(); //调用已同步的读方法 temp += 1L; //普通写操作 set(temp); //调用已同步的写方法 } public synchronized long get() { //对单个的普通变量的读用同一个锁同步 return vl; } }如上面示例程序所示，对一个 volatile 变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个锁来同步，他们的之间的执行效果相同。锁的 happens-before 规则保证释放锁和获得锁的两个线程之间的内存可见性，这意味着对一个 volatile 变量的读，总是能看到（任意线程）对这个 volitile 变量最后的写入。锁的语义决定了临界区代码的执行具有原子性。这意味着即使是 64 位的 long 型和 double 型变量（在 32 位处理器上），只要它是 volatile 变量，对该变量的读写就将具有原子性。如果是多个 volatile 操作或类似于 volatile++这种复合操作，这些操作整体上不具有原子性。简而言之，volatile 变量自身具有下列特性：可见性对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入原子性对任意单个 volatile 变量的读/写具有原子性，但类似于 volatile++这种复合操作不具有原子性","link":"/2017/03/12/深入理解Java内存模型/"},{"title":"聊聊Kafka","text":"背景kafka 的诞生，是为了解决 linkedin 的数据管道问题，期初 linkedin 采用了 ActiveMQ 来进行数据交换，大约是在 2010 年前后，那时的 ActiveMQ 还远远无法满足 linkedin 对数据传递系统的要求，经常由于各种缺陷而导致消息阻塞或者服务无法正常访问，为了能够解决这个问题，linkedin 决定研发自己的消息传递系统，当时 linkedin 的首席架构师 jay kreps 便开始组织团队进行消息传递系统的研发；kafka 名字的由来kafka 的架构师 jay kreps 对于 kafka 的名称由来是这样讲的，由于 jay kreps 非常喜欢 franz kafka, 并且觉得 kafka 这个名字很酷，因此取了个和消息传递系统完全不相干的名称 kafka，取名字是并没有特别的含义。kafka 的设计目标使用推送和拉取模式 实现生产者和消费者的解耦；微消息系统中的消息提供数据持久化，以便支持多个消费者；系统可以随着数据流的增长进行横向扩展；通过系统优化实现高吞吐量；kafka 历史2010 年底，开源到 github，初始版本为 0.7.0；2011 年 7 月因为备受关注，被纳入 apache 孵化器项目；2012 年 10 月，kafka 从 apache 孵化器项目毕业，成为 apache 顶级项目；2014 年，jay kreps, neha narkhede, jun rao 离开 linkedin, 成立 confluent, 此后 linkedin 和 confluent 成为 kafka 的核心贡组织，致力于将 kafka 推广应用应用场景消息系统Kafka 作为一款优秀的消息系统，具有高吞吐量、内置的分区、备份冗余分布式等特点，为大规模消息处理提供了一种很好的解决方案。应用监控利用 Kafka 采集应用程序和服务器健康相关的指标，如 CPU 占用率、IO、内存、连接数、TPS、QPS 等，然后将指标信息进行处理，从而构建一个具有监控仪表盘、曲线图等可视化监控系统。例如，很多公司采用 Kafka 与 ELK（ElasticSearch、Logstash 和 Kibana）整合构建应用服务监控系统。网站用户行为追踪为了更好地了解用户行为、操作习惯，改善用户体验，进而对产品升级改进，将用户操作轨迹、内容等信息发送到 Kafka 集群上，通过 Hadoop、Spark 或 Strom 等进行数据分析处理，生成相应的统计报告，为推荐系统推荐对象建模提供数据源，进而为每个用户进行个性化推荐。流处理需要将已收集的流数据提供给其他流式计算框架进行处理，用 Kafka 收集流数据是一个不错的选择，而且当前版本的 Kafka 提供了 Kafka Streams 支持对流数据的处理。持久性日志Kafka 可以为外部系统提供一种持久性日志的分布式系统。日志可以在多个节点间进行备份，Kafka 为故障节点数据恢复提供了一种重新同步的机制。同时，Kafka 很方便与 HDFS 和 Flume 进行整合，这样就方便将 Kafka 采集的数据持久化到其他外部系统。基本概念Kafka 是一种高效的分布式消息系统。在性能上，它具有内置的数据冗余度与弹性，也具有高吞吐能力和可扩展性。在功能上，它支持自动化的数据保存限制，能够以 “流” 的方式为应用提供数据转换，以及按照 “键 - 值（key-value）” 的建模关系 “压缩” 数据流。Message（消息）Kafka 中的一条记录或数据单位。每条消息都有一个键和对应的一个值，有时还会有可选的消息头。Producer（生产者）Producer 将消息发布到 Kafka 的 topics 上。Producer 决定向 topic 分区的发布方式，如：轮询的随机方法、或基于消息键（key）的分区算法。Broker（代理）Kafka 以分布式系统或集群的方式运行。那么群集中的每个节点称为一个 Broker。Topic（主题）Topic 是那些被发布的数据记录或消息的一种类别。消费者通过订阅 Topic，来读取写给它们的数据。Topic Partition（主题分区）不同的 Topic 被分为不同的分区，而每一条消息都会被分配一个 Offset，通常每个分区都会被复制至少一到两次。每个分区都有一个 Leader 和存放在各个 Follower 上的一到多个副本（即：数据的副本），此法可防止某个 Broker 的失效。群集中的所有 Broker 都可以作为 Leader 和 Follower，但是一个 Broker 最多只能有一个 Topic Partition 的副本。Leader 可被用来进行所有的读写操作。Offset（偏移量）单个分区中的每一条消息都被分配一个 Offset，它是一个单调递增的整型数，可用来作为分区中消息的唯一标识符。Consumer（消费者）Consumer 通过订阅 Topic partition，来读取 Kafka 的各种 Topic 消息。然后，消费类应用处理会收到消息，以完成指定的工作。Consumer group（消费组）Consumer 可以按照 Consumer group 进行逻辑划分。Topic Partition 被均衡地分配给组中的所有Consumers。因此，在同一个 Consumer group 中，所有的 Consumer 都以负载均衡的方式运作。换言之，同一组中的每一个 Consumer 都能看到每一条消息。如果某个 Consumer 处于 “离线” 状态的话，那么该分区将会被分配给同组中的另一个 Consumer。这就是所谓的 “再均衡（rebalance）”。当然，如果组中的 Consumer 多于分区数，则某些 Consumer 将会处于闲置的状态。相反，如果组中的 Consumer 少于分区数，则某些 Consumer 会获得来自一个以上分区的消息。Lag（延迟）当 Consumer 的速度跟不上消息的产生速度时，Consumer 就会因为无法从分区中读取消息，而产生延迟。延迟表示为分区头后面的 Offset 数量。从延迟状态（到 “追赶上来”）恢复正常所需要的时间，取决于 Consumer 每秒能够应对的消息速度。其公式如下：1time = messages / (consume rate per second - produce rate per second)架构原理对于 Kafka 的架构原理，我们先提出如下几个问题：Kafka 的 topic 和分区内部是如何存储的，有什么特点？与传统的消息系统相比，Kafka 的消费模型有什么优点？Kafka 如何实现分布式的数据存储与数据读取？架构图进程视角主题视角消费模型消息由生产者发送到 Kafka 集群后，会被消费者消费。一般来说我们的消费模型有两种：推送模型 (Push)拉取模型 (Pull)基于推送模型的消息系统，由消息代理记录消费状态。消息代理将消息推送到消费者后，标记这条消息为已经被消费，但是这种方式无法很好地保证消费的处理语义。比如当我们已经把消息发送给消费者之后，由于消费进程挂掉或者由于网络原因没有收到这条消息，如果我们在消费代理将其标记为已消费，这个消息就永久丢失了。如果我们利用生产者收到消息后回复这种方法，消息代理需要记录消费状态，这种不可取。如果采用 Push，消息消费的速率就完全由消费代理控制，一旦消费者发生阻塞，就会出现问题。Kafka 采取拉取模型 (Poll)，由自己控制消费速度，以及消费的进度，消费者可以按照任意的偏移量进行消费。比如消费者可以消费已经消费过的消息进行重新处理，或者消费最近的消息等等。网络模型Kafka Client：单线程 Selector单线程模式适用于并发链接数小，逻辑简单，数据量小。在kafka中，consumer和producer都是使用的上面的单线程模式。这种模式不适合kafka的服务端，在服务端中请求处理过程比较复杂，会造成线程阻塞，一旦出现后续请求就会无法处理，会造成大量请求超时，引起雪崩。而在服务器中应该充分利用多线程来处理执行逻辑。Kafka server：多线程Selector在kafka服务端采用的是多线程的Selector模型，Acceptor运行在一个单独的线程中，对于读取操作的线程池中的线程都会在selector注册read事件，负责服务端读取请求的逻辑。成功读取后，将请求放入message queue共享队列中。然后在写线程池中，取出这个请求，对其进行逻辑处理，即使某个请求线程阻塞了，还有后续的县城从消息队列中获取请求并进行处理，在写线程中处理完逻辑处理，由于注册了OP_WIRTE事件，所以还需要对其发送响应。日志结构与数据存储Kafka中的消息是以主题（Topic）为基本单位进行组织的，各个主题之间相互独立。在这里主题只是一个逻辑上的抽象概念，而在实际数据文件的存储中，Kafka中的消息存储在物理上是以一个或多个分区（Partition）构成，每个分区对应本地磁盘上的一个文件夹，每个文件夹内包含了日志索引文件（“.index”和“.timeindex”）和日志数据文件（“.log”）两部分。分区数量可以在创建主题时指定，也可以在创建Topic后进行修改。在Kafka中正是因为使用了分区（Partition）的设计模型，通过将主题（Topic）的消息打散到多个分区，并分布保存在不同的Kafka Broker节点上实现了消息处理的高吞吐量。其生产者和消费者都可以多线程地并行操作，而每个线程处理的是一个分区的数据。同时，Kafka为了实现集群的高可用性，在每个Partition中可以设置有一个或者多个副本（Replica），分区的副本分布在不同的Broker节点上。同时，从副本中会选出一个副本作为Leader，Leader副本负责与客户端进行读写操作。而其他副本作为Follower会从Leader副本上进行数据同步。分区、副本的日志文件存储在三台虚拟机上搭建完成Kafka的集群后（Kafka Broker节点数量为3个），通过在Kafka Broker节点的/bin下执行以下的命令即可创建主题和指定数量的分区以及副本：1./kafka-topics.sh --create --zookeeper 10.154.0.73:2181 --replication-factor 3 --partitions 3 --topic kafka-topic-01创建完主题、分区和副本后可以查到出主题的状态（该方式主要列举了主题所有分区对应的副本以及ISR列表信息）：12345./kafka-topics.sh --describe --zookeeper 10.154.0.73:2181 --topic kafka-topic-01Topic:kafka-topic-01 PartitionCount:3 ReplicationFactor:3 Configs: Topic: kafka-topic-01 Partition: 0 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0 Topic: kafka-topic-01 Partition: 1 Leader: 2 Replicas: 2,0,1 Isr: 2,1,0 Topic: kafka-topic-01 Partition: 2 Leader: 0 Replicas: 0,1,2 Isr: 1,2,0通过实现一个简单的Kafka Producer的demo，即可完成生产者发送消息给Kafka Broker的功能。在使用Producer产生大量的消息后，可以看到部署集群的三台虚拟机在Kafka的config/server.properties配置文件中“log.dirs”指定的日志数据存储目录下存在三个分区目录，同时在每个分区目录下存在很多对应的日志数据文件和日志索引文件文件，具体如下：12345678910111213141516#1、分区目录文件drwxr-x--- 2 root root 4096 Jul 26 19:35 kafka-topic-01-0drwxr-x--- 2 root root 4096 Jul 24 20:15 kafka-topic-01-1drwxr-x--- 2 root root 4096 Jul 24 20:15 kafka-topic-01-2#2、分区目录中的日志数据文件和日志索引文件-rw-r----- 1 root root 512K Jul 24 19:51 00000000000000000000.index-rw-r----- 1 root root 1.0G Jul 24 19:51 00000000000000000000.log-rw-r----- 1 root root 768K Jul 24 19:51 00000000000000000000.timeindex-rw-r----- 1 root root 512K Jul 24 20:03 00000000000022372103.index-rw-r----- 1 root root 1.0G Jul 24 20:03 00000000000022372103.log-rw-r----- 1 root root 768K Jul 24 20:03 00000000000022372103.timeindex-rw-r----- 1 root root 512K Jul 24 20:15 00000000000044744987.index-rw-r----- 1 root root 1.0G Jul 24 20:15 00000000000044744987.log-rw-r----- 1 root root 767K Jul 24 20:15 00000000000044744987.timeindex-rw-r----- 1 root root 10M Jul 24 20:21 00000000000067117761.index-rw-r----- 1 root root 511M Jul 24 20:21 00000000000067117761.log-rw-r----- 1 root root 10M Jul 24 20:21 00000000000067117761.timeindex由上面可以看出，每个分区在物理上对应一个文件夹，分区的命名规则为主题名后接“—”连接符，之后再接分区编号，分区编号从0开始，编号的最大值为分区总数减1。每个分区又有1至多个副本，分区的副本分布在集群的不同代理上，以提高可用性。从存储的角度上来说，分区的每个副本在逻辑上可以抽象为一个日志（Log）对象，即分区副本与日志对象是相对应的。下图是在三个Kafka Broker节点所组成的集群中分区的主/备份副本的物理分布情况图：日志索引和数据文件的存储结构在Kafka中，每个Log对象又可以划分为多个LogSegment文件，每个LogSegment文件包括一个日志数据文件和两个索引文件（偏移量索引文件和消息时间戳索引文件）。其中，每个LogSegment中的日志数据文件大小均相等（该日志数据文件的大小可以通过在Kafka Broker的config/server.properties配置文件的中的log.segment.bytes进行设置，默认为1G大小（1073741824字节），在顺序写入消息时如果超出该设定的阈值，将会创建一组新的日志数据和索引文件）。Kafka将日志文件封装成一个FileMessageSet对象，将偏移量索引文件和消息时间戳索引文件分别封装成OffsetIndex和TimerIndex对象。Log和LogSegment均为逻辑概念，Log是对副本在Broker上存储文件的抽象，而LogSegment是对副本存储下每个日志分段的抽象，日志与索引文件才与磁盘上的物理存储相对应。下图为Kafka日志存储结构中的对象之间的对应关系图：为了进一步查看“.index”偏移量索引文件、“.timeindex”时间戳索引文件和“.log”日志数据文件，可以执行下面的命令将二进制分段的索引和日志数据文件内容转换为字符型文件：12345678910111213141516171819202122232425262728293031323334353637383940414243# 1、执行下面命令即可将日志数据文件内容dump出来./kafka-run-class.sh kafka.tools.DumpLogSegments --files /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.log --print-data-log &gt; 00000000000022372103_txt.log#2、dump出来的具体日志数据内容Dumping /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.logStarting offset: 22372103offset: 22372103 position: 0 CreateTime: 1532433067157 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 5d2697c5-d04a-4018-941d-881ac72ed9fdoffset: 22372104 position: 0 CreateTime: 1532433067159 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 0ecaae7d-aba5-4dd5-90df-597c8b426b47offset: 22372105 position: 0 CreateTime: 1532433067159 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 87709dd9-596b-4cf4-80fa-d1609d1f2087............offset: 22372444 position: 16365 CreateTime: 1532433067166 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 8d52ec65-88cf-4afd-adf1-e940ed9a8ff9offset: 22372445 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 5f5f6646-d0f5-4ad1-a257-4e3c38c74a92offset: 22372446 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 51dd1da4-053e-4507-9ef8-68ef09d18ccaoffset: 22372447 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 80d50a8e-0098-4748-8171-fd22d6af3c9b............offset: 22372785 position: 32730 CreateTime: 1532433067174 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: db80eb79-8250-42e2-ad26-1b6cfccb5c00offset: 22372786 position: 32730 CreateTime: 1532433067176 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 51d95ab0-ab0d-4530-b1d1-05eeb9a6ff00............#3、同样地，dump出来的具体偏移量索引内容Dumping /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.indexoffset: 22372444 position: 16365offset: 22372785 position: 32730offset: 22373467 position: 65460offset: 22373808 position: 81825offset: 22374149 position: 98190offset: 22374490 position: 114555............#4、dump出来的时间戳索引文件内容Dumping /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.timeindextimestamp: 1532433067174 offset: 22372784timestamp: 1532433067191 offset: 22373466timestamp: 1532433067206 offset: 22373807timestamp: 1532433067214 offset: 22374148timestamp: 1532433067222 offset: 22374489timestamp: 1532433067230 offset: 22374830............由上面dump出来的偏移量索引文件和日志数据文件的具体内容可以分析出来，偏移量索引文件中存储着大量的索引元数据，日志数据文件中存储着大量消息结构中的各个字段内容和消息体本身的值。索引文件中的元数据postion字段指向对应日志数据文件中message的实际位置（即为物理偏移地址）。下面的表格先列举了Kakfa消息体结构中几个主要字段的说明：Kafka消息字段各个字段说明offset消息偏移量message size消息总长度CRC32CRC32编码校验和attributes表示为独立版本、或标识压缩类型、或编码类型magic表示本次发布Kafka服务程序协议版本号key length消息Key的长度key消息Key的实际数据valuesize消息的实际数据长度playload消息的实际数据日志数据文件Kafka将生产者发送给它的消息数据内容保存至日志数据文件中，该文件以该段的基准偏移量左补齐0命名，文件后缀为“.log”。分区中的每条message由offset来表示它在这个分区中的偏移量，这个offset并不是该Message在分区中实际存储位置，而是逻辑上的一个值（Kafka中用8字节长度来记录这个偏移量），但它却唯一确定了分区中一条Message的逻辑位置，同一个分区下的消息偏移量按照顺序递增（这个可以类比下数据库的自增主键）。另外，从dump出来的日志数据文件的字符值中可以看到消息体的各个字段的内容值。偏移量索引文件如果消息的消费者每次fetch都需要从1G大小（默认值）的日志数据文件中来查找对应偏移量的消息，那么效率一定非常低，在定位到分段后还需要顺序比对才能找到。Kafka在设计数据存储时，为了提高查找消息的效率，故而为分段后的每个日志数据文件均使用稀疏索引的方式建立索引，这样子既节省空间又能通过索引快速定位到日志数据文件中的消息内容。偏移量索引文件和数据文件一样也同样也以该段的基准偏移量左补齐0命名，文件后缀为“.index”。从上面dump出来的偏移量索引内容可以看出，索引条目用于将偏移量映射成为消息在日志数据文件中的实际物理位置，每个索引条目由offset和position组成，每个索引条目可以唯一确定在各个分区数据文件的一条消息。其中，Kafka采用稀疏索引存储的方式，每隔一定的字节数建立了一条索引，可以通过index.interval.bytes设置索引的跨度；有了偏移量索引文件，通过它，Kafka就能够根据指定的偏移量快速定位到消息的实际物理位置。具体的做法是，根据指定的偏移量，使用二分法查询定位出该偏移量对应的消息所在的分段索引文件和日志数据文件。然后通过二分查找法，继续查找出小于等于指定偏移量的最大偏移量，同时也得出了对应的position（实际物理位置），根据该物理位置在分段的日志数据文件中顺序扫描查找偏移量与指定偏移量相等的消息。下面是Kafka中分段的日志数据文件和偏移量索引文件的对应映射关系图（ps：其中也说明了如何按照起始偏移量来定位到日志数据文件中的具体消息）。时间戳索引文件从上面一节的分区目录中，我们还可以看到存在一些以“.timeindex”的时间戳索引文件。这种类型的索引文件是Kafka从0.10.1.1版本开始引入的的一个基于时间戳的索引文件，它们的命名方式与对应的日志数据文件和偏移量索引文件名基本一样，唯一不同的就是后缀名。从上面dump出来的该种类型的时间戳索引文件的内容来看，每一条索引条目都对应了一个8字节长度的时间戳字段和一个4字节长度的偏移量字段，其中时间戳字段记录的是该LogSegment到目前为止的最大时间戳，后面对应的偏移量即为此时插入新消息的偏移量。另外，时间戳索引文件的时间戳类型与日志数据文件中的时间类型是一致的，索引条目中的时间戳值及偏移量与日志数据文件中对应的字段值相同（ps：Kafka也提供了通过时间戳索引来访问消息的方法）。副本机制Kafka的副本机制是多个服务端节点对其他节点的主题分区的日志进行复制。当集群中的某个节点出现故障，访问故障节点的请求会被转移到其他正常节点(这一过程通常叫Reblance),kafka每个主题的每个分区都有一个主副本以及0个或者多个副本，副本保持和主副本的数据同步，当主副本出故障时就会被替代。在Kafka中并不是所有的副本都能被拿来替代主副本，所以在kafka的leader节点中维护着一个ISR(In sync Replicas)集合，翻译过来也叫正在同步中集合，在这个集合中的需要满足两个条件:节点必须和ZK保持连接在同步的过程中这个副本不能落后主副本太多另外还有个AR(Assigned Replicas)用来标识副本的全集,OSR用来表示由于落后被剔除的副本集合，所以公式如下:ISR = leader + 没有落后太多的副本; AR = OSR+ ISR;这里先要说下两个名词:HW(高水位)是consumer能够看到的此partition的位置，LEO是每个partition的log最后一条Message的位置。HW能保证leader所在的broker失效，该消息仍然可以从新选举的leader中获取，不会造成消息丢失。当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别：1（默认）：这意味着producer在ISR中的leader已成功收到的数据并得到确认后发送下一条message。如果leader宕机了，则会丢失数据。0：这意味着producer无需等待来自broker的确认而继续发送下一批消息。这种情况下数据传输效率最高，但是数据可靠性确是最低的。-1：producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。但是这样也不能保证数据不丢失，比如当ISR中只有leader时(其他节点都和zk断开连接，或者都没追上)，这样就变成了acks=1的情况。高可用及幂等在分布式系统中一般有三种处理语义:at-least-once至少一次，有可能会有多次。如果producer收到来自ack的确认，则表示该消息已经写入到Kafka了，此时刚好是一次，也就是我们后面的exactly-once。但是如果producer超时或收到错误，并且request.required.acks配置的不是-1，则会重试发送消息，客户端会认为该消息未写入Kafka。如果broker在发送Ack之前失败，但在消息成功写入Kafka之后，这一次重试将会导致我们的消息会被写入两次，所以消息就不止一次地传递给最终consumer，如果consumer处理逻辑没有保证幂等的话就会得到不正确的结果。在这种语义中会出现乱序，也就是当第一次ack失败准备重试的时候，但是第二消息已经发送过去了，这个时候会出现单分区中乱序的现象,我们需要设置Prouducer的参数max.in.flight.requests.per.connection，flight.requests是Producer端用来保存发送请求且没有响应的队列，保证Producer端未响应的请求个数为1。at-most-once如果在ack超时或返回错误时producer不重试，也就是我们讲request.required.acks=-1，则该消息可能最终没有写入kafka，所以consumer不会接收消息。exactly-once刚好一次，即使producer重试发送消息，消息也会保证最多一次地传递给consumer。该语义是最理想的，也是最难实现的。在0.10之前并不能保证exactly-once，需要使用consumer自带的幂等性保证。0.11.0使用事务保证了如何实现exactly-once要实现exactly-once在Kafka 0.11.0中有两个官方策略:单Producer单Topic每个producer在初始化的时候都会被分配一个唯一的PID，对于每个唯一的PID，Producer向指定的Topic中某个特定的Partition发送的消息都会携带一个从0单调递增的sequence number。在我们的Broker端也会维护一个维度为，每次提交一次消息的时候都会对齐进行校验:如果消息序号比Broker维护的序号大一以上，说明中间有数据尚未写入，也即乱序，此时Broker拒绝该消息，Producer抛出InvalidSequenceNumber如果消息序号小于等于Broker维护的序号，说明该消息已被保存，即为重复消息，Broker直接丢弃该消息，Producer抛出DuplicateSequenceNumber如果消息序号刚好大一，就证明是合法的上面所说的解决了两个问题:当Prouducer发送了一条消息之后失败，broker并没有保存，但是第二条消息却发送成功，造成了数据的乱序。当Producer发送了一条消息之后，broker保存成功，ack回传失败，producer再次投递重复的消息。上面所说的都是在同一个PID下面，意味着必须保证在单个Producer中的同一个seesion内，如果Producer挂了，被分配了新的PID，这样就无法保证了，所以Kafka中又有事务机制去保证。事务在kafka中事务的作用是实现exactly-once语义保证操作的原子性，要么全部成功，要么全部失败。有状态的操作的恢复事务可以保证就算跨多个，在本次事务中的对消费队列的操作都当成原子性，要么全部成功，要么全部失败。并且，有状态的应用也可以保证重启后从断点处继续处理，也即事务恢复。在kafka的事务中，应用程序必须提供一个唯一的事务ID，即Transaction ID，并且宕机重启之后，也不会发生改变，Transactin ID与PID可能一一对应。区别在于Transaction ID由用户提供，而PID是内部的实现对用户透明。为了Producer重启之后，旧的Producer具有相同的Transaction ID失效，每次Producer通过Transaction ID拿到PID的同时，还会获取一个单调递增的epoch。由于旧的Producer的epoch比新Producer的epoch小，Kafka可以很容易识别出该Producer是老的Producer并拒绝其请求。为了实现这一点，Kafka 0.11.0.0引入了一个服务器端的模块，名为Transaction Coordinator，用于管理Producer发送的消息的事务性。该Transaction Coordinator维护Transaction Log，该log存于一个内部的Topic内。由于Topic数据具有持久性，因此事务的状态也具有持久性。Producer并不直接读写Transaction Log，它与Transaction Coordinator通信，然后由Transaction Coordinator将该事务的状态插入相应的Transaction Log。Transaction Log的设计与Offset Log用于保存Consumer的Offset类似。最佳实践针对 Partitions 的最佳实践了解分区的数据速率，以确保提供合适的数据保存空间此处所谓“分区的数据速率”是指数据的生成速率。换言之，它是由“平均消息大小”乘以“每秒消息数”得出的数据速率决定了在给定时间内，所能保证的数据保存空间的大小（以字节为单位）。如果您不知道数据速率的话，则无法正确地计算出满足基于给定时间跨度的数据，所需要保存的空间大小。同时，数据速率也能够标识出单个 Consumer 在不产生延时的情况下，所需要支持的最低性能值。除非您有其他架构上的需要，否则在写 Topic 时请使用随机分区在您进行大型操作时，各个分区在数据速率上的参差不齐是非常难以管理的。其原因来自于如下三个方面：首先，“热”（有较高吞吐量）分区上的 Consumer 势必会比同组中的其他 Consumer 处理更多的消息，因此很可能会导致出现在处理上和网络上的瓶颈。其次，那些为具有最高数据速率的分区，所配置的最大保留空间，会导致Topic 中其他分区的磁盘使用量也做相应地增长。第三，根据分区的 Leader 关系所实施的最佳均衡方案，比简单地将 Leader 关系分散到所有 Broker 上，要更为复杂。在同一 Topic 中，“热”分区会“承载”10 倍于其他分区的权重。有关 Topic Partition 的使用，可以参阅《Kafka Topic Partition的各种有效策略》https://blog.newrelic.com/engineering/effective-strategies-kafka-topic-partitioning/ 。针对 Consumers 的最佳实践如果 Consumers 运行的是比 Kafka 0.10 还要旧的版本，那么请马上升级在 0.8.x 版中，Consumer 使用 Apache ZooKeeper 来协调 Consumer group，而许多已知的 Bug 会导致其长期处于再均衡状态，或是直接导致再均衡算法的失败（我们称之为“再均衡风暴”）。因此在再均衡期间，一个或多个分区会被分配给同一组中的每个 Consumer。而在再均衡风暴中，分区的所有权会持续在各个 Consumers 之间流转，这反而阻碍了任何一个 Consumer 去真正获取分区的所有权。调优 Consumer 的套接字缓冲区（socket buffers），以应对数据的高速流入在 Kafka 的 0.10.x 版本中，参数 receive.buffer.bytes 的默认值为 64KB。而在 Kafka 的 0.8.x 版本中，参数 socket.receive.buffer.bytes 的默认值为 100KB。这两个默认值对于高吞吐量的环境而言都太小了，特别是如果 Broker 和 Consumer 之间的网络带宽延迟积（bandwidth-delay product）大于局域网（local areanetwork，LAN）时。对于延迟为 1 毫秒或更多的高带宽的网络（如 10Gbps 或更高），请考虑将套接字缓冲区设置为 8 或 16MB。如果您的内存不足，也至少考虑设置为 1MB。当然，您也可以设置为 -1，它会让底层操作系统根据网络的实际情况，去调整缓冲区的大小。但是，对于需要启动“热”分区的 Consumers 来说，自动调整可能不会那么快。设计具有高吞吐量的 Consumers，以便按需实施背压（back-pressure）通常，我们应该保证系统只去处理其能力范围内的数据，而不要超负荷“消费”，进而导致进程中断“挂起”，或出现 Consume group 的溢出。如果是在 Java 虚拟机（JVM）中运行，Consumers 应当使用固定大小的缓冲区，而且最好是使用堆外内存（off-heap）。请参见 Disruptor 模式：http://lmax-exchange.github.io/disruptor/files/Disruptor-1.0.pdf固定大小的缓冲区能够阻止 Consumer 将过多的数据拉到堆栈上，以至于 JVM 花费掉其所有的时间去执行垃圾回收，进而无法履行其处理消息的本质工作。在 JVM 上运行各种 Consumers 时，请警惕垃圾回收对它们可能产生的影响例如，长时间垃圾回收的停滞，可能导致 ZooKeeper 的会话被丢弃、或 Consumer group 处于再均衡状态。对于 Broker 来说也如此，如果垃圾回收停滞的时间太长，则会产生集群掉线的风险。针对 Producers 的最佳实践配置 Producer，以等待各种确认籍此 Producer 能够获知消息是否真正被发送到了 Broker 的分区上。在 Kafka 的 0.10.x 版本上，其设置是 Acks；而在 0.8.x 版本上，则为 request.required.acks。Kafka 通过复制，来提供容错功能，因此单个节点的故障、或分区 Leader 关系的更改不会影响到系统的可用性。如果您没有用 Acks 来配置 Producer（或称“fireand forget”）的话，则消息可能会悄然丢失。为各个 Producer 配置 Retries其默认值为 3，当然是非常低的。不过，正确的设定值取决于您的应用程序，即：就那些对于数据丢失零容忍的应用而言，请考虑设置为 Integer.MAX_VALUE（有效且最大）。这样将能够应对 Broker 的 Leader 分区出现无法立刻响应 Produce 请求的情况。为高吞吐量的 Producer，调优缓冲区的大小特别是 buffer.memory 和 batch.size（以字节为单位）。由于 batch.size 是按照分区设定的，而 Producer 的性能和内存的使用量，都可以与 Topic 中的分区数量相关联。因此，此处的设定值将取决于如下几个因素：Producer 数据速率（消息的大小和数量）要生成的分区数可用的内存量请记住，将缓冲区调大并不总是好事，如果 Producer 由于某种原因而失效了（例如，某个 Leader 的响应速度比确认还要慢），那么在堆内内存（on-heap）中的缓冲的数据量越多，其需要回收的垃圾也就越多。检测应用程序，以跟踪诸如生成的消息数、平均消息大小、以及已使用的消息数等指标针对 Brokers 的最佳实践在各个 Brokers 上，请压缩 Topics 所需的内存和 CPU 资源日志压缩（请参见https://kafka.apache.org/documentation/#compaction）需要各个 Broker 上的堆栈（内存）和 CPU 周期都能成功地配合实现而如果让那些失败的日志压缩数据持续增长的话，则会给 Brokers 分区带来风险。您可以在 Broker 上调整 log.cleaner.dedupe.buffer.size 和 log.cleaner.threads 这两个参数，但是请记住，这两个值都会影响到各个 Brokers 上的堆栈使用。如果某个 Broker 抛出 OutOfMemoryError 异常，那么它将会被关闭、并可能造成数据的丢失。而缓冲区的大小和线程的计数，则取决于需要被清除的 Topic Partition 数量、以及这些分区中消息的数据速率与密钥的大小。对于 Kafka 的 0.10.2.1 版本而言，通过 ERROR 条目来监控日志清理程序的日志文件，是检测其线程可能出现问题的最可靠方法。通过网络吞吐量来监控 Brokers请监控发向（transmit，TX）和收向（receive，RX）的流量，以及磁盘的 I/O、磁盘的空间、以及 CPU 的使用率，而且容量规划是维护群集整体性能的关键步骤。在群集的各个 Brokers 之间分配分区的 Leader 关系Leader 通常会需要大量的网络 I/O 资源。例如，当我们将复制因子（replication factor）配置为 3、并运行起来时。Leader 必须首先获取分区的数据，然后将两套副本发送给另两个 Followers，进而再传输到多个需要该数据的 Consumers 上。因此在该例子中，单个 Leader 所使用的网络 I/O，至少是 Follower 的四倍。而且，Leader 还可能需要对磁盘进行读操作，而 Follower 只需进行写操作。不要忽略监控 Brokers 的 in-sync replica（ISR）shrinks、under-replicatedpartitions 和 unpreferred leaders这些都是集群中潜在问题的迹象。例如，单个分区频繁出现 ISR 收缩，则暗示着该分区的数据速率超过了 Leader 的能力，已无法为 Consumer 和其他副本线程提供服务了。按需修改 Apache Log4j 的各种属性详细内容可以参考：https://github.com/apache/kafka/blob/trunk/config/log4j.propertiesKafka 的 Broker 日志记录会耗费大量的磁盘空间，但是我们却不能完全关闭它。因为有时在发生事故之后，需要重建事件序列，那么 Broker 日志就会是我们最好的、甚至是唯一的方法。禁用 Topic 的自动创建，或针对那些未被使用的 Topics 建立清除策略例如，在设定的 x 天内，如果未出现新的消息，您应该考虑该 Topic 是否已经失效，并将其从群集中予以删除。此举可避免您花时间去管理群集中被额外创建的元数据。对于那些具有持续高吞吐量的 Brokers，请提供足够的内存，以避免它们从磁盘子系统中进行读操作我们应尽可能地直接从操作系统的缓存中直接获取分区的数据。然而，这就意味着您必须确保自己的 Consumers 能够跟得上“节奏”，而对于那些延迟的 Consumer 就只能强制 Broker 从磁盘中读取了。对于具有高吞吐量服务级别目标（service level objectives，SLOs）的大型群集，请考虑为 Brokers 的子集隔离出不同的 Topic至于如何确定需要隔离的 Topics，则完全取决于您自己的业务需要。例如，您有一些使用相同群集的联机事务处理（multipleonline transaction processing，OLTP）系统。那么将每个系统的 Topics 隔离到不同 Brokers 子集中，则能够有助于限制潜在事件的影响半径。在旧的客户端上使用新的 Topic 消息格式。应当代替客户端，在各个 Brokers 上加载额外的格式转换服务当然，最好还是要尽量避免这种情况的发生。不要错误地认为在本地主机上测试好 Broker，就能代表生产环境中的真实性能了要知道，如果使用复制因子为 1，并在环回接口上对分区所做的测试，是与大多数生产环境截然不同的。在环回接口上网络延迟几乎可以被忽略的，而在不涉及到复制的情况下，接收 Leader 确认所需的时间则同样会出现巨大的差异。参考资料Kafka 入门与实践 牟大恩著 2017.11 出版NewRelic 官方博客：20 Best Practices for Working With Apache Kafka at ScaleMapR 官方博客：Kafka as Streaming TransportApache Kafka 官方文档公众号咖啡拿铁：你必须知道的 Kafka","link":"/2018/10/21/聊聊Kafka/"}],"tags":[{"name":"源码剖析","slug":"源码剖析","link":"/tags/源码剖析/"},{"name":"Java核心","slug":"Java核心","link":"/tags/Java核心/"},{"name":"集合","slug":"集合","link":"/tags/集合/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"deploy","slug":"deploy","link":"/tags/deploy/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Shell","slug":"Shell","link":"/tags/Shell/"},{"name":"心得","slug":"心得","link":"/tags/心得/"},{"name":"部署","slug":"部署","link":"/tags/部署/"},{"name":"APM","slug":"APM","link":"/tags/APM/"},{"name":"Java性能监控","slug":"Java性能监控","link":"/tags/Java性能监控/"},{"name":"设计","slug":"设计","link":"/tags/设计/"},{"name":"架构模式","slug":"架构模式","link":"/tags/架构模式/"},{"name":"业务场景","slug":"业务场景","link":"/tags/业务场景/"},{"name":"通知推送","slug":"通知推送","link":"/tags/通知推送/"},{"name":"轮询算法","slug":"轮询算法","link":"/tags/轮询算法/"},{"name":"容器","slug":"容器","link":"/tags/容器/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"内存模型","slug":"内存模型","link":"/tags/内存模型/"},{"name":"论文","slug":"论文","link":"/tags/论文/"},{"name":"AQS","slug":"AQS","link":"/tags/AQS/"},{"name":"并发","slug":"并发","link":"/tags/并发/"},{"name":"JMM","slug":"JMM","link":"/tags/JMM/"},{"name":"中间件","slug":"中间件","link":"/tags/中间件/"},{"name":"消息队列","slug":"消息队列","link":"/tags/消息队列/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"}],"categories":[{"name":"计算机科学与技术","slug":"计算机科学与技术","link":"/categories/计算机科学与技术/"}]}